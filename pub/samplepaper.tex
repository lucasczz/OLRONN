% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

% User imported packages
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{pifont}
\usepackage[backend=biber, style=lncs, maxcitenames=2, maxbibnames=5]{biblatex}
\usepackage{booktabs}
\usepackage{caption, subcaption}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cleveref}
\usepackage{todonotes}
\usepackage{varwidth}
\usepackage{tikz}
\usepackage{graphicx}
\newcommand{\cmark}{\ding{51}} % checkmark symbol
\newcommand{\xmark}{\ding{55}} % X-mark symbol

\addbibresource{bibliography.bib}

\begin{document}
%
\title{Learning Rate Optimization in Online Deep Learning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Anonymous\inst{1}}
%
\authorrunning{Anonymous et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%\institute{
%    Princeton University, Princeton NJ 08544, USA
%    \and
%    Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%    \email{lncs@springer.com}\\
%    \url{http://www.springer.com/gp/computer-science/lncs}
%    \and
%    ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%    \email{\{abc,lncs\}@uni-heidelberg.de}
%    }
%

\maketitle
%
\begin{abstract}
	Efficient training via gradient-based optimization techniques is an essential building block to the success of deep learning. Extensive research on the impact and the effective estimation of an appropriate learning rate has partly enabled these techniques. Despite the proliferation of data streams generated by IoT devices, digital platforms, etc., previous research has been primarily focused on batch learning, which assumes that all training data is available a priori. However, characteristics such as the gradual emergence of data and the occurrence of distributional shifts also known as \textit{concept drift} pose additional challenges. Therefore, the findings on batch learning may not be applicable to streaming environments, where the underlying model has to adapt each time a new data instance appears. In this work, we seek to address this knowledge gap by (i) evaluating and comparing typical learning rate schedules and optimizers, (ii) exploring adaptations of these techniques, and (iii) providing insights into effective learning rate tuning in the context of stream-based deep learning.

	\keywords{Data streams  \and Learning Rate \and Neural Networks.}
\end{abstract}
%
%
%
\section{Introduction}
Deep learning models have demonstrated exceptional performance in various domains.
One of the main factors leading to such outstanding results is the choice of the optimization method used to train the target model.
Nearly all modern deep learning applications, use first-order stochastic optimization methods like \textit{stochastic gradient descent}, which iteratively update the parameters of the underlying model based on gradient information, for this purpose.
One of the most important variables of such algorithms is the step size or \textit{learning rate} (LR).

As a result, many techniques for setting and optimizing the learning rate have emerged over the years (see Figure~\ref{fig:lr_overview}).
Based on prior knowledge, the learning rate can for instance be set as a fixed value or a schedule altering the step size over time.
Alternatively, one could use an adaptive learning rate technique, which considers historical gradient information to modify the learning rate at each iteration.

In batch learning scenario where all training data is available a priori, the aforementioned methods are well researched.
Despite the increasing prevalence of online learning environments, where data becomes available step by step as part of a data stream, their use in such scenarios has however received little attention in research.

According to \textcite{bifetMOAMassiveOnline2010} a machine learning model operating on a data stream must be able to
\begin{center}
	\begin{description}
		\item[R1:] process a single instance at a time,\label{rq:single_instance}
		\item[R2:] process each instance in a limited amount of time,\label{rq:limited_time}
		\item[R3:] use a limited amount of memory,\label{rq:limited_memory}
		\item[R4:] predict at any time,\label{rq:predict_any_time}
		\item[R5:] adapt to changes in the data distribution.\label{rq:adapt_to_drift}
	\end{description}
\end{center}

These requirements introduce additional challenges when it comes to the selection of an appropriate optimizer and learning rate.

To enable more informed decisions when it comes to learning rate optimization, we provide insight into these challenges and empirically evaluate commonly used optimization techniques in an online learning setting (i).
We further introduce a \textit{drift reset} mechanism to adapt the learning rate to concept drifts, that commonly occur in streaming environments (ii).
Lastly, we propose a \textit{pre-tuning} approach for effectively optimizing the learning rate of online deep learning models \textit{pre-stream} (see Figure~\ref{fig:lr_overview}) (iii).

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{figures/lr_overview.pdf}
	\caption{Overview of different learning rate optimization approaches.}
	\label{fig:lr_overview}
\end{figure}

\section{Learning Rate in First-Order Optimization}

In the following, we will explain the theoretical background of first-order stochastic optimization enabling modern deep learning models.
% We will also outline the differences between the application of these techniques in traditional batch learning and online learning in terms of impact of the learning rate and its optimization.

First-order stochastic optimization algorithms like stochastic gradient descent typically aim to solve
\begin{equation}
	\min_{\theta} \mathbb{E}_{x \sim p(x)} [\mathcal{L}(x, \theta)],
\end{equation}
where $\mathcal{L}(x, \theta)$ represents a loss function that quantifies the predictive error of the model given a mini-batch of data samples $x$ and model parameters $\theta$.
The blueprint process of solving this problem via first order stochastic optimization consists of the following steps for each iteration $t \in 0, \ldots, T$:
\begin{enumerate}
	\item Draw a mini-batch of samples $x_t$ from distribution $p(x)$.
	\item Calculate the loss $\mathcal{L}_t = \mathcal{L}(x_t, \theta_t)$ for $x_t$ and current parameters $\theta_t$.
	\item Compute the gradient $g_t = \nabla_{\theta_t} \mathcal{L}_t$ with respect to the parameters.
	\item Update the parameters for the next iteration using $g_t$ and potentially information from past iterations.
\end{enumerate}

For basic SGD, we can define the parameter update performed at the end of each iteration as $\theta_{t}  = \theta_{t} - \eta_t \cdot g_t$,
% \begin{equation}
% \theta_{t}  = \theta_{t} - \eta_t \cdot g_t,
% \end{equation}
where $\eta_t$ denotes the step size or \textit{learning rate} at timestep $t$.

The primary trade-off with respect to $\eta$ is that increasing it speeds up convergence, but also increases stochasticity and the risk of divergence~\cite{bengioPracticalRecommendationsGradientbased2012}.
In fact, \textcite{smithBayesianPerspectiveGeneralization2018}, found that the “noise scale” of SGD is directly tied to $\eta$~\cite{smithBayesianPerspectiveGeneralization2018}.

\subsection{Learning Rate Schedules}

Often times, the performance of a model can be improved by using a schedule that changes the learning rate as training progresses~\cite{wuDemystifyingLearningRate2019b}.
For example, to ensure fast convergence early in training while mitigating jumping around potential minima later, it is common to use a decaying schedule that starts with a large learning rate and decreases over time.
An additional benefit of this approach is potentially better generalization, since larger learning rates can help skip sharp minima with poor generalization~\cite{hochreiterFlatMinima1997,chaudhariEntropySGDBiasingGradient2017}.

Commonly used forms of decay are exponential decay, where $\eta_{t}$ calculates as
$\eta_{t} = \eta_0 \cdot \gamma^t$,
with $\gamma < 1$, and stepwise decay, which for a regular interval between steps of length $s$ is given as $\eta_0 \cdot \gamma^{\lfloor t/s \rfloor}$.
Another common approach involves decaying $\eta$ every time the training loss plateaus for a set number of iterations.

Other popular schedules include cyclic learning rates which oscillate $\eta$ between two values over a predefined interval.
For a triangular cycle, the learning rate calculates as
\begin{equation}
	\eta_t = \eta_0 + \frac{\hat{\eta} - \eta_0}{2s} \cdot \min_{i} \{|t-i\cdot s|\},
\end{equation}
with $\hat{\eta}$ being the learning rate at the middle of each cycle of length $s$.
Some studies~\cite{smithCyclicalLearningRates2017, smithSuperConvergenceVeryFast2018a} have found cyclic schedules to significantly speed up the convergence of neural networks even when compared to adaptive techniques like Adam in some cases~\cite{kingmaAdamMethodStochastic2017b}.
While there are many alternatives, in this work we focus on  exponential, stepped and cyclic learning rates, as some of the most commonly used generic schedules.
For a comprehensive overview and detailed analysis on learning rate policies, refer to \textcite{wuDemystifyingLearningRate2019b}.

\subsection{Adaptive Learning Rates}

% Although determining the learning rate through a separate tuning phase with parameter searches like grid- or random-search is still the de facto standard in deep learning~\cite{defazioLearningRateFreeLearningDAdaptation2023a}, this approach causes significant computational overhead.

Various studies have proposed \textit{adaptive optimizers} that increase the robustness of the training process with respect to the learning rate.
These optimizers adjust the step size based on previous gradients at each optimization step~\cite{duchiAdaptiveSubgradientMethods2011}.
One of the earlier techniques in this category is \textit{AdaGrad}~\cite{duchiAdaptiveSubgradientMethods2011}, which scales the learning rate based on the sum of squares of past gradients for each parameter, resulting in a parameter specific step size.
% Parameter specific learning rates not only influence the length, but also the direction of update steps, in case of AdaGrad by shifting updates in the direction of smaller gradients~\cite{wuWNGradLearnLearning2020}. % Weglassen? 
Several other approaches like AdaDelta~\cite{zeilerADADELTAAdaptiveLearning2012a} and RMSProp~\cite{tielemanLecture5rmspropDivide2012}, subsequently built on AdaGrad's scaling approach.
The same applies to the commonly used Adam optimizer~\cite{kingmaAdamMethodStochastic2017b}, that additionally takes a momentum term of past gradients into account to speed up the convergence for parameters with consistent derivatives.
Another optimizer building on AdaGrad is \textit{WNGrad}~\cite{wuWNGradLearnLearning2020}, which adaptively scales each parameter update based on the squared sum of past gradients.

So called parameter-free variants of SGD aim to eliminate the learning rate entirely by optimizing it as training progresses.
% One of the earlier works on parameter-free optimization~\cite{schaulNoMorePesky2013} proposed \textit{vSGD}, which, like Adam, uses first and second order moments of the gradients as well as local curvature information to estimate $\eta$~\cite{schaulNoMorePesky2013}.
% Due to the age and complexity of vSGD compared to similar approaches, we did not include it in our evaluations.
For example, the \textit{COCOB} algorithm~\cite{orabonaTrainingDeepNetworks2017} models parameter optimization as a gambling problem, where the goal is to maximize the rewards from betting on each gradient.
The resulting strategy corresponds to running a meta-optimization algorithm that estimates the expected optimal learning rate~\cite{orabonaTrainingDeepNetworks2017}.
Several other contributions~\cite{vanervenMetaGradMultipleLearning2016a,baydinOnlineLearningRate2018,cutkoskyMechanicLearningRate2023} have also used the idea of learning $\eta$ via a meta-optimization process.
The \textit{hypergradient descent} (HD) approach~\cite{baydinOnlineLearningRate2018} for instance adapts the learning rate of a base optimizers like SGD using a meta-gradient descent procedure, although this does not remove the learning rate completely but replaces it with a less sensitive hypergradient step size.
Mechanic~\cite{cutkoskyMechanicLearningRate2023} pursues the same goal by applying a meta \textit{online convex optimization} (OCO) algorithm to an arbitrary base optimizer.

Research has shown that in an OCO problem setting with stationary data, the worst-case optimal fixed step size for SGD is
\begin{equation}\label{eq:oco_optimal_lr}
	\eta^* = \frac{||\theta_0 - \theta^*||}{\sqrt{\sum_{t=0}^{T} ||g_t||^2}}.
\end{equation}
Multiple parameter-free optimizers, make use of this notion.
As its name suggests, the \textit{Distance over Gradients} (DoG)~\cite{ivgiDoGSGDBest2023} algorithm estimates the unknown numerator in Equation~\ref{eq:oco_optimal_lr} as the maximum distance $\max_{i<t}||\theta_0 - \theta_i||$ between the initial parameters and the parameters of all previous iterations.
% \begin{equation}
% 	\max_{i<t}||\theta_0 - \theta_i||.
% \end{equation}
DoG additionally makes use of polynomial decay averaging as proposed by~\textcite{shamirStochasticGradientDescent2012}.
\textit{D-Adaptation} by \textcite{defazioLearningRateFreeLearningDAdaptation2023a} on the other hand employs weighted dual averaging~\cite{duchiDualAveragingDistributed2012} to calculate bounds on the distance between initial and optimal parameters.
Although adaptive optimization techniques seem intuitively well-suited for non-stationary data, their application on data streams has rarely been investigated.
Therefore, we assess the suitability of some of the most prominent adaptive optimizers, listed in Table~\ref{tab:param_free_optims} for stream-based learning.

\begin{table}[ht]
	\centering
	\scriptsize
	\caption{Overview of additional time- and space-complexity of evaluated adaptive first-order optimizers compared to basic SGD. Values are given in big O notation with respect to the number of model parameters $D$.}\label{tab:param_free_optims}
	\begin{tabular}{@{}lllcc@{}}
		\toprule
		Optimizer                & Runtime            & Space             & Param. specific & LR free \\ \midrule
		AdaGrad                  & $\mathcal{O}(5D)$  & $\mathcal{O}(1D)$ & \cmark          & \xmark  \\
		Adam                     & $\mathcal{O}(12D)$ & $\mathcal{O}(2D)$ & \cmark          & \xmark  \\
		WNGrad                   & $\mathcal{O}(2D)$  & $\mathcal{O}(0)$  & \xmark          & \xmark  \\
		COCOB                    & $\mathcal{O}(14D)$ & $\mathcal{O}(4D)$ & \cmark          & \cmark  \\
		HD \footnotemark[1]      & $\mathcal{O}(2D)$  & $\mathcal{O}(1D)$ & \xmark          & \xmark  \\
		Mechanic                 & $\mathcal{O}(10D)$ & $\mathcal{O}(1D)$ & \cmark          & \cmark  \\
		DoG \footnotemark[1]     & $\mathcal{O}(5D)$  & $\mathcal{O}(1D)$ & \xmark          & \cmark  \\
		D-Adapt \footnotemark[1] & $\mathcal{O}(6D)$  & $\mathcal{O}(2D)$ & \xmark          & \cmark  \\
		% vSGD      & $\mathcal{O}(21D)$\footnotemark & $\mathcal{O}(4D)$ & \cmark          & \cmark  \\ % Remove since not evaluated?
		\bottomrule
	\end{tabular}
\end{table}
\footnotetext[1]{Variant with SGD as the base algorithm.}

\section{Learning Rate in Online Learning}

In a batch learning setting, optimizing the learning rate involves minimizing the expected loss on a hold-out set of validation data.
Formally, we can denote this task as
\begin{equation}
	\label{eq:batch_lr_optim}
	\min_{\eta_0, \ldots, \eta_T} \sum_{i=0}^V \mathcal{L}(x_i, \theta_T),
\end{equation}
% \begin{equation}
% 	\label{eq:batch_lr_optim}
% 	\min_{\eta_0, \ldots, \eta_T} \sum_{\forall x \in V} \mathcal{L}(x, \theta_T),
% \end{equation}
where all $x_i$ are part of a separate validation dataset and $\theta_T$ are the parameters at the end of training.
In online learning where data is generated incrementally, this notion of learning rate optimization is infeasible.
Due to requirements \textbf{R1-R5}, models operating in an online learning environment should be evaluated in a \textit{prequential} manner~\cite{bifetMOAMassiveOnline2010}, where each sample $x_t$ in the data stream is first used to test and then to train the model ensuring testing is done on previously unobserved data.

Training in such a scenario can be more accurately modeled as an online convex optimization problem~\cite{shalev-shwartzOnlineLearningOnline2011,hazanIntroductionOnlineConvex2016}, where the optimizer suffers a loss $\mathcal{L}(x_t, \theta_{t})$ and produces updated parameters $\theta_{t+1}$ at each iteration of the data stream.
Learning rate optimization in this setting can be formulated as
\begin{equation}
	\label{eq:stream_lr_optim}
	\min_{\eta_0, \ldots, \eta_T} \sum_{t=0}^{T} \mathcal{L}_t(x_t,\theta_t).
\end{equation}

Compared to Equation~\eqref{eq:batch_lr_optim}, Equation~\eqref{eq:stream_lr_optim} features some key differences.
Due to the requirement of being able to predict at any time (\textbf{R4}), the goal is to minimize the total sum of losses over all timesteps of the prequential evaluation process, instead of the validation loss for the final parameters $\theta_T$.
% This means that the loss suffered at every timestep of the stream contributes equally to the objective.
Therefore, the speed of convergence is more important in the streaming setting, while the performance of the final $\theta_T$ parameter has a much smaller impact.
Since memory is limited (Requirement~\ref{rq:limited_memory}), it is also not possible to continue training on previously observed data as long as the loss decreases, which puts an even greater emphasis on quick adaptation.
At the same time, a larger learning rate causing temporary loss increases, due to skipping over local minima can be suboptimal with respect to Equation~\eqref{eq:stream_lr_optim} even if it eventually yields a lower loss.
Another difference to conventional batch learning is that the loss function $\mathcal{L}_t$ is time dependent, due to the fact that data streams are commonly subjected to distributional change in the form of so called \textit{concept drift} over time.
% Under such circumstances, the optimal parameter values $\theta^*$ move throughout the progression of the stream requiring the model parameters to adapt.

\subsection{Learning Rate Tuning}\label{subsec:pre-tuning}

% Tuning the learning rate of an online machine learning model is a challenging task owing to the possibility of concept drift that may cause the data stream to move away from the distribution used for tuning the model.
The previously described differences in evaluation schemes may cause conventional learning rate tuning to produce unsuitable results for stream-based learning.
We therefore propose a modified tuning approach, approximating Equation~\eqref{eq:stream_lr_optim}, which we call learning rate \textit{pre-tuning} in the following.

To emulate the targeted data stream we continually draw samples with replacement from the tuning data in a bootstrapping procedure instead of training on all data for multiple epochs.
By doing so we aim to increase data variability, and therefore the resemblance to an actual data stream.
We then optimize the learning rate with respect to the mean prequential performance over the emulated stream using an arbitrary parameter search technique.
We provide a detailed experimental evaluation of our approach in Section~\ref{sec:experiments}.

\subsection{Concept Drift Adaptation}

Concept drift requires the model parameters to repeatedly adapt.
When interpreting the post-drift training as a new online optimization problem, the worst-case optimal learning rate can be calculated according to Equation~\ref{eq:oco_optimal_lr} substituting the initial parameter values $\theta_0$ with the values at the time of drift onset $\theta_{t_d}$.
As a result, more severe drifts, causing $\theta^*$ to move away from $\theta_{t_d}$, may benefit from larger learning rates.

Based on this notion, we propose a simple adaptation to decaying learning rate schedules that resets $\eta$ to its original value if a concept drift has been detected.
An exponential schedule modified with our approach therefore yield learning rates
\begin{equation}
	\eta_t = \eta_0 \cdot \gamma^{t-t_d},
\end{equation}\label{eq:drift_reset}
where $t_d$ marks the timestep in which drift was last detected.
For the purpose of drift detection we apply ADWIN~\cite{bifetLearningTimeChangingData2007} to the prequential losses.
To avoid mistakenly detecting drops in loss as concept drifts, we use a one-tailed ADWIN variant that tests only for increases.

Our approach is similar to some \textit{forgetting mechanisms} commonly employed in conventional non-deep online learning~\cite{gamaSurveyConceptDrift2014}.
To improve model plasticity, such mechanisms partly or completely reset the current model parameters to their initial values.
However, we hypothesize that this approach is not well suited for deep learning purposes.
The reason for this is that, under the assumption of convexity, the newly initiated parameters must be closer to the optimal parameters $\theta^*$ than the current parameters to be beneficial.
We experimentally compare our approach with this weight-resetting mechanism in Section~\ref{sec:experiments}.

\section{Experiments}\label{sec:experiments}

We empirically evaluate our hypotheses using the following setup\footnote[2]{Code available at \url{anonymous.4open.science/r/LODL-D458/}.}:

We use both synthetic and publicly available real-world classification datasets with different sizes and types of concept drift, listed in Table~\ref{tab:datasets}.
Our evaluations include two synthetic \textit{Random Radial Basis Function} (RBF) datasets that we manipulated to incorporate concept drift using the online learning framework \textit{River}~\cite{montiel2021river}.
We further employ the \textit{Electricity} and \textit{Covertype}~\cite{misc_covertype_31} datasets, which are commonly used to evaluate online learning models, as well as two Insects datasets~\cite{souzaChallengesBenchmarkingStream2020} with predefined types of concept drift.
\textit{Covertype} is accessible through the \textit{OpenML} Platform~\cite{vanschorenOpenMLNetworkedScience2014}, while the remaining datasets are part of River.

As a model, we use an MLP with a single hidden layer whose number of units is equal to number of input features implemented in \textit{PyTorch}~\cite{paszkePyTorchImperativeStyle2019}.
This choice is based on our experience that smaller models exhibit faster convergence and are therefore usually best suited for streaming environments.

We tune the base learning rate $\eta_0$ of all but the parameter-free approaches using a grid search of ten geometrically spaced values and configure adaptive optimizers with their default parameter values. For HD, Mechanic and D-Adaptation we select standard SGD as the base algorithm.
We select a fixed factor $\gamma$ for decay schedules on all datasets.
For the proposed learning rate resetting mechanism, we select a smaller decay factor and set the confidence level $\delta$ for drift detection to $10^{-4}$.
% For more details on our hyperparameter setup, please refer to Appendix~\ref{app:hyperparams}.
\begin{table}[t]
	\centering
	\scriptsize
	\caption{Datasets used for experimental evaluations.}
	\begin{tabular}{@{}clcccc@{}}
		\toprule
		Type                    & Data Stream               & Instances & Features & Classes \\
		\midrule
		\multirow{2}{*}{Synth.} & RBF abrupt                & 20000     & 20       & 5       \\
		                        & RBF incremental           & 20000     & 20       & 5       \\
		\midrule
		\multirow{5}{*}{Real}   & Insects abrupt            & 52848     & 33       & 6       \\
		                        & Insects gradual           & 24150     & 33       & 6       \\
		                        & Covertype\footnotemark[3] & 100000    & 54       & 7       \\
		                        & Electricity               & 45312     & 8        & 2       \\
		\bottomrule
	\end{tabular}
	\label{tab:datasets}
\end{table}
\footnotetext[3]{We use the first 100k from a total of 581k examples only.}
For our evaluations we process each dataset sequentially, emulating streams of mini-batches with four instances each, while recording the prequential accuracy and other metrics in intervals of 25 iterations.
We report our results averaged over five random seeds.
% Since the prequential binary crossentropy used for training occasionally produced large outliers, we focus on the classification accuracy as a performance metric.

\subsection{Learning Rate Schedules}

\begin{table*}[ht]
	\centering
	\scriptsize
	\caption{Average prequential accuracy [\%] for static and drift adaptive learning rate schedules with SGD. For LR-Reset Oracle we manually reset the learning rate at timesteps where concept drift occurs. Best values are shown in \textbf{bold}, values within the $1\sigma$ interval of best values \underline{underlined}.}
	\begin{tabular}{llccccccc}
		\toprule
		                                                  & Schedule        & RBF abrupt            & RBF incr.           & Covertype             & Electricity           & Insects abrupt        & Insects gradual       \\
		\midrule
		\multirow{4}{*}{\rotatebox[origin=c]{90}{Static}} & Fixed           & \underline{94.79±.32} & 70.95±2.89          & \underline{83.42±.50} & \underline{73.77±.40} & 71.50±.08             & 75.31±.21             \\
		                                                  & Step            & \bfseries 94.87±.28   & 70.19±3.02          & 82.89±.37             & \underline{73.62±.53} & \bfseries 72.23±.27   & \underline{75.83±.21} \\
		                                                  & Cyclic          & \underline{94.79±.32} & \bfseries 74.96±.86 & \bfseries 83.44±.08   & 68.38±.81             & 71.74±.39             & 75.64±.06             \\
		                                                  & Exponential     & \underline{94.85±.29} & 70.23±2.40          & 82.95±.26             & \underline{73.51±.48} & \underline{72.19±.37} & \bfseries 75.91±.14   \\ \midrule
		\multirow{3}{*}{\rotatebox[origin=c]{90}{Adapt.}} & Weight-Reset    & 69.96±.38             & 65.13±.80           & 83.12±.13             & 70.08±1.66            & 51.52±.90             & 62.55±2.34            \\
		                                                  & LR-Reset (Ours) & \underline{94.83±.26} & 73.38±2.32          & 82.99±.20             & \bfseries 73.79±.62   & 71.73±.20             & 75.52±.12             \\
		                                                  & LR-Reset Oracle & 95.12±.21             & ---                 & ---                   & ---                   & 71.88±.26             & ---                   \\
		\bottomrule
	\end{tabular}
	\label{tab:lr_resetting}
\end{table*}

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.47\textwidth}
		\includegraphics[width=\textwidth]{figures/lr_norms_schedules_insects_abrupt.pdf}
		\caption{Schedules}
		\label{fig:prequential_schedulers_insects}
	\end{subfigure}
	\begin{subfigure}[b]{0.47\textwidth}
		\includegraphics[width=\textwidth]{figures/lr_norms_optims_insects_abrupt.pdf}
		\caption{Adaptive Optimizers}
		\label{fig:prequential_optims_insects}
	\end{subfigure}
	\caption{Prequential accuracy and learning rate for different schedules and adaptive optimizers on \textit{Insects abrupt} dataset. Concept drifts are marked by dashed lines. Accuracy is exponentially smoothed with a decay factor of 0.75.}
\end{figure}

To evaluate the effectiveness of our learning rate resetting mechanism for drift adaptation (see Equation~\eqref{eq:drift_reset}), we compare its average prequential accuracy to that of model weight resetting, commonly used in online learning.

As can be seen in Table~\ref{tab:lr_resetting}, our approach clearly outperforms weight-resetting on all but \textit{Covertype}, but rarely yields an advantage over a static schedule.
This is also reflected in Figure~\ref{fig:prequential_schedulers_insects}, where the standard exponential schedule's accuracy initially rises faster and recovers faster after the first concept drift, which is likely caused by its larger step size in the first half of the stream.
Since the oracle variant of our resetting approach, that was triggered only for timesteps with drift, shows only marginally better results, this performance gap is not caused by the drift detector.
Rather, it appears that using a larger initial learning rate and slower decay is sufficient for assuring adequate adaptability to concept drift throughout a data stream while granting better stability at later stages.
Overall, a slower but static decay paired with a larger initial learning rate seems to be preferable over a more aggressive schedule with our drift resetting mechanism, unless severe concept drift as in \textit{RBF incremental} is expected.

With accuracy values within the $1\sigma$ interval of one another on all evaluated streams, step-wise decay displays almost identical performance to exponential decay.
The cyclic schedule's accuracy for \textit{RBF incremental} and \textit{Covertype} on the other hand significantly exceeded that of the other static and adaptive schedules but lacks behind on all other streams.
We also did not find improvements in convergence speed by an order of magnitude as observed by~\cite{smithSuperConvergenceVeryFast2018a} for the investigated scenario.

\subsection{Adaptive Learning Rates}

\begin{table*}[ht]
	\centering
	\scriptsize
	\caption{Average prequential accuracy [\%] for adaptive optimizers and SGD. Best values are shown in \textbf{bold}, values within the $1\sigma$ interval of best values \underline{underlined}.}
	\begin{tabular}{llcccccccc}
		\toprule
		                                                   & Optimizer                                                                         & RBF abrupt          & RBF incr.            & Covertype             & Electricity          & Insects abrupt      & Insects gradual     \\
		\midrule
		\multirow{5}{*}{\rotatebox[origin=c]{90}{Tuned}}   & SGD                                                                               & \bfseries 94.79±.32 & 70.95±2.89           & \bfseries 83.42±.50   & 73.77±.40            & 71.50±.08           & 75.31±.21           \\
		                                                   & Adam \cite{kingmaAdamMethodStochastic2017b}                                       & 93.45±.30           & 69.26±5.14           & 79.01±.27             & 69.79±.54            & \bfseries 75.38±.24 & 75.78±.74           \\
		                                                   & AdaGrad \cite{duchiAdaptiveSubgradientMethods2011}                                & 92.45±1.37          & 52.87±6.62           & 81.68±.35             & \bfseries 76.99±1.20 & 74.87±.40           & \bfseries 77.15±.27 \\
		                                                   & WNGrad  \cite{wuWNGradLearnLearning2020}                                          & 87.30±.68           & 44.92±.73            & 76.98±.15             & 70.80±.59            & 66.25±.19           & 66.75±.40           \\
		                                                   & HD \cite{baydinOnlineLearningRate2018}                                            & 93.92±.31           & \bfseries 72.29±2.90 & \underline{83.36±.25} & 73.83±.32            & 70.67±.06           & 73.37±.21           \\ \midrule
		\multirow{4}{*}{\rotatebox[origin=c]{90}{LR-Free}} & COCOB                               \cite{orabonaTrainingDeepNetworks2017}        & \bfseries 93.40±.38 & 63.52±2.70           & 82.27±.46             & \bfseries 84.30±.56  & \bfseries 74.75±.11 & \bfseries 77.00±.05 \\
		                                                   & DoG \cite{ivgiDoGSGDBest2023}                                                     & 92.72±.59           & \bfseries 73.17±2.72 & \bfseries 83.07±.64   & 71.53±.70            & 70.59±.26           & 74.01±.21           \\
		                                                   & D-Adapt                    \cite{defazioLearningRateFreeLearningDAdaptation2023a} & 74.91±4.22          & 45.47±2.75           & 76.69±.79             & 66.03±1.75           & 50.05±11.26         & 48.21±10.62         \\
		                                                   & Mechanic \cite{cutkoskyMechanicLearningRate2023}                                  & 88.94±.58           & 49.26±1.44           & 78.67±.18             & 50.73±7.60           & 55.31±21.47         & 65.80±.53           \\
		\bottomrule
	\end{tabular}
	\label{tab:results_adaptive_optims}
\end{table*}

% To judge the usefulness of adaptive first-order optimization techniques for online learning, we ran prequential evaluations with all methods listed in Table~\ref{tab:param_free_optims}.

From the results for adaptive optimizers displayed in Table~\ref{tab:results_adaptive_optims}, it can be deduced that none of the approaches that require tuning a step size parameter stands out as generally superior for the investigated datasets.
Rather, each of SGD, AdaGrad, and Adam achieve the best accuracy on two of the seven datasets.
This once again underlines the data dependency of the optimizer performance.
However, since SGD yields the best accuracy on \textit{RBF abrupt} but is clearly surpassed on \textit{Insects abrupt}, the type of concept drift does not seem to be significant.

Due to its simplicity and favorable computational efficiency, it appears that standard SGD should be selected out of the non-parameter-free approaches if the characteristics of the targeted data stream are unknown.
The SGD variant of Hypergradient Descent (HD)~\cite{baydinOnlineLearningRate2018} and WNGrad~\cite{wuWNGradLearnLearning2020} on the other hand seem to rarely be optimal choices.

In the category of learning rate free optimizers COCOB~\cite{orabonaTrainingDeepNetworks2017}, outperformed its competitors on all but two datasets.
It comes close to or even exceeds the best tuned approaches in terms of accuracy.
Although yielding lower accuracy on average, DoG also comes within reach of the tuned methods, while offering much better runtime and memory efficiency compared to COCOB (see Table~\ref{tab:param_free_optims}).
Mechanic~\cite{cutkoskyMechanicLearningRate2023} and D-Adaptation~\cite{defazioLearningRateFreeLearningDAdaptation2023a}, which we ran with SGD as the base optimizer performed significantly worse than their competitors on the evaluated streams.

To gain additional insights into the investigated adaptive optimizers, we calculated their effective learning rates as $\frac{||\eta_t||}{\sqrt{D}}$, where $\eta_t \in \mathbb{R}^D$ is the vector of parameter specific learning rates.
The resulting learning rate curves shown in Figure~\ref{fig:prequential_schedulers_insects}, provide an indication regarding the reason for the poor performance of WNGrad and D-Adaption.
Whereas the learning rate of DoG quickly approaches the tuned SGD learning rate, the two parameter-free methods diverge considerably from it.
A possible cause for this could be the higher level of gradient noise introduced by the small mini-batches and concept drift associated with data streams.

Another interesting observation in Figure~\ref{fig:prequential_optims_insects} is that the learning rate of the best performing Adam features spikes for most change points, suggesting some form of adaptability to drift.
Since the much worse performing Mechanic shows similar spikes, this is however unlikely to be a significant contributing factor to Adam's high accuracy on \textit{Insects abrupt}.
Instead, it likely stems from its second moment scaling, which is also part of the similarly performing AdaGrad.

It may also be noted that the learning rates of the COCOB, Adam and Mechanic optimizers with parameter specific learning rates exceed those of single value step sizes by multiple orders of magnitude.
This is an effect of second moment scaling, which creates larger learning rates for parameters with small and consistent gradients~\cite{cutkoskyMechanicLearningRate2023}.
Therefore, the parameter updates generated by these approaches are not necessarily larger.

\subsection{Learning Rate Tuning}

\begin{figure}[ht]
	\centering
	\includegraphics[width=.45\textwidth]{figures/pretune_1x64_acc_lr_exp_schedule.pdf}
	\caption{Pre-tuned LR (LR that maximizes accuracy on pre-tuning data) and resulting accuracy on data streams when using SGD and an exponential learning rate schedule with 500 or 1000 separate tuning samples. Results are averaged over all real-world datasets. The shaded area represents the 1$\sigma$-interval.}\label{fig:pretune_lr_accuracy}
\end{figure}

We assessed our suggested approach of pre-tuning learning rates for MLPs with 64 or 128 hidden units per layer and either one or three hidden layers.
In the following part, we focus on the smallest network that is the most representative for streaming applications.
The results of all architectures are reported in Appendix~\ref{app:pretune_model_sizes}.

Prequential evaluation runs were performed with a range of initial learning rates $\eta_0$ and exponential decay factors $\gamma$.
From each data stream, we select a subset of either 500 or 1000 instances at the beginning by holding them out from the remaining data.
In the pre-tuning process, we then bootstrap samples from the held out data to emulate a stream and determine the learning rate yielding the best mean prequential accuracy at each step.
We use accuracy instead of loss as the selection criterion, because the binary crossentropy employed for training commonly decreases by orders of magnitude throughout optimization.
A selection based on the prequential loss would therefore only consider the initial iterations of the tuning process.

Figure~\ref{fig:pretune_lr_accuracy} shows the learning rate resulting from the pre-tuning process at each tuning step averaged over all real-world datasets.
The bottom row of the figure displays the mean accuracy achieved when using this learning rate on the remaining data stream not used for tuning.

After overshooting initially, the tuned learning rate converges in the vicinity of optimal learning rate after 1000 iterations.
While tuning with 1000 samples yields a better approximation of the optimal value, both subset sizes on average achieve significantly better learning rates for more than 1000 pre-tuning steps, than conventional tuning performed with 800 training and 200 validation samples.
This is also reflected in the accuracy scores, which exceed conventional tuning after 500 iterations.
Our approach achieves notably higher accuracy than DoG, which we selected as a baseline due to being best performing parameter-free optimizer with a global learning rate.
This is the case despite deviating further from the optimal learning rate, likely due to excessively low learning rates having a larger impact than excessively high ones in the evaluated scenario.

In conclusion, our proposed tuning approach enables significantly better learning rate selection for prequential evaluation on data streams compared to both conventional tuning and DoG.
Additionally, pre-tuning has the benefit that once completed, no additional memory or runtime costs are incurred.
In streaming applications, where computing resources are often times a limiting factor, this could be a critical advantage.
Although, if computational efficiency is insignificant, the highly performant but expensive COCOB~\cite{orabonaTrainingDeepNetworks2017} or the slightly less performant and much less expensive DoG~\cite{ivgiDoGSGDBest2023} may be more appropriate.


\section{Conclusion}

In this work, we investigate the influence and selection of the learning rate and optimization procedure with respect to deep learning in streaming environments.
We first provide theoretical background on discrepancies between learning rate optimization in conventional batch learning and online learning.
Based on these differences, we derive a simple mechanism resetting the learning rate on concept drift occurrences.
We then give an overview learning rate free algorithms popular in batch learning, which we compare experimentally on multiple synthetic and real-world datasets,
finding both COCOB and DoG to come close to the performance of optimizers with tuned learning rates.
Lastly, we introduce a streaming specific learning rate tuning approach that grants significant performance increases over conventional tuning via a train-validation split.


\printbibliography

\end{document}
