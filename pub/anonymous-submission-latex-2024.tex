\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}

% User imported packages
\usepackage{multirow}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cleveref}
\usepackage{todonotes}
\usepackage{varwidth}
\usepackage{tikz}
\usepackage{graphicx}
\newcommand{\cmark}{\ding{51}} % checkmark symbol
\newcommand{\xmark}{\ding{55}} % X-mark symbol

\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\iffalse\title{AAAI Press Anonymous Submission\\Instructions for Authors Using \LaTeX{}}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1900 Embarcadero Road, Suite 101\\
    Palo Alto, California 94303-3310 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}
\fi

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse\title{My Publication Title --- Single Author}
\author{
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{Learning Rate Optimization in Online Deep Learning}
\author{
    % Authors
    Lucas Cazzonelli\textsuperscript{\rm 1},
    Cedric Kulbach\textsuperscript{\rm 2},
}
\affiliations{
    % Affiliations
    \textsuperscript{\rm 1}FZI Research Center for Information Technology\\
    \textsuperscript{\rm 2}Affiliation 2\\
    cazzonelli@fzi.de, secondAuthor@affilation2.com
}


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle


\begin{abstract}
	\noindent Efficient training via gradient-based optimization techniques is an essential building block to the success of deep learning. Extensive research on the impact and the effective estimation of an appropriate learning rate has partly enabled these techniques. Despite the proliferation of data streams generated by IoT devices, digital platforms, etc., previous research has been primarily focused on batch learning, which assumes that all training data is available a priori. However, characteristics such as the gradual emergence and non-stationarity of data pose additional challenges. Therefore, the findings on batch learning may not be applicable to deep learning in streaming environments. In this work, we seek to address this knowledge gap by (i) evaluating and comparing typical learning rate schedules and optimizers, (ii) exploring adaptations of these techniques, and (iii) providing insights into effective learning rate tuning in the context of stream-based deep learning.
\end{abstract}

\section{Introduction}
Deep learning models have demonstrated exceptional performance in various domains.
One of the main factors leading to such outstanding results is the choice of the optimization method used to train the target model.
Nearly all modern deep learning applications, use first-order stochastic optimization methods like \text{stochastic gradient descent}, which iteratively update the parameters of the underlying model based on gradient information, for this purpose.
One of the most important variables of such algorithms is the step size or \textit{learning rate} (LR).
This applies arguably even more so to online learning in data stream environments, since in such circumstances training cannot simply be repeated if the learning rate proves to be suboptimal.

According to \citet{bifetMOAMassiveOnline2010} an online learning model operating on data streams must be able to
\begin{center}
	\begin{varwidth}{0.5\textwidth}
		\begin{description}
			\item[R1:] process a single instance at a time,\label{rq:single_instance}
			\item[R2:] process each instance in a limited amount of time,\label{rq:limited_time}
			\item[R3:] use a limited amount of memory,\label{rq:limited_memory}
			\item[R4:] predict at any time,\label{rq:predict_any_time}
			\item[R5:] adapt to changes in the data distribution.\label{rq:adapt_to_drift}
		\end{description}
	\end{varwidth}
\end{center}

These requirements introduce additional challenges when it comes to the selection of an appropriate optimizer and learning rate compared to a batch learning scenario where all training data is assumed to be available a priori.

Despite the increasing prevalence of data streams, this issue has received little attention in research.
In this paper, we therefore provide insight into the key differences between batch and online learning in light of the learning rate in first-order optimization based on which we provide the following contributions:

\begin{enumerate}
	\item An in-depth experimental comparison between learning rate schedules and optimizers with adaptive learning rates on synthetic and real-world data streams.
	\item An investigation into the adaptation of the learning rate to drifting data.
	\item An effective approach for learning rate tuning for deep learning models operating in streaming environments.
\end{enumerate}

\begin{figure}
	\centering
	\includegraphics[width=0.474\textwidth]{figures/lr_overview.pdf}
	\caption{Overview of different learning rate optimization approaches.}
	\label{fig:lr_overview}
\end{figure}

\section{The Learning Rate in First-Order Optimization}\todo{Better stochastic or first-order Optimization? Or both? }

In the following, we will explain the theoretical background of first-order stochastic optimization enabling modern deep learning models.
We will also outline the differences between the application of these techniques in traditional batch learning and online learning in terms of impact of the learning rate and its optimization.

First-order stochastic optimization algorithms like stochastic gradient descent typically aim to solve
\begin{equation}
	\min_{\theta} \mathbb{E}_{x \sim p(x)} [\mathcal{L}(x, \theta)],
\end{equation}
where $\mathcal{L}(x, \theta)$ represents a loss function that quantifies the predictive error of the model given a mini-batch of data samples $x$ and model parameters $\theta$.
The blueprint process of solving this problem via first order stochastic optimization consists of the following steps for each iteration $t \in 0, \ldots, T$:
\begin{enumerate}
	\item Draw a mini-batch of samples $x_t$ from distribution $p(x)$.
	\item Calculate the loss $\mathcal{L}_t = \mathcal{L}(x_t, \theta_t)$ for $x_t$ and current parameters $\theta_t$.
	\item Compute the gradient $g_t = \nabla_{\theta_t} \mathcal{L}_t$ with respect to the parameters.
	\item Update the parameters for the next iteration using $g_t$ and potentially information from past iterations.
\end{enumerate}

For basic SGD, we can define the parameter update performed at the end of each iteration as
\begin{equation}
	\theta_{t}  = \theta_{t} - \eta_t \cdot g_t,
\end{equation}
where $\eta_t$ denotes the step size or \textit{learning rate} at timestep $t$.

As previously described, the learning rate has an immense impact on the performance of the optimization process and therefore on the performance of a deep learning model as a whole.

The primary trade-off to consider with respect to the choice of $\eta$ is that increasing the learning rate speeds up convergence, but at the same time also increases stochasticity and therefore leads to the divergence of the training criterion beyond a certain threshold.~\cite{bengioPracticalRecommendationsGradientbased2012}.
In biological terms, increasing the learning rate increases plasticity, whereas decreasing it increases stability.
In fact, \citet{smithBayesianPerspectiveGeneralization2018}, found that when modelling SGD as a stochastic differential equation, the “noise scale” is directly tied to $\eta$~\cite{smithBayesianPerspectiveGeneralization2018}.

\subsection{Learning Rate Schedules}

While using a single fixed learning rate $\eta_t = \eta$ for all iterations simplifies the learning selection and can often yield sufficient performance, results can generally be improved with a schedule with step sizes specific to each iteration~\cite{wuDemystifyingLearningRate2019b}.
To ensure fast convergence at the start of training, while mitigating jumping around potential minima at later stages it is, for instance, common to use a decaying schedule starting with a large learning rate that decreases over time.
An additional benefit of this approach is that of potentially better generalization, since larger learning rates can help skipping over sharp minima with poor generalization~\cite{hochreiterFlatMinima1997,chaudhariEntropySGDBiasingGradient2017}.
Some have likened this procedure to simulated annealing, which shifts its focus from exploration at high temperatures to exploitation once temperatures have sufficiently decreased~\cite{smithDonDecayLearning2018}.

Commonly used forms of decay are exponential decay, where $\eta_{t}$ calculates as
\begin{equation}
	\eta_{t} = \eta_0 \cdot \gamma^t,
\end{equation}
with $\gamma < 1$, and stepwise decay, which for a regular interval between steps of length $s$ is given as
\begin{equation}
	\eta_{t} = \eta_0 \cdot \gamma^{\lfloor t/s \rfloor}.
\end{equation}
Another common approach involves decaying the learning rate every time the training loss plateaus for a set number of iterations.

Other popular schedules include cyclic learning rates which oscillate $\eta$ between two values over a predefined interval.
For a basic triangular cycle, the learning rate calculates as
\begin{equation}
	\eta_t = \eta_0 + \frac{\hat{\eta} - \eta_0}{2s} \cdot \min_{i} \{|t-i\cdot s|\},
\end{equation}
with $\hat{\eta}$ being the learning rate at the middle of each cycle of length $s$.
Some studies~\cite{smithCyclicalLearningRates2017, smithSuperConvergenceVeryFast2018a} have found cyclic schedules to significantly speed up the convergence of neural networks even when compared to adaptive techniques like Adam in some cases~\cite{kingmaAdamMethodStochastic2017b}.
While there are many other learning rate schedules, we focus on the use of the three aforementioned schedules within data streaming applications in this work.
For a comprehensive overview and detailed analysis on learning rate policies, we refer to \citet{wuDemystifyingLearningRate2019b}.

\subsection{Adaptive Learning Rates}

While determining the learning rate through a separate tuning phase with parameter searches like grid- or random-search is still the de facto standard in deep learning~\cite{defazioLearningRateFreeLearningDAdaptation2023a}, this approach causes significant computational overhead.

To decrease such computational overhead, various studies have proposed \textit{adaptive optimizers}.
These optimizers adjust the learning rate by considering additional loss landscape information from the previous gradients at each optimization step, enhancing their robustness with respect to the learning rate~\cite{duchiAdaptiveSubgradientMethods2011}.

One of the earlier optimizers in this category is \textit{AdaGrad}~\cite{duchiAdaptiveSubgradientMethods2011}, which scales the learning rate as
\begin{equation}
	\eta_t = \frac{\eta}{\sqrt{\sum_{i=0}^{t} g_i^2}}
\end{equation}
resulting in a parameter specific learning rate.
Unlike a single global value, parameter specific learning rates therefore not only influence the length, but also the direction of update steps, in case of AdaGrad by shifting updates in the direction of smaller gradients~\cite{wuWNGradLearnLearning2020}. % Weglassen? 

Among several other approaches like AdaDelta~\cite[see e.g.]{zeilerADADELTAAdaptiveLearning2012a} and RMSProp~\cite{tielemanLecture5rmspropDivide2012}, \citet{kingmaAdamMethodStochastic2017b} subsequently introduced Adam as an extension of AdaGrad, that additionally takes a momentum term of past gradients into account~\cite[see]{sutskeverImportanceInitializationMomentum2013} to speed up the convergence for parameters with consistent gradients.

Another optimizer building on AdaGrad is \textit{WNGrad}~\cite{wuWNGradLearnLearning2020}, which adaptively scales each parameter update based on the squared sum of past gradients.
By doing so, WNGrad achieves a single, step size robust, learning rate~\cite{wuWNGradLearnLearning2020}.

While adaptive approaches such as AdaGrad and Adam have been shown to reduce the dependence on the learning rate, they often times still require manual tuning~\cite{wuWNGradLearnLearning2020}.
A problem that parameter-free variants of SGD aim to solve by estimating the optimal step size online as training progresses, thus eliminating the learning rate altogether.

In one of the earlier works on parameter-free optimization, \citet{schaulNoMorePesky2013} proposed \textit{vSGD}, which, like Adam, uses first and second order moments of the gradients as well as local curvature information to estimate $\eta$~\cite{schaulNoMorePesky2013}.
The authors obtain the latter by estimating positive diagonal entries of the Hessian with respect to the parameters through a back-propagation formula~\cite{schaulNoMorePesky2013}.
Due to the age and higher complexity of vSGD compared to similar approaches, we did not include it in our evaluations.
% Even though \citet{schaulNoMorePesky2013} demonstrate \textit{vSGD's} robustness to non-stationary data distributions, it has, to the best of our knowledge, not been widely adopted in the online learning space.
% Due to the lack of publicly available implementations of the non-trivial algorithm, we have not been able to evaluate vSGD at the time of writing. \todo{Did not evaluate vSGD since its old and there is no implementation. Is that valid?}

Instead of using curvature information for adapting $\eta$, the \textit{COCOB} algorithm proposed by~\citet{orabonaTrainingDeepNetworks2017} models parameter optimization as a gambling problem, in which the goal is to maximize the rewards obtained from betting on each gradient.
The model parameters are then computed based on the rewards accumulated over all previous timesteps~\cite{orabonaTrainingDeepNetworks2017}.
Intuitively, this corresponds to running a meta optimization algorithm, that estimates the expected optimal learning rate in parallel with the actual parameter optimization process.

Several other contributions~\cite{vanervenMetaGradMultipleLearning2016a,baydinOnlineLearningRate2018,cutkoskyMechanicLearningRate2023} have also used the idea of learning $\eta$ via a meta-optimization process.
The \textit{hypergradient descent} (HD) approach~\cite{baydinOnlineLearningRate2018} for instance adapts the learning rate of a base optimizers like SGD using a meta-gradient descent procedure, although this does not remove the learning rate completely but replaces it with a less sensitive hypergradient step size.
Mechanic~\cite{cutkoskyMechanicLearningRate2023} pursues the same goal using a meta \textit{online convex optimization} (OCO) algorithm.

Research has shown that in an OCO problem setting with stationary data, the worst-case optimal fixed learning rate for SGD is
\begin{equation}\label{eq:oco_optimal_lr}
	\eta^* = \frac{||\theta_0 - \theta^*||}{\sqrt{\sum_{t=0}^{T} ||g_t||^2}}.
\end{equation}

Multiple recently introduced parameter free optimizers, have made use of this result.
As its name suggests, the \textit{Distance over Gradients} (DoG)~\cite{ivgiDoGSGDBest2023} algorithm uses the maximum distance between the initial parameters and the parameters of all previous iterations as a surrogate for the unknown distance between initial and optimal parameters $||\theta^* - \theta_0||$ to compute its learning rate as
\begin{equation}
	\eta^* = \frac{\max_{i<t}||\theta_0 - \theta_i||}{\sqrt{\sum_{t=0}^{T} ||g_t||^2}}.
\end{equation}

\textit{D-Adaptation} by \citet{defazioLearningRateFreeLearningDAdaptation2023a} on the other hand employs weighted dual averaging~\cite{duchiDualAveragingDistributed2012} to calculate bounds on the distance between initial and optimal parameters, often denoted as $D$ and use them to adapt the learning rate of a base optimization algorithm.

Despite the fact that parameter-free stochastic optimization techniques are inherently well-suited for the highly non-stationary streaming data~\cite{schaulNoMorePesky2013} and in some cases even developed based on online convex optimization, their application on data streams has rarely been investigated.
We therefore investigate the suitability of some of the most prominent adaptive optimizers, listed in Table~\ref{tab:param_free_optims} for stream-based learning (ii).

There are also several lesser-known studies that have explored adaptive learning rates in specific application domains of online learning such as time series prediction~\cite{miyaguchiCograConceptDriftAwareStochastic2019, fekriDeepLearningLoad2021, zhangPOLAOnlineTime2021a}, federated learning~\cite{canonacoAdaptiveFederatedLearning2021}, and recommender systems~\cite{ferreirajoseADADRIFTAdaptiveLearning2020}.
However, since we focus on general data stream applications in this paper, we did not investigate these techniques further. \todo{Is this valid?}

\begin{table}[ht]
	\centering
	\small
	\begin{tabular}{@{}lllcc@{}}
		\toprule
		Optimizer               & Runtime            & Space             & Param. specific & LR free \\ \midrule
		AdaGrad                 & $\mathcal{O}(5D)$  & $\mathcal{O}(1D)$ & \cmark          & \xmark  \\
		Adam                    & $\mathcal{O}(12D)$ & $\mathcal{O}(2D)$ & \cmark          & \xmark  \\
		WNGrad                  & $\mathcal{O}(2D)$  & $\mathcal{O}(0)$  & \xmark          & \xmark  \\
		COCOB                   & $\mathcal{O}(14D)$ & $\mathcal{O}(4D)$ & \cmark          & \cmark  \\
		HD \footnotemark[1]     & $\mathcal{O}(2D)$  & $\mathcal{O}(1D)$ & \xmark          & \xmark  \\
		Mechanic                & $\mathcal{O}(10D)$ & $\mathcal{O}(1D)$ & \cmark          & \cmark  \\
		DoG \footnotemark[1]    & $\mathcal{O}(5D)$  & $\mathcal{O}(1D)$ & \xmark          & \cmark  \\
		DAdapt \footnotemark[1] & $\mathcal{O}(6D)$  & $\mathcal{O}(2D)$ & \xmark          & \cmark  \\
		% vSGD      & $\mathcal{O}(21D)$\footnotemark & $\mathcal{O}(4D)$ & \cmark          & \cmark  \\ % Remove since not evaluated?
		\bottomrule
	\end{tabular}
	\caption{Overview of additional time- and space-complexity of evaluated adaptive first-order optimizers compared to basic SGD. Values are given in big O notation with respect to the number of model parameters $D$ and based on pseudo-codes provided in the original works. Due to lack of comparability, we do not list provided convergence guarantees. Note that this is not a comprehensive list but an overview of some of the most prominent algorithms.}\label{tab:param_free_optims}
\end{table}
\footnotetext[1]{Variant with SGD as the base algorithm.}

\section{Differences between Batch and Online Learning}\label{sec:diffs_batch_online}

In a batch learning setting, optimizing the learning rate comes down to finding values that minimize the expected loss on a hold-out set of validation data at the end of the training process.
Formally, we can denote this task as
\begin{equation}
	\label{eq:batch_lr_optim}
	\min_{\eta_0, \ldots, \eta_T} \mathbb{E}_{x \sim p_v(x)}[\mathcal{L}(x, \theta_T)],
\end{equation}
where $p_v$ is a distribution of held-out validation data and $\theta_T$ the parameters at the end of training, which for basic SGD are given by
\begin{equation}
	\theta_T = \sum_{t=0}^{T} \eta_t \cdot g_t.
\end{equation}

In online learning where data is generated incrementally, this notion of learning rate optimization is infeasible.
Due to requirements \textbf{RQ1-RQ5} models operating in an online streaming environment must be evaluated in a \textit{prequential} manner~\cite{bifetMOAMassiveOnline2010}, where each instance $x_t$ in the data stream is first used to test and then to train the model ensuring that testing is done exclusively on unseen data.

Training in such a scenario can therefore be more accurately modeled as an online convex optimization problem~\cite{shalev-shwartzOnlineLearningOnline2011,hazanIntroductionOnlineConvex2016}, where the optimizer suffers a loss $\mathcal{L}_t(\theta_t) = \mathcal{L}(x_t, \theta_{t})$ and produces updated parameters $\theta_{t+1}$ at each iteration of the data stream.

The task of finding an optimal learning rate schedule in this setting, can be formulated as
\begin{equation}
	\label{eq:stream_lr_optim}
	\min_{\eta_0, \ldots, \eta_T} \sum_{t=0}^{T} \mathcal{L}_t(\theta_t).
\end{equation}

Compared to Problem~\eqref{eq:batch_lr_optim}, Problem~\eqref{eq:stream_lr_optim} features some key differences.
Due to Requirement~\ref{rq:predict_any_time}, the goal is to minimize the total sum of losses incurred over all timesteps of the prequential evaluation process, instead of minimizing only the validation loss for the final parameters $\theta_T$.
This means that not only the loss achieved by the final parameters $\theta_T$, but the loss suffered at every timestep of the stream contributes equally to the objective.
Therefore, speed of convergence is of larger importance in the streaming setting, whereas the performance of the final parameters $\theta_T$ has relatively little impact.
Since memory is limited (Requirement~\ref{rq:limited_memory}), it is also not possible to continue training on previously observed data as long as $\mathcal{L}$ decreases, which puts an even greater emphasis on quick adaptation.
At the same time, a larger learning rate causing temporary loss increases, due to skipping over local minima can be suboptimal with respect to Problem~\eqref{eq:stream_lr_optim} even if it eventually yields a lower loss.

Another difference to conventional batch learning is that the loss function $\mathcal{L}_t$ is time dependent, due to the fact that data streams might, and in practice most likely will, be subjected to change in the form of \textit{concept drift}\footnote{We use concept drift as an umbrella term for any form of distributional shift.} over time.
Under such circumstances, the optimal parameter values $\theta^*$ move throughout the progression of the stream requiring the model parameters to adapt.

\subsection{Learning Rate Tuning}\label{subsec:pre-tuning}

Tuning the learning rate of an online machine learning model is a challenging task owing to the possibility of concept drift that may cause data stream to move away from the distribution that was used for tuning the model.
This effect, combined with the previously described differences in the evaluation scheme may cause conventional learning rate tuning to produce unsuitable results for stream-based learning.

We therefore propose a modified online learning specific tuning approach approximating Problem~\eqref{eq:stream_lr_optim}, which we call learning rate \textit{pre-tuning} in the following.

To emulate
% a static version of 
the targeted data stream we continually draw samples with replacement from the tuning data in a bootstrapping procedure instead of training on all data for multiple epochs.
By doing so we aim to increase data variability% better: randomness?
, and therefore the resemblance to an actual data stream with random distributional shifts.
We then optimize $\eta$ with respect to the mean prequential performance over the emulated stream instead of the performance on a validation set.
For this purpose we use a basic grid-search as is customary in batch learning~\cite{defazioLearningRateFreeLearningDAdaptation2023a}.
We provide a detailed experimental evaluation of our approach in Section~\ref{sec:experiments}.\todo{Add pseudo-code of this?}

\subsection{Learning Rate Adaptation}

% Since learning from data streams can be modeled as an online convex optimization problem, a minimax optimal learning rate for this scenario could be estimated using Equation~\ref{eq:oco_optimal_lr}, as is done by some parameter-free optimizers~\cite{ivgiDoGSGDBest2023,cutkoskyMechanicLearningRate2023,defazioLearningRateFreeLearningDAdaptation2023a}.
% -using a fixed learning rate would 
% However, due to the non-stationary nature of data streams, using such a fixed learning rate may be 
% -Abrupt concept drifts can be interpreted as

As described earlier, model parameters in streaming environments must be regularly adjusted to account for concept drift.
To enhance the model's ability to do so, it appears intuitive, to increase the learning rate whenever distributional change occurs.

Based on this notion, \citet{kunchevaAdaptiveLearningRate2008} introduced an adaptive schedule that uses the predictive losses as an indicator for concept drift.
Their approach updates the learning rate using
\begin{equation}
	\eta_{t+1} = \eta_t^{1+	\bar{\mathcal{L}}_{t-M} - \bar{\mathcal{L}}_{t}},
\end{equation}\label{eq:kuncheva_lr}
where $\bar{\mathcal{L}}_{t}$ is the running mean of $M$ previous losses.
By doing so, the authors aim to achieve higher stability, when data is stationary and losses decline and higher adaptability, when data is drifting and losses rise.
While this approach seems intuitively sound, for an initial learning rate $\eta_0 \leq 1$ it bears a high risk of increasing up to a value of 1, since increases in loss caused by an excessive learning rate would lead to a feedback loop.
Furthermore, loss plateaus that could be avoided by lowering $\eta$ would instead cause $\eta$ to remain stable, diminishing performance.

To offer the same potential benefits as \citet{kunchevaAdaptiveLearningRate2008} approach while addressing its fundamental issues, we propose a simple adaptation to decaying learning rate schedules that resets $\eta$ to its original value if a concept drift has been detected.
An exponential schedule modified with our approach therefore yield learning rates
\begin{equation}
	\eta_t = \eta_0 \cdot \gamma^{t-t_d},
\end{equation}\label{eq:drift_reset}
where $t_d$ marks the timestep in which drift was last detected.
As a result, feedback-loops are avoided assuming $\eta_0$ is small enough to not cause divergence and $\eta_t$ can also decay in the presence of loss plateaus.

For the purpose of drift detection we apply ADWIN~\cite{bifetLearningTimeChangingData2007} to the prequential losses.
To avoid mistakenly detecting drops in loss as concept drifts, we use a one-tailed ADWIN variant that tests only for increases.

% We further hypothesize, that our approach could also be beneficial to the performance of adaptive optimizers like Adam, that make use of momentum terms.
% As Figure~\ref{fig:adam_reset}, which shows the trajectory of Adam for a logistic regression toy problem, demonstrates, if an abrupt concept drift occurs, momentum accumulated over the previous concept can cause the parameters to continue their previous trajectory for some time.
% When resetting the optimizers state once the drift occurs on the other hand, the parameters immediately begin moving towards the new minimum.
% This results in considerably better expected prequential losses, as shown by the marker colors.

Our approach is similar to some \textit{forgetting mechanisms}~\cite{gamaSurveyConceptDrift2014} commonly employed in conventional non-deep online learning, which improve model plasticity by partly~\cite{bifetAdaptiveLearningEvolving2009} or resetting the current model's parameters to their initial values.
However, we hypothesize that this approach is not well suited for deep learning purposes.
The reason for this is that, under the assumption of convexity, it requires that the newly initiated parameters be closer to the optimal parameters $\theta^*$ than the current parameters to be beneficial.
For all but the most severe drifts, this seems highly unlikely.
Nevertheless, we experimentally compare our approach with this mechanism in Section~\ref{sec:experiments}.

% A limitation of our learning rate resetting technique can, however, be seen in the fact, that it is insensitive to drifts that are not significant enough to be detected by ADWIN.
% To address this, we develop a \textit{soft resetting} adaptation approach that only partly resets the learning rate based on an estimate of the drift probability $\hat{p}$ by using
% \begin{equation}
% 	\eta_{t+1} = (\eta_t + \alpha\cdot \hat{p}(\eta_0 - \eta_t)) \cdot \gamma,
% \end{equation}\label{eq:soft_drift_reset}
% where $\alpha \in (0, 1]$ is a hyperparameter.
% We obtain $\hat{p}$ by performing a Kolmogorov-Smirnoff test on two time shifted rolling windows of prediction losses as is also done by the \textit{KSWIN} drift detector~\cite{raabReactiveSoftPrototype2020a}.
% With this confidence estimate we aim to achieve smaller steps that depend on the severity of drift and therefore cause more granular adaptation.


\section{Experiments}\label{sec:experiments}

To evaluate our hypotheses we performed computational experiments using the following setup\footnote{Our code is available at \url{anonymous.4open.science/r/LODL-D458/}.}:

We used both synthetic and publicly available real-world classification datasets, with different sizes and types of concept drift, listed in Table~\ref{tab:datasets}.

With the purpose of generating similar datasets with different types of concept drift, we generated Random Radial Basis Function (RBF) datasets using the online learning framework River~\cite{montiel2021river}. We then caused concept drift by either incrementally moving all RBF centroids or abruptly switching the random seed of the generator in the center of the stream.

We further employed the Electricity and Covertype~\cite{misc_covertype_31} datasets, which are commonly used to evaluate online learning models, as well as the Insects datasets~\cite{souzaChallengesBenchmarkingStream2020} with known types of concept drift.
Covertype is accessible through the OpenML Platform~\cite{vanschorenOpenMLNetworkedScience2014}, while the remaining datasets are part of River.

As the model architecture, we used a single hidden layer MLP implemented in PyTorch~\cite{paszkePyTorchImperativeStyle2019}.
To account for the different dimensionality of the selected data streams, with the number of hidden units equal to number of input features.
This choice was informed by our experience that smaller models exhibit faster convergence and are therefore usually preferable over larger ones in online learning scenarios.

For selecting the decay factor $\gamma$ in decaying learning rate schedules, we ran a grid search with three geometrically spaced values.
We then used the value yielding the best accuracy for most datasets and the next smaller value for schedules with our resetting mechanism.
To ensure a minimal level of adaptability even for longer streams, we set a lower bound at 10\% of the base learning rate.
Unless stated otherwise, we tuned the base learning rate $\eta_0$ of all but the parameter-free approaches using a grid search of ten geometrically spaced values.
For the proposed learning rate resetting mechanism, we set the confidence level $\delta$ of ADWIN to $10^{-4}$.
For more details on our hyperparameter setup, refer to Appendix~\ref{app:hyperparams}.

For our evaluations we processed each dataset sequentially, emulating streams of mini-batches with four instances each, while recording the prequential accuracy and other metrics in intervals of 25 iterations.
We report our results averaged over five random seeds.
Since the prequential binary crossentropy used for training, can contain large outliers, we focus on the classification accuracy as a performance metric.



\begin{table}[]
	\small
	\begin{tabular}{@{}clcccc@{}}
		\toprule
		Type                    & Data Stream            & Instances & Features & Classes \\
		\midrule
		\multirow{2}{*}{Synth.} & RBF abrupt             & 20000     & 20       & 5       \\
		                        & RBF incremental        & 20000     & 20       & 5       \\
		\midrule
		\multirow{5}{*}{Real}   & Insects abrupt         & 52848     & 33       & 6       \\
		                        & Insects incremental    & 57018     & 33       & 6       \\
		                        & Insects gradual        & 24150     & 33       & 6       \\
		                        & Covertype\footnotemark & 100000    & 54       & 7       \\
		                        & Electricity            & 45312     & 8        & 2       \\
		\bottomrule
	\end{tabular}
	\caption{Datasets used for experimental evaluations.}
	\label{tab:datasets}
\end{table}

\footnotetext[2]{We used the first 100k from a total of 581k examples only.}


\subsection{Drift Adaptation}

\begin{figure}[h]
	\centering
	\includegraphics[width=.47\textwidth]{figures/lr_norms_schedules_insects_abrupt.pdf}
	\caption{Prequential accuracy and learning rate for our drift resetting approach (see Equation~\eqref{eq:drift_reset}), drift adaptation by~\citet{kunchevaAdaptiveLearningRate2008} and different basic schedules on Insects abrupt. Concept drifts are marked by dashed lines. The accuracy is exponentially smoothed with a decay factor of 0.75.}
	\label{fig:prequential_schedulers_insects}
\end{figure}

\begin{table*}[h]
	\centering
	\small
	\begin{tabular}{llccccccc}
		\toprule
		                                                    & Schedule        & RBF abrupt            & RBF incr.           & Covertype             & Electricity           & Insects abrupt        & Insects gradual       & Insects incr.         \\
		\midrule
		\multirow{4}{*}{\rotatebox[origin=c]{90}{Static}}   & Fixed           & \underline{94.79±.32} & 70.95±2.89          & \underline{83.42±.50} & \underline{73.77±.40} & 71.50±.08             & 75.31±.21             & 60.48±.20             \\
		                                                    & Step            & \bfseries 94.87±.28   & 70.19±3.02          & 82.89±.37             & \underline{73.62±.53} & \bfseries 72.23±.27   & \underline{75.83±.21} & \underline{61.18±.11} \\
		                                                    & Cyclic          & \underline{94.79±.32} & \bfseries 74.96±.86 & \bfseries 83.44±.08   & 68.38±.81             & 71.74±.39             & 75.64±.06             & 60.48±.20             \\
		                                                    & Exponential     & \underline{94.85±.29} & 70.23±2.40          & 82.95±.26             & \underline{73.51±.48} & \underline{72.19±.37} & \bfseries 75.91±.14   & \bfseries 61.28±.16   \\ \midrule
		\multirow{4}{*}{\rotatebox[origin=c]{90}{Adaptive}} & Weight Reset    & 69.96±.38             & 65.13±.80           & 83.12±.13             & 70.08±1.66            & 51.52±.90             & 62.55±2.34            & 34.11±.44             \\
		                                                    & Kuncheva        & 70.60±6.24            & 42.37±1.31          & 76.98±.15             & 67.06±.01             & 67.45±.50             & 72.43±.61             & 54.17±.30             \\
		                                                    & LR Reset (Ours) & \underline{94.83±.26} & 73.38±2.32          & 82.99±.20             & \bfseries 73.79±.62   & 71.73±.20             & 75.52±.12             & 60.77±.08             \\
		                                                    & LR Reset Oracle & 95.12±.21             & ---                 & ---                   & ---                   & 71.88±.26             & ---                   & ---                   \\
		\bottomrule
	\end{tabular}
	\caption{Average prequential accuracy [\%] for static (top) and drift adaptive learning rate schedules (bottom) with SGD. For LR Reset Oracle we manually reset the learning rate at timesteps where concept drift occurs.}
	\label{tab:lr_resetting}
\end{table*}

To evaluate the effectiveness of our learning rate resetting mechanism for drift adaptation (see Equation~\eqref{eq:drift_reset}), we compare its average prequential accuracy to that of the adaptation algorithm by \citet{kunchevaAdaptiveLearningRate2008} (see Equation~\eqref{eq:kuncheva_lr}) and complete model resetting, commonly used in online learning.

As can be seen in Table~\ref{tab:lr_resetting}, our approach clearly outperforms both other drift adaptation techniques by a wide margin on all but Covertype, where weight resetting yielded slightly higher accuracy.
It also compares favorably against static exponential decay on the RBF incremental, Covertype and Electricity datasets.
However, aside from the results on RBF incremental the accuracy increases are negligible.
Furthermore, learning rate resetting did not yield improvements for the Insects datasets regardless of the type of drift.
This is also reflected in Figure~\ref{fig:prequential_schedulers_insects}, where the standard exponential schedule's accuracy initially rises faster and also recovers faster after the first concept drift, which is likely caused by its larger step size in the first half of the stream.
Although the resetting mechanism was frequently triggered when no concept drift occurred, our results for the oracle resetting approach that was triggered only for timesteps with drift show that this performance gap is not caused by the drift detector.
Rather, it appears that using a larger initial learning rate and slower decay is sufficient for assuring adequate adaptability to concept drift throughout most data streams while granting better stability at later stages.

Overall, a slower static decay and a larger initial learning rate seems to be preferable over a more aggressive schedule with our drift resetting mechanism, unless severe concept drift as in RBF incremental can be expected.

With accuracy values within the standard deviation range of one another on all evaluated streams, step-wise decay displays almost identical performance to exponential decay.
The cyclic schedule's accuracy for RBF incremental and Covertype on the other hand significantly exceeded that of the other static and adaptive schedules but lacks behind on all other streams.
We also did not find improvements in convergence speed by an order of magnitude as observed by~\cite{smithSuperConvergenceVeryFast2018a} for the investigated scenario.
Based on our results, the usefulness of cyclic learning rates for online learning applications therefore seems to be more data dependent than conventional decaying schedules.


\subsection{Adaptive Learning Rates}

\begin{figure}[hb]
	\centering
	\includegraphics[width=.47\textwidth]{figures/lr_norms_optims_insects_abrupt.pdf}
	\caption{Prequential accuracy and learning rate for adaptive optimizers on Insects abrupt. Concept drifts are marked by dashed lines. Accuracy is exponentially smoothed with a decay factor of 0.75.}
	\label{fig:prequential_optims_insects}
\end{figure}
\begin{table*}[h]
	\centering
	\small
	\begin{tabular}{llcccccccc}
		\toprule
		                                                   & Optimizer & RBF abrupt          & RBF incr.            & Covertype             & Electricity          & Insects abrupt      & Insects gradual     & Insects incr.       \\
		\midrule
		\multirow{5}{*}{\rotatebox[origin=c]{90}{Tuned}}   & SGD       & \bfseries 94.79±.32 & 70.95±2.89           & \bfseries 83.42±.50   & 73.77±.40            & 71.50±.08           & 75.31±.21           & 60.48±.20           \\
		                                                   & Adam      & 93.45±.30           & 69.26±5.14           & 79.01±.27             & 69.79±.54            & \bfseries 75.38±.24 & 75.78±.74           & \bfseries 64.17±.13 \\
		                                                   & AdaGrad   & 92.45±1.37          & 52.87±6.62           & 81.68±.35             & \bfseries 76.99±1.20 & 74.87±.40           & \bfseries 77.15±.27 & 62.51±.59           \\
		                                                   & WNGrad    & 87.30±.68           & 44.92±.73            & 76.98±.15             & 70.80±.59            & 66.25±.19           & 66.75±.40           & 56.14±.21           \\
		                                                   & HD        & 93.92±.31           & \bfseries 72.29±2.90 & \underline{83.36±.25} & 73.83±.32            & 70.67±.06           & 73.37±.21           & 59.92±.18           \\ \midrule
		\multirow{4}{*}{\rotatebox[origin=c]{90}{LR-Free}} & COCOB     & \bfseries 93.40±.38 & 63.52±2.70           & 82.27±.46             & \bfseries 84.30±.56  & \bfseries 74.75±.11 & \bfseries 77.00±.05 & \bfseries 63.65±.16 \\
		                                                   & DoG       & 92.72±.59           & \bfseries 73.17±2.72 & \bfseries 83.07±.64   & 71.53±.70            & 70.59±.26           & 74.01±.21           & 59.66±.22           \\
		                                                   & DAdapt    & 74.91±4.22          & 45.47±2.75           & 76.69±.79             & 66.03±1.75           & 50.05±11.26         & 48.21±10.62         & 36.00±11.81         \\
		                                                   & Mechanic  & 88.94±.58           & 49.26±1.44           & 78.67±.18             & 50.73±7.60           & 55.31±21.47         & 65.80±.53           & 47.89±17.46         \\
		\bottomrule
	\end{tabular}
	\caption{Average prequential accuracy [\%] for different adaptive optimizers and SGD.}
	\label{tab:results_adaptive_optims}
\end{table*}
To make inferences about the usefulness of different adaptive first-order optimization techniques for online deep learning, we performed prequential evaluations for the methods listed in Table~\ref{tab:param_free_optims}.

From the results displayed in Table~\ref{tab:results_adaptive_optims}, it can be deduced that none of the approaches that require tuning a step size parameter stands out as generally superior for the investigated datasets.
Rather, each of SGD, AdaGrad, and Adam achieve the best accuracy on two of the seven datasets.  This once again underlines the data dependency of the optimizer performance.
However, since SGD yields the best accuracy on RBF abrupt but is clearly surpassed on Insects abrupt, the type of concept drift does not seem to play a significant role.

Due to its simplicity and favorable computational efficiency, it appears that standard SGD should be selected out of the non-parameter-free approaches if the characteristics of the targeted data stream are unknown.
The SGD variant of Hypergradient Descent~\cite{baydinOnlineLearningRate2018} and WNGrad~\cite{wuWNGradLearnLearning2020} on the other hand seem to rarely be optimal choices.

In the category of learning rate free optimizers COCOB~\cite{orabonaTrainingDeepNetworks2017}, outperformed its competitors on all but two datasets.
Its accuracy also comes close to that or even exceeds that of the best tuned approaches.
As the second best parameter-free method, DoG also comes within reach of the accuracy of the tuned methods, while offering much better runtime and memory efficiency compared to COCOB (see Table~\ref{tab:param_free_optims}).
WNGrad~\cite{wuWNGradLearnLearning2020} as well as the Mechanist~\cite{cutkoskyMechanicLearningRate2023} and DAdaptation~\cite{defazioLearningRateFreeLearningDAdaptation2023a} SGD variants on the other hand performed significantly worse than all other optimization approaches.

To gain additional insights into the investigated adaptive optimizers, we calculated their effective learning rates as $\frac{||\eta_t||}{\sqrt{D}}$, where $\eta_t \in \mathbb{R}^D$ is the vector of parameter specific learning rates.
The resulting learning rate curves shown in Figure~\ref{fig:prequential_schedulers_insects}, provide an indication regarding the reason for the poor performance of WNGrad and DAdaption.
Whereas the learning rate of DoG quickly approaches the tuned SGD learning rate, the two parameter-free methods diverge considerably from it.
A possible cause for this could be the higher level of gradient noise introduced by the small mini-batches and concept drift associated with the online learning setting.

Another interesting observation that can be made in Figure~\ref{fig:prequential_optims_insects} is that the learning rate of the best performing Adam features spikes for most change points, suggesting some form of adaptability to drift.
Since the much worse performing Mechanic shows similar spikes, this is however unlikely to be a significant contributing factor to Adams high accuracy on Insects abrupt.

It may also be noted that the learning rates of the COCOB, Adam and Mechanic optimizers with parameter specific learning rates exceed those of single value step sizes by multiple orders of magnitude.
This is an effect of AdaGrad-like scaling, which creates larger learning rates for parameters with small and consistent gradients \cite{cutkoskyMechanicLearningRate2023}.
Therefore, the parameter updates generated by these approaches are not necessarily larger.



\subsection{Learning Rate Tuning}

\begin{figure}[h]
	\centering
	\includegraphics[width=.45\textwidth]{figures/pretune_1x64_acc_lr_exp_schedule.pdf}
	\caption{Pre-tuned LR (LR that maximizes accuracy on pre-tuning data) and resulting accuracy on data streams when using SGD and an exponential learning rate schedule with 500 or 1000 separate tuning samples. Results are averaged over all real-world datasets. The shaded area represents the 1$\sigma$-interval.}\label{fig:pretune_lr_accuracy}
\end{figure}



We evaluated our proposed learning rate pre-tuning approach for MLPs with either one or three hidden layers and 64 or 128 hidden units per layer.
To this end, we performed prequential evaluation runs with multiple values for the initial learning rate $\eta_0$ as well as the exponential decay factor $\gamma$, bootstrapping samples from a subset of 500 or 1000 instances from the beginning of each data stream, held out from the remaining data.
At each iteration of this process we determine the hyperparameters with the best mean prequential accuracy.
We use accuracy instead of loss as the selection criterion, due to the fact that the binary crossentropy employed for training commonly decreases by orders of magnitude throughout optimization.
A selection based on the prequential loss would therefore only consider the initial iterations of the tuning process.

Figure~\ref{fig:pretune_lr_accuracy} shows the learning rate resulting from the pre-tuning process at each tuning step averaged over all real-world datasets.
The bottom row of the figure displays the mean accuracy achieved when using this learning rate on the remaining data stream not used for tuning.
Since we find our tuning procedure to be robust to the model size (see Appendix~\ref{app:pretune_model_sizes}) we focus on the results for the smallest network.

It can be seen that after overshooting initially, the tuned learning rate converges in the vicinity of optimal learning rate after 1000 iterations.
While tuning with 1000 samples yields a better approximation of the optimal value, both subset sizes on average achieve significantly better learning rates for more than 1000 pre-tuning steps, than conventional tuning performed with 800 training and 200 validation samples.
This is also clearly reflected in the accuracy scores, which already exceed conventional tuning after 500 iterations.
Although our approach maintains a larger deviation from the optimal learning rate, it notably surpasses DoG in terms of accuracy.
This advantage seemingly stems from the fact that excessively low learning rates have a more negative impact than excessively high ones for the evaluated scenario.

Figure~\ref{fig:pretune_fixed_vs_exp_lr} shows that when provided with sufficient data, pre-tuning also significantly outperforms DoG for a fixed learning rate.
However, its performance benefit compared to DoG is considerably larger with a decaying schedule.

In conclusion, our proposed tuning approach enables significantly better learning rate selection for prequential evaluation on data streams compared to both conventional tuning as well as DoG.
Additionally, pre-tuning has the benefit that once completed, no additional memory or runtime costs are incurred.
In streaming applications, where computing resources are often times a limiting factor, this could be a critical advantage.
Although, if computational efficiency is insignificant, the highly performant but expensive COCOB~\cite{orabonaTrainingDeepNetworks2017} or the slightly less performant and much less expensive DoG~\cite{ivgiDoGSGDBest2023} may be more appropriate.





% \begin{figure}[ht]
% 	\centering
% 	\begin{tikzpicture}
% 		% Upper image
% 		\node[inner sep=0pt] (upper) {\includegraphics[width=0.4\textwidth]{figures/sgd_trajectory_optims1.pdf}};

% 		% Lower image
% 		\node[inner sep=0pt, below=3mm of upper] (lower){\includegraphics[width=0.4\textwidth]{figures/sgd_trajectory_optims2.pdf}};

% 		\path ([xshift=-8pt]upper) -- ([xshift=-8pt]lower) node[midway] (text){Concept Drift};
% 		\draw[->] ([xshift=-5pt, yshift=5pt]upper.south-|text.west) -- ([xshift=-5pt, yshift=-3pt]lower.north-|text.west);
% 		\draw[->] ([xshift=5pt, yshift=5pt]upper.south-|text.east) -- ([xshift=5pt, yshift=-3pt]lower.north-|text.east);

% 	\end{tikzpicture}
% 	\caption{Parameter trajectory of COCOB~\cite{orabonaTrainingDeepNetworks2017}, DoG~\cite{ivgiDoGSGDBest2023} and Adam~\cite{kingmaAdamMethodStochastic2017b} on synthetic data stream with abrupt concept drift. Marker colors depict the expected prequential loss over the last 16 data instances.}
% \end{figure}

\section{Conclusion}

In this work, we investigate the influence and selection of the learning rate and optimization procedure with respect to deep learning in streaming environments.
We first provide theoretical background on discrepancies between learning rate optimization in conventional batch learning and online learning.
Based on these differences, we derive a simple mechanism adapting the learning rate on concept drift occurrences.
We then give an overview learning rate free algorithms popular in batch learning, which we compare experimentally on multiple synthetic and real-world datasets,
finding both COCOB~\cite{orabonaTrainingDeepNetworks2017} and DoG~\cite{ivgiDoGSGDBest2023} to come close to the performance of optimizers with tuned learning rates.
Lastly, we introduce a streaming specific learning rate tuning approach that grants significant performance increases over conventional tuning via a train-validation split.




\bibliography{aaai24}
\newpage
\appendix

\section{Hyperparameter values}\label{app:hyperparams}

In our experiments we used the following hyperparameter settings.

\begin{table}[H]
	\centering
	\begin{tabular}{ll}
		\toprule
		Schedule    & Static                            \\ \midrule
		Exponential & $\gamma=1-2^{-13}$                \\
		Exp. Reset  & $\gamma=1-2^{-12}, \delta=0.0001$ \\
		Step        & $\gamma=0.75, s=2000$             \\
		Cyclic      & $\hat{\eta}=0.25, s=8000$         \\ \bottomrule
	\end{tabular}
	\caption{Learning Rate Schedule Hyperparameters.}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{ll}
		\toprule
		Optimizer & Learning Rate                                  \\
		\midrule
		SGD       & $\{2^{1}, 2^{0}, \ldots, 2^{-8}\}$             \\
		Adam      & $\{2^{-3}, 2^{-4}, \ldots, 2^{-12}\}$          \\
		AdaGrad   & $\{2^{1}, 2^{0}, \ldots, 2^{-8}\}$             \\
		WNGrad    & $\{10^{1.25}, 10^{0.75}, \ldots, 10^{-7.75}\}$ \\
		SGD-HD    & $\{2^{-3}, 2^{-4}, \ldots, 2^{-12}\}$          \\ \midrule
		COCOB     & $100$                                          \\
		DoG       & $1$                                            \\
		DAdaptSGD & $1$                                            \\
		Mechanic  & $0.01$                                         \\
		\bottomrule
	\end{tabular}
	\caption{Search spaces for learning rates of different optimizers.}
\end{table}

\section{Learning Rate Pre-Tuning Results}\label{app:pretune_model_sizes}

\begin{figure}[h]
	\centering
	\includegraphics[width=.47\textwidth]{figures/pretune_architectures_exp_schedule.pdf}
	\caption{Accuracy achieved by pre-tuning on 500 or 1000 samples when using SGD with an exponential schedule on different network sizes, averaged over all real-world datasets. The shaded area represents the 1$\sigma$-interval.}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=.47\textwidth]{figures/pretune_1x64_fixed_vs_exp_schedule.pdf}
	\caption{Accuracy achieved by pre-tuning on 500 or 1000 samples when using SGD with a fixed LR schedule (top) or an exponential schedule (bottom), averaged over all real-world datasets. The shaded area represents the 1$\sigma$-interval.}\label{fig:pretune_fixed_vs_exp_lr}
\end{figure}

\end{document}
