% This file was adapted from ICLR2022_conference.tex example provided for the ICLR conference
\documentclass{article} % For LaTeX2e
\usepackage{collas2024_conference,times}
\usepackage{easyReview}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{caption, subcaption}
\usepackage{graphicx}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}


\newcommand{\cmark}{\ding{51}} % checkmark symbol
\newcommand{\xmark}{\ding{55}} % X-mark symbol

% Please leave these options as they are
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=purple,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\usepackage{cleveref}


\title{Optimizing the learning rate for training neural networks on the fly}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \collasfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro  \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Random University\\
Country \\
\texttt{\{hippo,brain\}@cs.random.edu} \\
\And % Use And to have authors side by side
Koala Learnus \& D. Q. ResNet  \\
Department of Computational Neuroscience \\
University of Random City \\
Another Country \\
\texttt{\{koala,net\}@random.rand} \\
\AND % Use AND to have authors block one under the other
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\collasfinalcopy % Uncomment for camera-ready version, but NOT for submission.

%\preprintcopy % Uncomment for the preprint version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
   Efficient training via gradient-based optimization techniques is an essential building block to the success of artificial neural networks. Extensive research on the impact and the effective estimation of an appropriate learning rate has partly enabled these techniques. Despite the proliferation of data streams generated by IoT devices, digital platforms, etc., previous research has been primarily focused on batch learning, which assumes that all training data is available a priori. However, characteristics such as the gradual emergence of data and the occurrence of distributional shifts also known as \textit{concept drift} pose additional challenges. Therefore, the findings on batch learning may not be applicable to streaming environments, where the underlying model needs to adapt on the fly, each time a new data instance appears. In this work, we seek to address this knowledge gap by (i) evaluating and comparing typical learning rate schedules and optimizers, (ii) exploring adaptations of these techniques, and (iii) providing insights into effective learning rate tuning in the context of stream-based training of neural networks.
\end{abstract}

\section{Introduction}
Artificial neural network models have shown exceptional performance in various domains.
One of the main factors leading to such outstanding results is the choice of the optimization method used to train the target model.
Almost all modern applications of neural architectures use first-order stochastic optimization methods such as \textit{stochastic gradient descent}, which iteratively update the parameters of the underlying model based on gradient information.
One of the most important variables of such algorithms is the step size or \textit{learning rate} (LR).

As a result, many techniques for setting and optimizing the learning rate have emerged over the years (see Figure~\ref{fig:lr_overview}).
For example, based on prior knowledge, the learning rate can be set as a fixed value or as a schedule that changes the step size over time.
Alternatively, one could use an adaptive learning rate technique that considers historical gradient information to modify the learning rate at each iteration.
\begin{figure}[ht]
   \centering
   \includegraphics[width=0.9\textwidth]{figures/lr_overview.pdf}
   \caption{Overview of different learning rate optimization approaches.}
   \label{fig:lr_overview}
\end{figure}
In batch learning scenarios, where all training data is available a priori, the above methods are well researched.
Despite the increasing prevalence of online learning environments, where data becomes available as part of a data stream, their use in such scenarios has received little research attention.

According to \citet{bifetMOAMassiveOnline2010} a machine learning model operating on a data stream must be able to
\begin{enumerate}
   \item[R1:] process a single instance at a time,\label{rq:single_instance}
   \item[R2:] process each instance in a limited amount of time,\label{rq:limited_time}
   \item[R3:] use a limited amount of memory,\label{rq:limited_memory}
   \item[R4:] predict at any time,\label{rq:predict_any_time}
   \item[R5:] adapt to changes in the data distribution.\label{rq:adapt_to_drift}
\end{enumerate}


These requirements pose additional challenges in selecting an appropriate optimizer and learning rate.
To enable more informed decisions when optimizing the learning rate, we provide insight into these challenges and empirically evaluate commonly used optimization techniques in an online learning setting (i).
Furthermore, we introduce a \textit{drift reset} mechanism to adapt the learning rate to concept drifts that commonly occur in streaming environments (ii).
Finally, we propose a \textit{pre-tuning} approach to effectively optimize the learning rate for online training of neural networks \textit{pre-stream} (see Figure~\ref{fig:lr_overview}) (iii). % TODO: clarify what pre-stream means

\section{Learning Rate in First-Order Optimization}

In the following, we will explain the theoretical background of the learning rate hyperparameter in first-order stochastic optimization.
First-order stochastic optimization algorithms, such as stochastic gradient descent, typically aim to solve the following problem
\begin{equation}
   \min_{\theta} \mathbb{E}_{x \sim p(x)} [\mathcal{L}(x, \theta)],
\end{equation}
where $\mathcal{L}(x, \theta)$ is a loss function that quantifies the prediction error of the model given a mini-batch of data samples $x$ and model parameters $\theta$.
The blueprint process for solving this problem using first-order stochastic optimization consists of the following steps for each iteration $t \in 0, \ldots, T$:
\begin{enumerate}
   \item Draw a mini-batch of samples $x_t$ from the distribution $p(x)$.
   \item Compute the loss $\mathcal{L}_t = \mathcal{L}(x_t, \theta_t)$ for $x_t$ and current parameters $\theta_t$.
   \item Compute gradient $g_t = \nabla_{\theta_t} \mathcal{L}_t$ with respect to the parameters.
   \item Update the parameters for the next iteration using $g_t$ and possibly information from previous iterations.
\end{enumerate}

For basic SGD, we can define the parameter update performed at the end of each iteration as
$\theta_{t}  = \theta_{t} - \eta_t \cdot g_t$,
where $\eta_t$ denotes the step size or \textit{learning rate} at timestep $t$.

The primary trade-off with respect to $\eta$ is that increasing it speeds up convergence, but also increases stochasticity and the risk of divergence~\citep{bengioPracticalRecommendationsGradientbased2012}.
In fact, \citet{smithBayesianPerspectiveGeneralization2018}, found that the “noise scale” of SGD is tied to $\eta$~\citep{smithBayesianPerspectiveGeneralization2018}.

\subsection{Learning Rate Schedules}

Often, the performance of a model can be improved by using a schedule that changes the learning rate as training progresses~\citep{wuDemystifyingLearningRate2019b}.
For example, to ensure fast convergence early in training while mitigating jumping around potential minima later, it is common to use a decaying schedule that starts with a large learning rate and decreases over time.
An additional benefit of this approach is potentially better generalization, since larger learning rates can help skip sharp minima with poor generalization~\citep{hochreiterFlatMinima1997,chaudhariEntropySGDBiasingGradient2017}.

Commonly used forms of decay are exponential decay, where $\eta_{t}$ is calculated as
$\eta_{t} = \eta_0 \cdot \gamma^t$,
with $\gamma < 1$, and stepwise decay, which for a regular interval between steps of length $s$ is given as $\eta_0 \cdot \gamma^{\lfloor t/s \rfloor}$.
Another common approach is to decay $\eta$ each time the training loss plateaus for a given number of iterations. Other popular schedules include cyclic learning rates that oscillate $\eta$ between two values over a predefined interval.
For a triangular cycle, the learning rate is computed as
\begin{equation}
   \eta_t = \eta_0 + \frac{\hat{\eta} - \eta_0}{2s} \cdot \min_{i} \{|t-i\cdot s|\},
\end{equation}
where $\hat{\eta}$ is the learning rate at the midpoint of each cycle of length $s$.
Some studies~\citep{smithCyclicalLearningRates2017, smithSuperConvergenceVeryFast2018a} have found that cyclic schedules can significantly speed up the convergence of neural networks, in some cases even compared to adaptive techniques like Adam~\citep{kingmaAdamMethodStochastic2017b}.
While there are many alternatives, in this work we focus on exponential, step, and cyclic learning rates as some of the most commonly used generic schedules.
For a comprehensive overview and detailed analysis of learning rate policies, see \citet{wuDemystifyingLearningRate2019b}.

\subsection{Adaptive Learning Rates}

Several studies have proposed \textit{adaptive optimizers} that increase the robustness of the training process with respect to the learning rate.
These optimizers adjust the step size based on previous gradients at each optimization step~\citep{duchiAdaptiveSubgradientMethods2011}.

\begin{table}[ht]
   \centering
   \scriptsize
   \caption{
      Overview of additional time- and space-complexity of evaluated adaptive first-order optimizers compared to basic SGD.
      Values are given in big O notation with respect to the number of model parameters $D$.
   }\label{tab:param_free_optims}
   \begin{tabular}{@{}lllcc@{}}
      \toprule
      Optimizer                & Runtime            & Space             & Param. specific & LR free \\ \midrule
      AdaGrad                  & $\mathcal{O}(5D)$  & $\mathcal{O}(1D)$ & \cmark          & \xmark  \\
      Adam                     & $\mathcal{O}(12D)$ & $\mathcal{O}(2D)$ & \cmark          & \xmark  \\
      WNGrad                   & $\mathcal{O}(2D)$  & $\mathcal{O}(0)$  & \xmark          & \xmark  \\
      COCOB                    & $\mathcal{O}(14D)$ & $\mathcal{O}(4D)$ & \cmark          & \cmark  \\
      HD\footnotemark[1]       & $\mathcal{O}(2D)$  & $\mathcal{O}(1D)$ & \xmark          & \xmark  \\
      Mechanic\footnotemark[1] & $\mathcal{O}(10D)$ & $\mathcal{O}(1D)$ & \cmark          & \cmark  \\
      DoG                      & $\mathcal{O}(5D)$  & $\mathcal{O}(1D)$ & \xmark          & \cmark  \\
      D-Adapt\footnotemark[1]  & $\mathcal{O}(6D)$  & $\mathcal{O}(2D)$ & \xmark          & \cmark  \\
      \bottomrule
   \end{tabular}
\end{table}

One of the earlier techniques in this category is \textit{AdaGrad}~\citep{duchiAdaptiveSubgradientMethods2011}, which scales the learning rate based on the sum of squares of past gradients for each parameter, resulting in a parameter-specific step size.
Several other approaches, such as AdaDelta~\citep{zeilerADADELTAAdaptiveLearning2012a} and RMSProp, later built on AdaGrad's scaling approach.
The same is true for the widely used Adam optimizer~\citep{kingmaAdamMethodStochastic2017b}, which adds a momentum term from prior gradients to speed up convergence for parameters with consistent derivatives.
Another AdaGrad based optimizer is \textit{WNGrad}~\citep{wuWNGradLearnLearning2020}, which adaptively scales each parameter update based on the squared sum of past gradients.

\footnotetext[1]{Variant with SGD as the underlying optimizer.}

So-called parameter-free variants of SGD aim to eliminate the learning rate altogether by optimizing it as training progresses.
For example, the \textit{COCOB} algorithm~\citep{orabonaTrainingDeepNetworks2017} models parameter optimization as a gambling problem, where the goal is to maximize the reward from betting on each gradient.
The resulting strategy is equivalent to running a meta-optimization algorithm that estimates the expected optimal learning rate~\citep{orabonaTrainingDeepNetworks2017}.
Several other studies~\citep{vanervenMetaGradMultipleLearning2016a,baydinOnlineLearningRate2018,cutkoskyMechanicLearningRate2023} have also used the idea of learning $\eta$ via a meta-optimization process.
The \textit{hypergradient descent} (HD) approach~\citep{baydinOnlineLearningRate2018}, for example, adapts the learning rate of a base optimizer like SGD using a meta-gradient descent procedure, although this does not remove the learning rate entirely, but replaces it with a less sensitive hypergradient step size.
Mechanic~\citep{cutkoskyMechanicLearningRate2023} pursues the same goal by applying a meta \textit{online convex optimization} (OCO) algorithm to an arbitrary base optimizer.

Research has shown that in an OCO problem setting with stationary data, the worst-case optimal fixed step size for SGD is
\begin{equation}\label{eq:oco_optimal_lr}
   \eta^* = \frac{||\theta_0 - \theta^*||}{\sqrt{\sum_{t=0}^{T} ||g_t||^2}}.
\end{equation}
Multiple parameter-free optimizers, make use of this notion.
As its name suggests, the \textit{Distance over Gradients} (DoG)~\citep{ivgiDoGSGDBest2023} algorithm estimates the unknown numerator in Equation~\eqref{eq:oco_optimal_lr} as the maximum distance $\max_{i<t}||\theta_0 - \theta_i||$ between the initial parameters and the parameters of all previous iterations.
DoG additionally uses polynomial decay averaging as proposed by~\citet{shamirStochasticGradientDescent2012}.
\textit{D-Adaptation} by \citet{defazioLearningRateFreeLearningDAdaptation2023a}, on the other hand, employs weighted dual averaging~\citep{duchiDualAveragingDistributed2012} to compute bounds on the distance between initial and optimal parameters.
Although adaptive optimization techniques seem intuitively well suited for non-stationary data, their application to data streams has rarely been investigated.
Therefore, we assess the suitability of some of the most prominent adaptive optimizers, listed in Table~\ref{tab:param_free_optims}, for stream-based learning.

\section{Learning Rate in Online Learning}

In a batch learning setting, optimizing the learning rate involves minimizing the expected loss on a hold-out set of validation data.
Formally, we can express this task as
\begin{equation}
   \label{eq:batch_lr_optim}
   \min_{\eta_0, \ldots, \eta_T} \sum_{i=0}^V \mathcal{L}(x_i, \theta_T),
\end{equation}
where all $x_i$ are part of a separate validation dataset and $\theta_T$ are the parameters at the end of training.
In online learning where data is generated incrementally, this notion of learning rate optimization is not feasible.
Due to requirements \textbf{R1-R5}, models operating in an online learning environment should be evaluated in a \textit{prequential} manner~\citep{bifetMOAMassiveOnline2010}, where each sample $x_t$ in the data stream is first used to test and then to train the model, ensuring that testing is done on previously unobserved data.

Training in such a scenario can be more accurately modeled as an online convex optimization problem~\citep{shalev-shwartzOnlineLearningOnline2011,hazanIntroductionOnlineConvex2016}, where the optimizer suffers a loss $\mathcal{L}(x_t, \theta_{t})$ and produces updated parameters $\theta_{t+1}$ at each iteration of the data stream.
Learning rate optimization in this setting can be formulated as
\begin{equation}
   \label{eq:stream_lr_optim}
   \min_{\eta_0, \ldots, \eta_T} \sum_{t=0}^{T} \mathcal{L}_t(x_t,\theta_t).
\end{equation}

Compared to Equation~\eqref{eq:batch_lr_optim}, Equation~\eqref{eq:stream_lr_optim} features some key differences.
Due to the requirement to be able to predict at any time (\textbf{R4}), the goal is to minimize the total sum of losses over all timesteps of the prequential evaluation process, instead of the validation loss for the final parameters $\theta_T$.
Therefore, the speed of convergence is more important in the streaming setting, while the performance of the final $\theta_T$ parameter has a much smaller impact.
Since memory is limited (Requirement~\ref{rq:limited_memory}), it is also not possible to continue training on previously observed data as long as the loss decreases, which puts even more emphasis on fast adaptation.
At the same time, a higher learning rate that temporarily increases the loss by skipping local minima may be suboptimal with respect to Equation~\eqref{eq:stream_lr_optim}, even if it eventually leads to a lower loss.
Another difference to conventional batch learning is that the loss function $\mathcal{L}_t$ is time-dependent, due to the fact that data streams are often subject to distributional changes in the form of so-called \textit{concept drift} over time.

\subsection{Learning Rate Tuning}\label{subsec:pre-tuning}

The differences in evaluation schemes described above may cause conventional learning rate tuning to produce poor results for stream-based learning.
Therefore, we propose a modified tuning approach that approximates Equation~\eqref{eq:stream_lr_optim}, which we call learning rate \textit{pre-tuning}.

To emulate the target data stream, we continuously draw samples with replacement from the tuning data in a bootstrapping procedure instead of training on all data for multiple epochs.
By doing so, we aim to increase the variability of the data and thus the similarity to a real data stream.
We then optimize the learning rate with respect to the average prequential performance over the emulated stream using an arbitrary parameter search technique.
We provide a detailed experimental evaluation of our approach in Section~\ref{sec:experiments}.

\subsection{Concept Drift Adaptation}

Concept drift requires repeated adaptation of the model parameters.
If post-drift training is interpreted as a new online optimization problem, the worst-case optimal learning rate can be computed according to Equation~\eqref{eq:oco_optimal_lr}, replacing the initial parameter values $\theta_0$ with the values at the time of drift onset $\theta_{t_d}$.
As a result, stronger drifts that cause $\theta^*$ to move away from $\theta_{t_d}$ can benefit from larger learning rates.

Based on this notion, we propose a simple adaptation to decaying learning rate schedules that resets $\eta$ to its original value when a concept drift is detected.
An exponential schedule modified with our approach will thus yield learning rates of
\begin{equation}
   \eta_t = \eta_0 \cdot \gamma^{t-t_d},
\end{equation}\label{eq:drift_reset}
where $t_d$ marks the timestep at which the drift was last detected.
For drift detection, we apply ADWIN~\citep{bifetLearningTimeChangingData2007} to the prequential losses.
To avoid mistakenly detecting loss decreases as concept drift, we use a one-sided ADWIN variant that tests only for increases.

Our approach is similar to some \textit{forgetting mechanisms} commonly used in conventional online learning~\citep{gamaSurveyConceptDrift2014}.
To improve model plasticity, such mechanisms partially or completely reset the current model parameters to their initial values.
However, we hypothesize that this approach is not well suited for neural-based approaches.
The reason is that, under the assumption of convexity, the newly initiated parameters must be closer to the optimal parameters $\theta^*$ than the current parameters to be beneficial.
We experimentally compare our approach with this weight-reset mechanism in Section~\ref{sec:experiments}.

\section{Experiments}\label{sec:experiments}

We empirically evaluate our hypotheses using the following setup\footnote[2]{Code available at \url{anonymous.4open.science/r/LODL-D458/}.}:

We use both synthetic and publicly available real-world classification datasets with different sizes and types of concept drift, listed in Table~\ref{tab:datasets}.
Our evaluations include two synthetic \textit{Random Radial Basis Function} (RBF) datasets that we manipulated to incorporate concept drift using the online learning framework \textit{River}~\citep{montiel2021river}.

\begin{table}[ht]
   \centering
   \scriptsize
   \caption{Datasets used for experimental evaluations.}
   \begin{tabular}{@{}clcccc@{}}
      \toprule
      Type                    & Data Stream     & Instances               & Features & Classes \\
      \midrule
      \multirow{2}{*}{Synth.} & RBF abrupt      & 20,000                  & 20       & 5       \\
                              & RBF incremental & 20,000                  & 20       & 5       \\
      \midrule
      \multirow{5}{*}{Real}   & Insects abrupt  & 52,848                  & 33       & 6       \\
                              & Insects gradual & 24,150                  & 33       & 6       \\
                              & Covertype       & 100,000\footnotemark[3] & 54       & 7       \\
                              & Electricity     & 45,312                  & 8        & 2       \\
      \bottomrule
   \end{tabular}
   \label{tab:datasets}
\end{table}

\footnotetext[3]{For Covertype we use only the first 100,000 from a total of 581,012 instances.}

We further employ the \textit{Electricity} and \textit{Covertype}~\citep{misc_covertype_31} datasets, which are commonly used to evaluate online learning models, as well as two \textit{Insects} datasets~\citep{souzaChallengesBenchmarkingStream2020} with predefined types of concept drift.
\textit{Covertype} is available on \textit{OpenML}~\citep{vanschorenOpenMLNetworkedScience2014}, while the remaining datasets are part of River.
We employ a \textit{PyTorch}~\citep{paszkePyTorchImperativeStyle2019} implementation of a single-hidden-layer MLP with units matching the number of input features.
This choice is based on our experience that smaller models exhibit faster convergence and are therefore usually best suited for streaming environments.

We tune the base learning rate $\eta_0$ of all but the parameter-free approaches using a grid search of ten geometrically spaced values and configure adaptive optimizers with their default parameter values. For HD, Mechanic and D-Adaptation we select standard SGD as the base algorithm.
We select a fixed factor $\gamma$ for decay schedules on all datasets.
For the proposed learning rate resetting mechanism, we select a smaller decay factor and set the confidence level $\delta$ for drift detection to $10^{-4}$.
For our evaluations we process each dataset sequentially, emulating streams of mini-batches of four instances each, while recording the prequential accuracy and other metrics in intervals of 25 iterations.
We report our results averaged over five random seeds.

\subsection{Learning Rate Schedules}

To evaluate the effectiveness of our learning rate resetting mechanism for drift adaptation (see Equation~\eqref{eq:drift_reset}), we compare its average prequential accuracy to that of model weight resetting, commonly used in online learning.

\begin{table}[ht]
   \centering
   \scriptsize
   \caption{Average prequential accuracy [\%] for static and drift adaptive learning rate schedules with SGD. For LR-Reset Oracle we manually reset the learning rate at timesteps where concept drift occurs. Best values are shown in \textbf{bold}, values within the $1\sigma$ interval of best values \underline{underlined}.}
   \begin{tabular}{lccccccc}
      \toprule
      Schedule        & RBF abrupt            & RBF incr.           & Covertype             & Electricity           & Insects abrupt        & Insects gradual       \\ \midrule
      Fixed           & \underline{94.79±.32} & 70.95±2.89          & \underline{83.42±.50} & \underline{73.77±.40} & 71.50±.08             & 75.31±.21             \\
      Step            & \bfseries 94.87±.28   & 70.19±3.02          & 82.89±.37             & \underline{73.62±.53} & \bfseries 72.23±.27   & \underline{75.83±.21} \\
      Cyclic          & \underline{94.79±.32} & \bfseries 74.96±.86 & \bfseries 83.44±.08   & 68.38±.81             & 71.74±.39             & 75.64±.06             \\
      Exponential     & \underline{94.85±.29} & 70.23±2.40          & 82.95±.26             & \underline{73.51±.48} & \underline{72.19±.37} & \bfseries 75.91±.14   \\
      Weight-Reset    & 69.96±.38             & 65.13±.80           & 83.12±.13             & 70.08±1.66            & 51.52±.90             & 62.55±2.34            \\
      LR-Reset (Ours) & \underline{94.83±.26} & 73.38±2.32          & 82.99±.20             & \bfseries 73.79±.62   & 71.73±.20             & 75.52±.12             \\
      LR-Reset Oracle & 95.12±.21             & ---                 & ---                   & ---                   & 71.88±.26             & ---                   \\ \bottomrule
   \end{tabular}
   \label{tab:lr_resetting}
\end{table}

As can be seen in Table~\ref{tab:lr_resetting}, our approach clearly outperforms weight-resetting on all but \textit{Covertype}, but rarely yields an advantage over a static schedule.
Since the oracle variant of our resetting approach, that was triggered only for timesteps with drift, shows only marginally better results, this performance gap is not caused by the drift detector.
Rather, it appears that using a larger initial learning rate and slower decay is sufficient to ensure adequate adaptability to concept drift throughout a data stream, while providing better stability in later stages.
Overall, a slower but static decay paired with a larger initial learning rate seems to be preferable to a more aggressive schedule with our drift resetting mechanism, unless severe concept drift as in \textit{RBF incremental} is expected.

With accuracy values within $1\sigma$ of each other on all evaluated streams, the stepwise decay shows almost identical performance to the exponential decay.
The accuracy of the cyclic schedule for \textit{RBF incremental} and \textit{Covertype}, on the other hand, significantly outperforms the other static and adaptive schedules, but lags behind on all other streams.
We also did not find an order of magnitude improvement in convergence speed as observed by~\citep{smithSuperConvergenceVeryFast2018a} for the scenario studied.

\def\asize{0.5\textwidth}
\begin{figure}[ht]
   \centering
   \begin{subfigure}[b]{\asize}
      \includegraphics[width=\textwidth]{figures/lr_norms_schedules_insects_abrupt.pdf}
      \caption{Schedules}
      \label{fig:prequential_schedulers_insects}
   \end{subfigure}
   \begin{subfigure}[b]{0.99\textwidth - \asize}
      \includegraphics[width=\textwidth]{figures/lr_norms_optims_insects_abrupt.pdf}
      \caption{Adaptive Optimizers}
      \label{fig:prequential_optims_insects}
   \end{subfigure}
   \caption{Prequential accuracy and learning rate for different schedules and adaptive optimizers on \textit{Insects abrupt} dataset. Concept drifts are marked by dashed lines. Accuracy is exponentially smoothed with a decay factor of 0.75.}
\end{figure}

\subsection{Adaptive Learning Rates}

Our results for adaptive optimizers displayed in Table~\ref{tab:results_adaptive_optims}, show a strong data dependency as none of the evaluated algorithms significantly outperforms its competitors on average.
However, since SGD yields the best accuracy on \textit{RBF abrupt} but is clearly surpassed on \textit{Insects abrupt}, the type of concept drift does not seem to be significant.
Due to its simplicity and favorable computational efficiency, it appears that standard SGD should be selected out of the non-parameter-free approaches in most cases.
The SGD variant of Hypergradient Descent (HD)~\citep{baydinOnlineLearningRate2018} and WNGrad~\citep{wuWNGradLearnLearning2020} on the other hand seem to rarely be optimal choices.

In the category of learning rate free optimizers COCOB~\citep{orabonaTrainingDeepNetworks2017}, outperformed its competitors on all but two datasets.
It comes close to or even exceeds the best tuned approaches in terms of accuracy.
Although yielding lower accuracy on average, DoG also comes within reach of the tuned methods, while offering much better runtime and memory efficiency compared to COCOB (see Table~\ref{tab:param_free_optims}).
Mechanic~\citep{cutkoskyMechanicLearningRate2023} and D-Adaptation~\citep{defazioLearningRateFreeLearningDAdaptation2023a} performed significantly worse than their competitors on the evaluated streams.

The learning rate curves shown in Figure~\ref{fig:prequential_schedulers_insects}, provide an indication regarding the reason for the poor performance of WNGrad and D-Adaption.
Whereas the learning rate of DoG quickly approaches the tuned SGD learning rate, WNGrad and D-Adaptation diverge considerably from it.

The learning rate of the best performing Adam exhibits spikes for most change points, suggesting some form of adaptability to drift.
However, since the much worse performing Mechanic shows similar spikes, this is unlikely to be contributing significantly to Adam's high accuracy on \textit{Insects abrupt}.
Instead, it likely stems from its second moment scaling, which is also a feature of the similarly performing AdaGrad.

\begin{table}[ht]
   \centering
   \scriptsize
   \caption{Average prequential accuracy [\%] for adaptive optimizers and SGD. Best values are shown in \textbf{bold}, values within the $1\sigma$ interval of best values \underline{underlined}.}
   \begin{tabular}{llcccccccc}
      \toprule
                                                         & Optimizer                                                                          & RBF abrupt          & RBF incr.            & Covertype             & Electricity          & Insects abrupt      & Insects gradual     \\
      \midrule
      \multirow{5}{*}{\rotatebox[origin=c]{90}{Tuned}}   & SGD                                                                                & \bfseries 94.79±.32 & 70.95±2.89           & \bfseries 83.42±.50   & 73.77±.40            & 71.50±.08           & 75.31±.21           \\
                                                         & Adam \citep{kingmaAdamMethodStochastic2017b}                                       & 93.45±.30           & 69.26±5.14           & 79.01±.27             & 69.79±.54            & \bfseries 75.38±.24 & 75.78±.74           \\
                                                         & AdaGrad \citep{duchiAdaptiveSubgradientMethods2011}                                & 92.45±1.37          & 52.87±6.62           & 81.68±.35             & \bfseries 76.99±1.20 & 74.87±.40           & \bfseries 77.15±.27 \\
                                                         & WNGrad  \citep{wuWNGradLearnLearning2020}                                          & 87.30±.68           & 44.92±.73            & 76.98±.15             & 70.80±.59            & 66.25±.19           & 66.75±.40           \\
                                                         & HD \citep{baydinOnlineLearningRate2018}                                            & 93.92±.31           & \bfseries 72.29±2.90 & \underline{83.36±.25} & 73.83±.32            & 70.67±.06           & 73.37±.21           \\ \midrule
      \multirow{4}{*}{\rotatebox[origin=c]{90}{LR-Free}} & COCOB                               \citep{orabonaTrainingDeepNetworks2017}        & \bfseries 93.40±.38 & 63.52±2.70           & 82.27±.46             & \bfseries 84.30±.56  & \bfseries 74.75±.11 & \bfseries 77.00±.05 \\
                                                         & DoG \citep{ivgiDoGSGDBest2023}                                                     & 92.72±.59           & \bfseries 73.17±2.72 & \bfseries 83.07±.64   & 71.53±.70            & 70.59±.26           & 74.01±.21           \\
                                                         & D-Adapt                    \citep{defazioLearningRateFreeLearningDAdaptation2023a} & 74.91±4.22          & 45.47±2.75           & 76.69±.79             & 66.03±1.75           & 50.05±11.26         & 48.21±10.62         \\
                                                         & Mechanic \citep{cutkoskyMechanicLearningRate2023}                                  & 88.94±.58           & 49.26±1.44           & 78.67±.18             & 50.73±7.60           & 55.31±21.47         & 65.80±.53           \\
      \bottomrule
   \end{tabular}
   \label{tab:results_adaptive_optims}
\end{table}

It may also be noted that the learning rates of the COCOB, Adam and Mechanic optimizers with parameter-specific learning rates exceed those of single value step sizes by multiple orders of magnitude.
This is an effect of second moment scaling, which creates larger learning rates for parameters with small and consistent gradients~\citep{cutkoskyMechanicLearningRate2023}.
Therefore, the norms of parameter updates generated by these approaches are not necessarily larger.


\subsection{Learning Rate Tuning}

We evaluate our pre-tuning approach by using either 500 or 1000 instances held out at the beginning of each stream for tuning. We assess Multi-Layer Perceptrons (MLPs) with 64 or 128 hidden units per layer and either one or three hidden layers.
Although our technique consistently performed well across all architectures, we specifically highlight the smallest network as it best represents resource-critical streaming applications. For an overview of results across all architectures, please refer to our supplementary material.

To determine the learning rate and decay factor, we rely on the optimal mean prequential accuracy. This choice accounts for the potential bias towards learning rates associated with lower initial losses, as loss values often decrease notably during training.

Figure~\ref{fig:pretune_lr_accuracy} shows the absolute difference between the learning rate resulting from the pre-tuning process and the optimal value $|\eta_p - \eta^*|$ at each tuning step averaged over all real-world datasets.
Batch tuning with 800 training and 200 validation samples initially yields a better approximation of the optimal learning rate. However, our streaming-specific approach undercuts the baseline after 1000 tuning steps, consistently decreasing and ultimately reaching approximately half of the approximation error observed in batch tuning.
The performance also remains nearly identical, even when using only 500 samples for tuning, which demonstrates the data efficiency of pre-tuning.
The superior performance of our approach is also reflected in the accuracy scores depicted in the right subfigure of Figure~\ref{fig:pretune_lr_accuracy}.
\begin{figure}[hb]
   \centering
   \includegraphics[width=\textwidth]{figures/pretune_1x64_acc_lr_exp_schedule.pdf}
   \caption{Absolute difference between pre-tuned and optimal learning rate and resulting accuracy on data streams for SGD and an exponential learning rate schedule with 500 or 1000 tuning samples. DoG is not included in first subplot since it is not intended for use with LR decay. Results are averaged over all real-world datasets. The shaded area represents the 1$\sigma$-interval.}\label{fig:pretune_lr_accuracy}
\end{figure}
In fewer than 1000 steps, our approach outperforms both conventional tuning and DoG.
We selected DoG as a baseline due to being the best performing parameter-free technique within the group of optimizers with a global learning rate that is most comparable to our approach.

In conclusion, our proposed tuning approach enables significantly better learning rate selection for prequential evaluation on data streams compared to both conventional tuning and DoG.
Additionally, pre-tuning has the benefit that once completed, no additional memory or runtime costs are incurred.
In streaming applications, where computing resources are often times a limiting factor, this could be a critical advantage.
Although, if computational efficiency is insignificant, the highly performant but expensive COCOB~\citep{orabonaTrainingDeepNetworks2017} or the slightly less performant and much less expensive DoG~\citep{ivgiDoGSGDBest2023} may be more appropriate.


\section{Conclusion}

In this work, we investigate the influence and selection of the learning rate and optimization procedure with respect to training neural networks in streaming environments.
We first provide theoretical background on discrepancies between learning rate optimization in conventional batch learning and online learning.
Based on these differences, we derive a simple mechanism resetting the learning rate on concept drift occurrences.
We then give an overview learning rate free algorithms popular in batch learning, which we compare experimentally on multiple synthetic and real-world datasets,
finding both COCOB and DoG to come close to the performance of optimizers with tuned learning rates.
Lastly, we introduce a streaming specific learning rate tuning approach that grants significant performance increases over conventional tuning via a train-validation split.

\bibliography{collas2024_conference}
\bibliographystyle{collas2024_conference}

\appendix
\section{Appendix}
You may include other additional sections here.

\end{document}
