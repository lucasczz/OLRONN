% This file was adapted from ICLR2022_conference.tex example provided for the ICLR conference
\documentclass{article} % For LaTeX2e
\usepackage{collas2024_conference,times}
\usepackage{easyReview}

\usepackage{booktabs}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{caption, subcaption}
\usepackage{graphicx}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}


\newcommand{\cmark}{\ding{51}} % checkmark symbol
\newcommand{\xmark}{\ding{55}} % X-mark symbol

% Please leave these options as they are
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=purple,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\usepackage{cleveref}


\title{Optimizing the Learning Rate for Online Training of Neural Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \collasfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro  \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Random University\\
Country \\
\texttt{\{hippo,brain\}@cs.random.edu} \\
\And % Use And to have authors side by side
Koala Learnus \& D. Q. ResNet  \\
Department of Computational Neuroscience \\
University of Random City \\
Another Country \\
\texttt{\{koala,net\}@random.rand} \\
\AND % Use AND to have authors block one under the other
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\collasfinalcopy % Uncomment for camera-ready version, but NOT for submission.

%\preprintcopy % Uncomment for the preprint version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
   Efficient training via gradient-based optimization techniques is an essential building block to the success of artificial neural networks. Extensive research on the impact and the effective estimation of an appropriate learning rate has partly enabled these techniques. Despite the proliferation of data streams generated by IoT devices, digital platforms, and many more, previous research has been primarily focused on batch learning, which assumes that all training data is available a priori. However, characteristics such as the gradual emergence of data and distributional shifts, also known as \textit{concept drift}, pose additional challenges. Therefore, the findings on batch learning may not apply to streaming environments, where the underlying model needs to adapt on the fly each time a new data instance appears. In this work, we seek to address this knowledge gap by (i) evaluating and comparing typical learning rate schedules and optimizers, (ii) exploring adaptations of these techniques, and (iii) providing insights into effective learning rate tuning in the context of stream-based training of neural networks.
\end{abstract}

\section{Introduction}
Artificial neural network models have shown exceptional performance in various domains.
One of the main factors leading to such outstanding results is the choice of the optimization method used to train the target model.
Almost all modern applications of neural architectures use first-order stochastic optimization methods such as \textit{stochastic gradient descent} (SGD), which iteratively update the parameters of the underlying model based on gradient information.
One of the most essential variables of such algorithms is the step size or \textit{learning rate} (LR).

As a result, many techniques for setting and optimizing the learning rate have emerged over the years (see Figure~\ref{fig:lr_overview}).
For example, based on prior knowledge, the learning rate can be set as a fixed value or as a schedule that changes the step size over time.
Alternatively, one could use an adaptive learning rate technique that considers historical gradient information to modify the learning rate at each iteration.
\begin{figure}[b]
   \centering
   \includegraphics[width=0.8\textwidth]{figures/lr_overview.pdf}
   \caption{Overview of different learning rate optimization approaches.}
   \label{fig:lr_overview}
\end{figure}
The above methods are well-researched in batch learning scenarios, where all training data is available a priori.
Despite the increasing prevalence of online learning environments, where data becomes available as part of a data stream, their use in such scenarios has received little research attention.

According to \citet{bifetMOAMassiveOnline2010}, a machine learning model operating on a data stream must be able to
\begin{enumerate}
   \item[R1:] process a single instance at a time,\label{rq:single_instance}
   \item[R2:] process each instance in a limited amount of time,\label{rq:limited_time}
   \item[R3:] use a limited amount of memory,\label{rq:limited_memory}
   \item[R4:] predict at any time,\label{rq:predict_any_time}
   \item[R5:] adapt to changes in the data distribution.\label{rq:adapt_to_drift}
\end{enumerate}

These requirements pose additional challenges in selecting an appropriate optimizer and learning rate.
To enable more informed decisions when optimizing the learning rate, we provide insight into these challenges and empirically evaluate commonly used optimization techniques in an online learning setting (i).
Furthermore, we introduce a \textit{drift reset} mechanism to adapt the learning rate to concept drifts that commonly occur in streaming environments (ii).
Finally, we propose a \textit{pre-tuning} approach to make effective use of any data that may be available prior to stream-based learning (\textit{pre-stream}) for learning rate tuning (see Figure~\ref{fig:lr_overview}) (iii).

\section{Learning Rate in First-Order Optimization}

In the following, we will explain the theoretical background of the learning rate hyperparameter in first-order stochastic optimization.
First-order stochastic optimization algorithms, such as stochastic gradient descent, typically aim to solve the following problem
\begin{equation}
   \min_{\vtheta} \E_{\vx \sim \ptrain} [L(f(\vx; \vtheta), y)],
\end{equation}
where $L$ is a loss function that quantifies the prediction error of the model given model parameters $\vtheta$, data samples $\vx$ and their corresponding target outputs $y$, both sampled from the distribution $\ptrain$ defined by a set of training data.
The blueprint process for solving this problem using first-order stochastic optimization consists of the following steps for each iteration $t \in 0, \ldots, T$:
\begin{enumerate}
   \item Draw a mini-batch of samples $\vx_t$ from the distribution $p(\vx)$.
   \item Compute the loss $L_t = L(f(\vx_t; \vtheta_t), y_t)$ for examples $\vx_t$, labels $y_t$\footnotemark[1] and current parameters $\vtheta_t$.
   \item Compute gradient $\vg_t = \nabla_{\vtheta_t} L_t$ with respect to the parameters.
   \item Update the parameters for the next iteration using $\vg_t$ and possibly information from previous iterations.
\end{enumerate}

\footnotetext[1]{Note that for ease of notation, we denote $\vx_t$ and $y_t$ as vectors and scalars even if they are typically mini-batches of multiple examples.}

For basic SGD, we can define the parameter update performed at the end of each iteration as
\begin{equation}\label{eq:sgd_update}
   \vtheta_{t+1}  = \vtheta_{t} - \eta_t \cdot \vg_t,
\end{equation}
where $\eta_t$ denotes the step size or \textit{learning rate} at timestep $t$.

The primary trade-off concerning $\eta$ is that increasing it speeds up convergence but increases stochasticity and the risk of divergence~\citep{bengioPracticalRecommendationsGradientbased2012}.
In fact, \citet{smithBayesianPerspectiveGeneralization2018}, found that the “noise scale” of SGD is tied to $\eta$~\citep{smithBayesianPerspectiveGeneralization2018}.

\subsection{Learning Rate Schedules}

Often, the performance of a model can be improved by using a schedule that changes the learning rate as training progresses, yielding a vector of step sizes $\bm{\eta}$ that defines a step size $\eta_t$ for each timestep $t \in \{0, \ldots, T\}$~\citep{wuDemystifyingLearningRate2019b}.
For example, to ensure fast convergence early in training while mitigating jumping around potential minima later, it is common to use a decaying schedule that starts with a large learning rate and decreases over time.
An additional benefit of this approach is potentially better generalization since larger learning rates can help skip sharp minima with poor generalization~\citep{hochreiterFlatMinima1997,chaudhariEntropySGDBiasingGradient2017}.

Commonly used forms of decay are exponential decay, where $\eta_{t}$ is calculated as
$\eta_{t} = \eta_1 \cdot \gamma^t$,
with $\gamma < 1$, and stepwise decay, which for a regular interval between steps of length $s$ is given as $\eta_1 \cdot \gamma^{\lfloor t/s \rfloor}$.
Another common approach is to decay $\eta$ each time the training loss plateaus for a given number of iterations. Other popular schedules include cyclic learning rates that let $\eta$ oscillate between two values over a predefined interval.
For a triangular cycle, the learning rate is defined as
\begin{equation}
   \eta_t = \eta_1 + \frac{\hat{\eta} - \eta_1}{2s} \cdot \min_{i} \{|t-i\cdot s|\},
\end{equation}
where $\hat{\eta}$ is the learning rate at the midpoint of each cycle of length $s$.
Some studies~\citep{smithCyclicalLearningRates2017, smithSuperConvergenceVeryFast2018a} have found that cyclic schedules can significantly speed up the convergence of neural networks, in some cases even compared to adaptive techniques like Adam~\citep{kingmaAdamMethodStochastic2017b}.
While there are many alternatives, we focus on exponential, step, and cyclic learning rates as some of the most commonly used generic schedules.
For a comprehensive overview and detailed analysis of learning rate policies, see \citet{wuDemystifyingLearningRate2019b}.

\subsection{Adaptive Learning Rates}

Several studies have proposed adaptive optimizers that increase the robustness of the training process with respect to the learning rate.
These optimizers adjust the step size based on previous gradients at each step~\citep{duchiAdaptiveSubgradientMethods2011}.

One of the earlier techniques in this category is \textit{AdaGrad}~\citep{duchiAdaptiveSubgradientMethods2011}, which scales the learning rate based on the sum of squares of past gradients for each parameter, resulting in a parameter-specific step size.
Several other approaches, such as \textit{AdaDelta}~\citep{zeilerADADELTAAdaptiveLearning2012a} and \textit{RMSProp}, later built on AdaGrad's scaling approach.
The same is true for the widely used \textit{Adam} optimizer~\citep{kingmaAdamMethodStochastic2017b}, which adds a momentum term from prior gradients to speed up convergence for parameters with consistent derivatives.
Another AdaGrad-based optimizer is \textit{WNGrad}~\citep{wuWNGradLearnLearning2020}, which adaptively scales each parameter update based on the squared sum of past gradients.

So-called \textit{parameter-free} gradient-based optimization approaches aim to eliminate the learning rate completely by optimizing it as training progresses.
For example, the \textit{COCOB} algorithm~\citep{orabonaTrainingDeepNetworks2017} models parameter optimization as a gambling problem, where the goal is to maximize the reward from betting on each gradient.
The resulting strategy is equivalent to running a meta-optimization algorithm that estimates the expected optimal learning rate~\citep{orabonaTrainingDeepNetworks2017}.
Several other studies~\citep{schraudolphLocalGainAdaptation1999,vanervenMetaGradMultipleLearning2016a,baydinOnlineLearningRate2018,cutkoskyMechanicLearningRate2023} have also used the idea of learning $\eta$ via a meta-optimization process.
The \textit{hypergradient descent} (HD) approach~\citep{baydinOnlineLearningRate2018}, for example, adapts the learning rate of a base optimizer like SGD using a meta-gradient descent procedure.
However, this does not remove the learning rate entirely but replaces it with a less sensitive hypergradient step size.
\textit{Mechanic}~\citep{cutkoskyMechanicLearningRate2023} pursues the same goal by applying a meta \textit{online convex optimization} (OCO) algorithm to an arbitrary base optimizer\add{, while \textit{stochastic meta-descent} (SMD) by \citet{schraudolphLocalGainAdaptation1999}, which extends \citeauthor{suttonAdaptingBiasGradient1992}'s work on linear systems \citep{suttonAdaptingBiasGradient1992}, uses second-order information for local adaptations of the learning rate}.


Research has shown that in an OCO setting with stationary data, the worst-case optimal fixed step size for SGD is
\begin{equation}\label{eq:oco_optimal_lr}
   \eta^* = \frac{||\vtheta_1 - \vtheta^*||}{\sqrt{\sum_{t=1}^{n} ||\vg_t||^2}}.
\end{equation}
Multiple parameter-free optimizers make use of this notion.
As its name suggests, the \textit{Distance over Gradients} (DoG)~\citep{ivgiDoGSGDBest2023} algorithm estimates the unknown numerator in \Eqref{eq:oco_optimal_lr} as the maximum distance $\max_{i<t}||\vtheta_1 - \vtheta_i||$ between the initial parameters and the parameters of all previous iterations.
DoG additionally uses polynomial decay averaging as proposed by~\citet{shamirStochasticGradientDescent2012}.
\textit{D-Adaptation} by \citet{defazioLearningRateFreeLearningDAdaptation2023a}, on the other hand, employs weighted dual averaging~\citep{duchiDualAveragingDistributed2012} to compute bounds on the distance between initial and optimal parameters.

\begin{table}[hb]
   \centering
   \small
   \caption{
      Overview of additional time- and space-complexity of evaluated adaptive first-order optimizers compared to basic SGD.
      Values are given in big O notation with respect to the number of model parameters $d$.
   }\label{tab:param_free_optims}
   \begin{tabular}{@{}lllcc@{}}
      \toprule
      Optimizer                & Runtime            & Space             & Param. specific & LR free \\ \midrule
      AdaGrad                  & $\mathcal{O}(5d)$  & $\mathcal{O}(1d)$ & \cmark          & \xmark  \\
      Adam                     & $\mathcal{O}(12d)$ & $\mathcal{O}(2d)$ & \cmark          & \xmark  \\
      WNGrad                   & $\mathcal{O}(2d)$  & $\mathcal{O}(0)$  & \xmark          & \xmark  \\
      COCOB                    & $\mathcal{O}(14d)$ & $\mathcal{O}(4d)$ & \cmark          & \cmark  \\
      HD\footnotemark[2]       & $\mathcal{O}(2d)$  & $\mathcal{O}(1d)$ & \xmark          & \xmark  \\
      Mechanic\footnotemark[2] & $\mathcal{O}(10d)$ & $\mathcal{O}(1d)$ & \cmark          & \cmark  \\
      DoG                      & $\mathcal{O}(5d)$  & $\mathcal{O}(1d)$ & \xmark          & \cmark  \\
      D-Adapt\footnotemark[2]  & $\mathcal{O}(6d)$  & $\mathcal{O}(2d)$ & \xmark          & \cmark  \\
      \bottomrule
   \end{tabular}
\end{table}
\footnotetext[2]{Variant with SGD as the underlying optimizer.}

Although adaptive optimization techniques seem intuitively well suited for non-stationary data, their application to data streams has yet to be investigated.
Therefore, we assess the suitability of some of the most prominent adaptive optimizers, listed in Table~\ref{tab:param_free_optims}, for stream-based learning.

\section{Learning Rate in Online Learning}

Optimizing the learning rate in a batch learning setting involves minimizing the expected loss on a hold-out validation data set.
Formally, we can express this task as
\begin{equation}
   \label{eq:batch_lr_optim}
   \min_{\bm{\eta}} \E_{(\vx, y) \sim \hat{p}_{\rm{val}}} L(f(\vx; \vtheta_n), y),
\end{equation}
where samples $(\vx, y)$ are drawn from a distribution $\hat{p}_{\rm{val}}$ defined by a validation dataset and $\vtheta_n$ are the model parameters at the end of training that depend on the learning rate schedule $\bm{\eta}$.
In online learning, where data is generated incrementally, this notion of learning rate optimization is not feasible.
Due to requirements \textbf{R1-R5}, models operating in an online learning environment should be evaluated in a \textit{prequential} manner~\citep{bifetMOAMassiveOnline2010}, where each sample $\vx_t$ in the data stream is first used to test and then to train the model, ensuring that testing is done on previously unobserved data.

Training in such a scenario can be more accurately modeled as an online convex optimization problem~\citep{shalev-shwartzOnlineLearningOnline2011,hazanIntroductionOnlineConvex2016}, where the optimizer suffers a loss $L(f(\vx; \vtheta_t), y)$ and produces updated parameters $\vtheta_{t+1}$ at each iteration of the data stream.
Learning rate optimization in this setting can be formulated as
\begin{equation}
   \label{eq:stream_lr_optim}
   \min_{\bm{\eta}} \sum_{t=1}^{n} \E_{(\vx, y) \sim \hat{p}_{\rm{stream}}^{(t)}} L(f(\vx; \vtheta_t), y).
\end{equation}

Compared to \Eqref{eq:batch_lr_optim}, \Eqref{eq:stream_lr_optim} features some key differences.
Due to the requirement to be able to predict at any time (\textbf{R4}), the goal is to minimize the expected total sum of losses over all timesteps of the prequential evaluation process instead of the validation loss for the final parameters $\vtheta_n$.
Therefore, the speed of convergence is more critical in the streaming setting, while the performance of the final $\vtheta_n$ parameter has a much smaller impact.
Since memory is limited (Requirement~\ref{rq:limited_memory}), it is also impossible to continue training on previously observed data as long as the loss decreases, putting even more emphasis on fast adaptation.
At the same time, a higher learning rate that temporarily increases the loss by skipping local minima may be suboptimal with respect to \Eqref{eq:stream_lr_optim}, even if it eventually leads to a lower loss.
Another difference to conventional batch learning is that the data distribution $\hat{p}_{\rm{stream}}^{(t)}$ is time-dependent because streams are often subject to distributional changes such as \textit{concept drifts} over time \citep{widmerLearningPresenceConcept1996}.
While other forms of drifts exist, we use \textit{concept drift} as an umbrella term for any kind of distributional shift in the following.

\subsection{Learning Rate Tuning}\label{subsec:pre-tuning}

Although strict online learning assumes that no data is available prior to the deployment of a model in the streaming environment, limited amounts of data may be available prior to the stream learning process in many real-world applications.
Such data can then be used to tune the learning rate of the model before stream deployment (pre-stream).
However, the differences in evaluation schemes described above may cause conventional learning rate tuning to produce poor results for stream-based learning.
In offline learning, learning rate tuning is typically performed by repeating the training process for a grid of possible parameter values and selecting the value that yields the best target metric on a held-out validation set after the training is completed.
As a result of only considering the performance at the end and not throughout the training process, this tuning approach may result in poor results with respect to the goal of learning rate optimization in streaming environments (see \Eqref{eq:stream_lr_optim}).
Therefore, we propose the modified tuning approach described in \Cref{alg:pre-tuning} aimed at approximating \Eqref{eq:stream_lr_optim}, which we call learning rate \textit{pre-tuning}.

\begin{algorithm}
   \caption{Pre-Stream Learning Rate Tuning for Online Learning Models}\label{alg:pre-tuning}
   \begin{algorithmic}[1]
      \Require Set of learning rate values $\sH$, set of schedule parameter values $\sG$, data samples $\sX$
      \Require Optimization function $o$, metric function to tune $m$, number of tuning steps $n_{\rm{steps}}$

      \State $\sS \gets \{(\vx_i, y_i) \sim \hat{p}_{\sX}| \forall i \in \{1, \ldots, n_{\text{steps}}\} \}$ \Comment{Create artificial stream by sampling $\sX$ with replacement.}
      \State $v^* \gets -\infty$
      \For{$\eta$ \textbf{in} $\sH$, $\gamma$ \textbf{in} $\sG$}
      \State $v \gets v_{\textrm{init}}$ \Comment{Initialize metric value.}
      \For{$\vx, y$ \textbf{in} $\sS$}
      \State $\hat{y} \gets f(\vx, \vtheta)$ \Comment{Calculate predictions with model function $f$.}
      \State $v \gets m(\hat{y}, y, v)$ \Comment{Update metric value.}
      \State $\vg \gets \nabla_{\vtheta} L(\hat{y}, y)$ \Comment{Calculate gradient of predictive loss w.r.t. $\vtheta$.}
      \State $\vtheta, \eta \gets o(\vtheta, \vg, \eta, \gamma)$ \Comment{Update model parameters $\vtheta$ and learning rate $\eta$.}
      \EndFor
      \If{$v > v^*$}
      \State $\eta^* \gets \eta$ \Comment{Update best learning rate.}
      \State $\gamma^* \gets \gamma$ \Comment{Update best schedule parameters.}
      \EndIf
      \EndFor
      \State \textbf{return} $\eta^*, \gamma^*$ \Comment{Return best hyperparameter values.}
   \end{algorithmic}
\end{algorithm}

To emulate the data stream to be processed after tuning, we continuously draw samples with replacement from the tuning data $\sX$ in a bootstrapping procedure instead of training on all data for multiple epochs.
By doing so, we aim to increase the variability of the data and, thus, the similarity to a real data stream.
We then optimize the learning rate and any optional learning rate schedule parameters $\gamma$ with respect to the selected performance metric $m$ over the emulated stream $\sS$.
While we use a simple grid search in \cref{alg:pre-tuning}, any parameter search technique could be used for this purpose.
We provide a detailed experimental evaluation of our approach in Section~\ref{sec:experiments}.

\subsection{Concept Drift Adaptation}

Concept drift requires repeated adaptation of the model parameters.
If post-drift training is interpreted as a new online optimization problem, the worst-case optimal learning rate can be computed according to \Eqref{eq:oco_optimal_lr}, replacing the initial parameter values $\vtheta_1$ with the values at the time of drift onset $\vtheta_{t_{\rm{drift}}}$.
As a result, stronger drifts that cause $\vtheta^*$ to move away from $\vtheta_{t_{\rm{drift}}}$ can benefit from larger learning rates.

Based on this notion, we propose a simple adaptation to decaying learning rate schedules that resets $\eta$ to its original value when a concept drift is detected.
An exponential schedule modified with our approach will thus yield learning rates of
\begin{equation}
   \eta_t = \eta_1 \cdot \gamma^{t-t_{\rm{drift}}},
\end{equation}\label{eq:drift_reset}
where $t_{\rm{drift}}$ marks the timestep at which the drift was last detected.
For drift detection, we apply either \textit{ADWIN}~\citep{bifetLearningTimeChangingData2007} or the Kolmogorov-Smirnov test~\citep{masseyKolmogorovSmirnovTestGoodness1951} to the prequential losses.
To avoid detecting loss decreases as concept drifts, we test only for increases of the prequential loss.

Our approach is similar to forgetting mechanisms commonly used in conventional online learning~\citep{gamaSurveyConceptDrift2014}.
To improve model plasticity, such mechanisms \remove{partially or completely} reset the current model parameters to their initial values.
\remove{However, we hypothesize that this approach is unsuitable for neural-based approaches. The reason is that, under the assumption of convexity, the newly initiated parameters must be closer to the optimal parameters $\vtheta^*$ than the current parameters to be beneficial.}
\add{
   While the simplest implementation of forgetting resets all parameters, there is a variety of more targeted approaches for increasing model plasticity.
   For instance, \citet{dohareLossPlasticityDeep2023} suggest computing a utility metric and resetting neurons that receive a low utility value, while \citet{paikOvercomingCatastrophicForgetting2019a} instead adapt the learning rate for each neuron using a utility metric.
   Similarly, \citet{elsayedUtilitybasedPerturbedGradient2023} limits changes to high-utility-weights while intentionally perturbing low-utility-weights.
}

\remove{
   We experimentally compare our approach with this weight-reset mechanism in Section 4.}
   

\section{Experiments}\label{sec:experiments}

We empirically evaluate our hypotheses using the following setup\footnote[3]{Code available at \url{anonymous.4open.science/r/LODL-D458/}.}:

We use synthetic and publicly available real-world classification datasets with different sizes and types of concept drift, listed in Table~\ref{tab:datasets}.
Our evaluations include the \textit{Agrawal} stream-generator~\citep{agrawalDatabaseMiningPerformance1993} for which we introduce concept drift by gradually switching the function that defines the target variable between the 45,000th and the 55,000th sample.
We also use the \textit{LED} generator~\citep{gordonClassificationRegressionTrees1984}, switching 5 of the 7 relevant features with irrelevant ones between instances 25000 and 75000.
For the \textit{Random Radial Basis Function} (RBF) datasets, we incorporate abrupt (RBF\textsubscript{a}) or incremental (RBF\textsubscript{i}) concept drift by switching or moving the centroids of the data distribution.

\begin{table}[ht]
   \centering
   \small
   \caption{Datasets used for experimental evaluations.}
   \begin{tabular}{@{}clcccc@{}}
      \toprule
      Type                    & Data Stream              & Instances               & Features & Classes \\
      \midrule
      \multirow{4}{*}{Synth.} & Agrawal                  & 100,000                 & 9        & 2       \\
                              & LED                      & 100,000                 & 24       & 10      \\
                              & RBF\textsubscript{a}     & 100,000                 & 20       & 5       \\
                              & RBF\textsubscript{i}     & 100,000                 & 20       & 5       \\
      \midrule
      \multirow{5}{*}{Real}   & Covertype                & 100,000\footnotemark[4] & 54       & 7       \\
                              & Electricity              & 45,312                  & 8        & 2       \\
                              & Insects\textsubscript{a} & 52,848                  & 33       & 6       \\
                              & Insects\textsubscript{g} & 24,150                  & 33       & 6       \\
                              & Insects\textsubscript{i} & 57,018                  & 33       & 6       \\
      \bottomrule
   \end{tabular}
   \label{tab:datasets}
\end{table}

\footnotetext[4]{For Covertype we use only the first 100,000 from a total of 581,012 instances.}

We further employ the \textit{Electricity} and \textit{Covertype}~\citep{misc_covertype_31} datasets, which are commonly used to evaluate online learning models, as well as two \textit{Insects} datasets~\citep{souzaChallengesBenchmarkingStream2020} with predefined types of concept drift.
In the following sections, we exclude the results for the RBF\textsubscript{i} and Insects\textsubscript{i} datasets, which can instead be found in \Cref{app:full_results}.

With respect to the neural network architecture, we limit our investigations to multi-layer perceptrons (MLPs) with a single-hidden-layer and hidden units matching the number of input features implemented with \textit{PyTorch}~\citep{paszkePyTorchImperativeStyle2019}.
Even though more elaborate architectures could, in theory, produce better results, they also typically require more tuning and longer training than smaller models, making them less suited for streaming environments from a practical standpoint.

We tune the base learning rate $\eta_1$ of all but the parameter-free approaches using a grid search of ten geometrically spaced values and configure adaptive optimizers with their default parameter values. For HD, Mechanic, and D-Adaptation, we select SGD as the base algorithm.
We select a fixed factor $\gamma$ for decay schedules on all datasets based on what we found to perform well across several scenarios in our preliminary investigations.
For the proposed learning rate resetting mechanism, we select a smaller decay factor and set the confidence level $\delta$ for drift detection to $10^{-4}$.
For our evaluations, we process each dataset sequentially, emulating streams of mini-batches of four instances each while recording the prequential accuracy and other metrics in intervals of 25 iterations.
We refer to \Cref{app:hyperparams} for more detailed information on our experimental setup.
We report our results as averages over five random seeds.

\subsection{Learning Rate Schedules}

To evaluate the effectiveness of our learning rate resetting mechanism for drift adaptation (see \Eqref{eq:drift_reset}), we compare its average prequential accuracy to that of model weight resetting, commonly used in online learning.

As seen in Table~\ref{tab:lr_resetting}, our approach outperforms weight-resetting on all but Covertype but rarely yields an advantage over a basic exponential or fixed learning rate.
Since both variants of our resetting approach perform almost identically, this performance gap is likely not caused by the drift detector.
Instead, it appears that most concept drifts, even artificially created ones, are not severe enough for a drastic learning rate increase to be beneficial compared to the better stability of a more continuous learning rate progression.
Overall, an appropriate fixed learning rate seems to be preferable to a decaying schedule with our drift resetting mechanism for the investigated data streams.

\begin{table}[b]
   \centering
   \small
   \caption{Average prequential accuracy [\%] for static and drift adaptive learning rate schedules with SGD. LR-Reset\textsubscript{A} uses ADWIN, LR-Reset\textsubscript{K} uses Kolmogorov-Smirnov testing for drift detection. Best values are shown in \textbf{bold}, values within the $1\sigma$ interval of best values \underline{underlined}.}
   \begin{tabular}{llllllll}
      \toprule
      Schedule                  & Agrawal                    & LED                      & RBF\textsubscript{a}       & Covertype                  & Electricity                & Insects\textsubscript{a}   & Insects\textsubscript{g}   \\
      \midrule

      Fixed                     & \bfseries 79.78$\pm$1.09   & \bfseries 73.91$\pm$0.04 & \bfseries 89.57$\pm$2.61   & \underline{83.42$\pm$0.50} & \bfseries 73.77$\pm$0.40   & 71.50$\pm$0.08             & 75.31$\pm$0.21             \\
      Exp.                      & 77.98$\pm$1.36             & 73.38$\pm$0.14           & \underline{87.66$\pm$3.53} & 82.95$\pm$0.26             & \underline{73.51$\pm$0.48} & \underline{72.19$\pm$0.37} & \bfseries 75.91$\pm$0.14   \\
      Step                      & 76.56$\pm$1.13             & 72.86$\pm$0.18           & 84.98$\pm$6.46             & 82.89$\pm$0.37             & \underline{73.62$\pm$0.53} & \bfseries 72.23$\pm$0.27   & \underline{75.83$\pm$0.21} \\
      Cyclic                    & \underline{78.72$\pm$0.90} & 73.67$\pm$0.17           & \underline{88.40$\pm$4.26} & \bfseries 83.44$\pm$0.08   & 68.38$\pm$0.81             & 71.74$\pm$0.39             & 75.64$\pm$0.06             \\
      Weight-Reset              & 72.03$\pm$0.91             & 64.31$\pm$2.16           & 70.41$\pm$0.92             & 82.92$\pm$0.57             & 71.17$\pm$0.62             & 63.55$\pm$0.42             & 69.66$\pm$0.65             \\
      LR-Reset\textsubscript{A} & 78.26$\pm$0.73             & 72.95$\pm$0.13           & \underline{87.60$\pm$3.43} & 82.92$\pm$0.32             & 73.07$\pm$0.66             & 71.48$\pm$0.34             & 75.54$\pm$0.11             \\
      LR-Reset\textsubscript{K} & 78.27$\pm$1.88             & 72.95$\pm$0.13           & \underline{87.60$\pm$3.43} & 82.92$\pm$0.32             & 73.07$\pm$0.66             & 71.48$\pm$0.34             & 75.54$\pm$0.11             \\
      \bottomrule
   \end{tabular}
   \label{tab:lr_resetting}
\end{table}

\def\asize{0.5\textwidth}
\begin{figure}[ht]
   \centering
   \begin{subfigure}[b]{\asize}
      \includegraphics[width=\textwidth]{figures/lr_norms_schedules_insects_abrupt.pdf}
      \caption{Schedules}
      \label{fig:prequential_schedulers_insects}
   \end{subfigure}
   \begin{subfigure}[b]{0.99\textwidth - \asize}
      \includegraphics[width=\textwidth]{figures/lr_norms_optims_insects_abrupt.pdf}
      \caption{Adaptive Optimizers}
      \label{fig:prequential_optims_insects}
   \end{subfigure}
   \caption{Prequential accuracy and learning rate for different schedules and adaptive optimizers on Insects\textsubscript{a} dataset. Concept drifts are marked by dashed lines. Accuracy is exponentially smoothed with a decay factor of 0.75.}
\end{figure}

With accuracy values within $1\sigma$ of each other on all evaluated streams, the stepwise decay shows almost identical performance to the exponential decay.
The accuracy of the cyclic schedule for Covertype slightly surpasses the one achieved with a fixed learning rate but lags behind on most real-world streams.
We also did not find an order of magnitude improvement in convergence speed as observed by~\citep{smithSuperConvergenceVeryFast2018a} for the scenario studied.


\subsection{Adaptive Learning Rates}

Our results for adaptive optimizers displayed in Table~\ref{tab:results_adaptive_optims}, show a strong data dependency as none of the evaluated algorithms consistently outperforms its competitors.
However, since SGD yields a high average accuracy on RBF\textsubscript{a} but is clearly surpassed on Insects\textsubscript{a}, the type of concept drift does not seem to be significant.
Due to its favorable performance in terms of average accuracy and computational efficiency, SGD should be selected out of the non-parameter-free approaches in most cases.
While some of the more elaborate optimizers like Adam or the SGD variant of Hypergradient Descent (HD)~\citep{baydinOnlineLearningRate2018} and WNGrad~\citep{wuWNGradLearnLearning2020} also yield high accuracy scores in our experiments, they incur a more significant computational cost compared to SGD.

\begin{table}[ht]
   \centering
   \small
   \caption{Average prequential accuracy [\%] for different optimizers. Best values are shown in \textbf{bold}, values within the $1\sigma$ interval of best values \underline{underlined}.}
   \begin{tabular}{lllllllll}
      \toprule
                                                         & Optimizer & Agrawal                    & LED                        & RBF\textsubscript{a}       & Covertype                  & Electricity              & Insects\textsubscript{a} & Insects\textsubscript{g} \\
      \midrule
      \multirow{5}{*}{\rotatebox[origin=t]{90}{Tuned}}   & SGD       & \underline{79.64$\pm$2.03} & \underline{73.91$\pm$0.04} & \underline{89.57$\pm$2.61} & \bfseries 83.42$\pm$0.50   & 73.77$\pm$0.40           & 71.50$\pm$0.08           & 75.31$\pm$0.21           \\
                                                         & Adam      & \underline{78.91$\pm$2.40} & \bfseries 73.98$\pm$0.20   & 86.33$\pm$1.85             & 79.01$\pm$0.27             & 69.79$\pm$0.54           & \bfseries 75.38$\pm$0.24 & 75.78$\pm$0.74           \\
                                                         & AdaGrad   & \underline{79.13$\pm$1.62} & 72.55$\pm$0.31             & 81.92$\pm$3.89             & 81.68$\pm$0.35             & 76.99$\pm$1.20           & 74.87$\pm$0.40           & 77.15$\pm$0.27           \\
                                                         & WNGrad    & 76.91$\pm$0.47             & 64.66$\pm$0.34             & 80.07$\pm$0.67             & 76.98$\pm$0.15             & 70.80$\pm$0.59           & 66.25$\pm$0.19           & 66.75$\pm$0.40           \\
                                                         & HD        & \bfseries 80.19$\pm$1.56   & 73.81$\pm$0.09             & \underline{89.78$\pm$3.37} & \underline{83.36$\pm$0.25} & 73.83$\pm$0.32           & 70.67$\pm$0.06           & 73.37$\pm$0.21           \\ \midrule
      \multirow{4}{*}{\rotatebox[origin=t]{90}{LR-Free}} & COCOB     & 78.21$\pm$1.12             & \underline{73.88$\pm$0.50} & \bfseries 90.75$\pm$1.28   & 82.27$\pm$0.46             & \bfseries 84.48$\pm$0.88 & 74.75$\pm$0.11           & \bfseries 77.67$\pm$0.17 \\
                                                         & DoG       & \underline{78.77$\pm$2.25} & 73.34$\pm$0.13             & 87.04$\pm$3.13             & 83.07$\pm$0.64             & 71.53$\pm$0.70           & 70.59$\pm$0.26           & 74.01$\pm$0.21           \\
                                                         & D-Adapt   & 60.16$\pm$0.96             & 54.69$\pm$8.34             & 41.37$\pm$3.34             & 76.69$\pm$0.79             & 66.03$\pm$1.75           & 50.05$\pm$11.26          & 48.21$\pm$10.62          \\
                                                         & Mechanic  & 62.09$\pm$9.20             & 69.04$\pm$0.19             & 87.33$\pm$0.50             & 78.67$\pm$0.18             & 50.73$\pm$7.60           & 55.31$\pm$21.47          & 65.80$\pm$0.53           \\
      \bottomrule
   \end{tabular}
   \label{tab:results_adaptive_optims}
\end{table}
In the category of learning-rate-free optimizers, COCOB~\citep{orabonaTrainingDeepNetworks2017} outperformed its competitors on all but two datasets.
It comes close to or even exceeds the best tuned approaches in terms of accuracy.
Although yielding lower accuracy on average, DoG also comes within reach of the tuned methods while offering much better runtime and memory efficiency compared to COCOB (see Table~\ref{tab:param_free_optims}).
Mechanic~\citep{cutkoskyMechanicLearningRate2023} and D-Adaptation~\citep{defazioLearningRateFreeLearningDAdaptation2023a} performed significantly worse than their competitors on the evaluated streams.

The learning rate curves shown in Figure~\ref{fig:prequential_schedulers_insects} indicate the reason for the poor performance of WNGrad and D-Adaption.
Whereas the learning rate of DoG quickly approaches the optimal learning rate, WNGrad and D-Adaptation diverge considerably from it.

The best-performing Adam's learning rate exhibits spikes for most change points, suggesting some form of adaptability to drift.
However, since the much worse performing Mechanic shows similar spikes, this is unlikely to contribute significantly to Adam's high accuracy on Insects\textsubscript{a}.
Instead, it likely stems from its second-moment scaling, also featured in the similarly performing AdaGrad.

It may also be noted that the learning rates of the COCOB, Adam, and Mechanic optimizers with parameter-specific learning rates exceed those of single-value step sizes by multiple orders of magnitude.
This effect of second-moment scaling creates larger learning rates for parameters with small and consistent gradients~\citep{cutkoskyMechanicLearningRate2023}.
Therefore, the norms of parameter updates generated by these approaches are not necessarily larger.


\subsection{Learning Rate Tuning}

We evaluate our Pre-Tuning approach by using either 500 or 1000 instances held out at the beginning of each stream for tuning. We assess Multi-Layer Perceptrons (MLPs) with 64 or 128 hidden units per layer and either one or three hidden layers.

To determine the learning rate and decay factor, we rely on the optimal mean prequential accuracy. This choice accounts for the potential bias towards learning rates associated with lower initial losses, as loss values often decrease notably during training.

Figure~\ref{fig:pretune_lr_accuracy} shows the absolute difference between the learning rate resulting from the Pre-Tuning process and the optimal value $|\eta_p - \eta^*|$ at each tuning step averaged over all real-world datasets and network architectures.

Batch tuning with 800 training and 200 validation samples initially yields a better approximation of the optimal learning rate. However, our streaming-specific approach undercuts the baseline after 1000 tuning steps, consistently decreasing and ultimately reaching approximately half of the approximation error observed in batch tuning.
The performance also remains nearly identical, even when using only 500 samples for tuning, demonstrating the data efficiency of Pre-Tuning.

\begin{figure}[hb]
   \centering
   \includegraphics[width=\textwidth]{figures/pretune_all_acc_lr_exp_schedule.pdf}
   \caption{Absolute difference between pre-tuned and optimal learning rate and resulting accuracy on data streams for SGD and an exponential learning rate schedule with 500 or 1000 tuning samples. DoG is not included in first subplot since it is not intended for use with LR decay. Results are averaged over all real-world datasets and network architectures. The shaded area represents the 1$\sigma$-interval.}\label{fig:pretune_lr_accuracy}
\end{figure}

\begin{table}[ht]
   \centering
   \small
   \caption{
      Prequential accuracy of learning rate tuning approaches averaged over all investigated MLP architectures. For Pre-Tuning\textsubscript{500} and Pre-Tuning\textsubscript{1000} we run our proposed learning rate tuning approach for 2000 steps with either 500 or 1000 samples and select the learning rate yielding the highest average prequential accuracy over the tuning run.
   }\label{tab:lr_tuning}
   \begin{tabular}{llllllll}
      \toprule
      Approach                       & Agrawal                  & LED                      & RBF\textsubscript{a}     & Covertype                  & Electricity                & Insects\textsubscript{a} & Insects\textsubscript{g}   \\
      \midrule
      SGD (Best LR)                  & 83.70                    & 73.95                    & 93.84                    & 83.14                      & 74.15                      & 71.98                    & 75.28                      \\
      COCOB                          & 83.71$\pm$0.25           & 74.34$\pm$0.09           & 95.29$\pm$0.24           & 82.96$\pm$0.19             & 84.57$\pm$0.08             & 75.39$\pm$0.10           & 77.62$\pm$0.08             \\ \midrule
      DoG                            & 82.06$\pm$0.52           & \bfseries 73.71$\pm$0.11 & 91.36$\pm$0.46           & \bfseries 82.56$\pm$0.15   & 70.47$\pm$0.40             & 70.59$\pm$0.10           & 73.92$\pm$0.11             \\
      Batch Tuning                   & 81.09$\pm$1.34           & 66.67$\pm$3.28           & 91.89$\pm$0.39           & \underline{82.41$\pm$0.61} & 72.86$\pm$0.76             & 69.81$\pm$2.18           & 73.91$\pm$0.64             \\
      Pre-Tuning\textsubscript{500}  & 82.01$\pm$0.32           & 73.26$\pm$0.03           & \bfseries 92.36$\pm$0.03 & 80.25$\pm$1.86             & \underline{73.34$\pm$0.45} & \bfseries 71.81$\pm$0.04 & \bfseries 75.22$\pm$0.08   \\
      Pre-Tuning\textsubscript{1000} & \bfseries 82.56$\pm$0.25 & 73.30$\pm$0.00           & 92.18$\pm$0.38           & 80.80$\pm$1.05             & \bfseries 73.37$\pm$0.34   & \bfseries 71.81$\pm$0.06 & \underline{75.17$\pm$0.05} \\
      \bottomrule
   \end{tabular}
\end{table}

The superior performance of our approach is also reflected in the accuracy scores depicted in the right subfigure of Figure~\ref{fig:pretune_lr_accuracy}.
In fewer than 1000 steps, our approach comes closer to the optimal learning rate and also exceeds the accuracy of conventional tuning.
Notably, our approach's usage of the average prequential accuracy more accurately resembles the online learning process and sets it apart from conventional tuning, which solely focuses on performance at the end of the tuning process.
Pre-Tuning also surpasses DoG, which we selected as a baseline due to being the best-performing parameter-free technique within the group of optimizers with a global learning rate that is the most comparable to our approach.
This advantage comes despite a smaller average absolute deviation of DoG compared to our approach, which can be explained by the fact that DoG strongly underestimates the optimal learning rate at the beginning of training (see e.g. \Cref{sub@fig:prequential_optims_insects}), where fast adaptation is crucial.
Because DoG closely approaches the best learning rate for most of the stream, this barely affects its average deviation from the optimal learning rate.

The effectiveness of Pre-Tuning can also be seen in the dataset-specific results displayed in \Cref{tab:lr_tuning}.
Our tuning technique executed with 1000 samples (Pre-Tuning\textsubscript{1000}) outperforms DoG and conventional tuning on all but the LED and Covertype datasets and often approaches the best learning rate determined via tuning directly on the testing stream (SGD Best LR).
Even with 500 samples, the technique yields favorable results, indicating its usefulness in cases where data that is available pre-stream is scarce.
While much more computationally expensive than the other evaluated approaches, the COCOB optimizer provides much better accuracy scores across all investigated data streams.

In conclusion, our proposed tuning approach enables significantly better learning rate selection for prequential evaluation on data streams than conventional tuning and DoG.
Compared to learning-rate-free approaches like DoG or COCOB, Pre-Tuning also has the benefit that no additional memory or runtime costs are incurred once completed.
This could be a critical advantage in streaming applications, where computing resources are often times a limiting factor.
However, if computational efficiency is insignificant, the highly performant but expensive COCOB~\citep{orabonaTrainingDeepNetworks2017} or the slightly less performant and much less expensive DoG~\citep{ivgiDoGSGDBest2023} may be more appropriate.
The usefulness of our Pre-Tuning approach is also likely limited in cases where the available tuning data is not representative of the subsequent data stream, such as for data streams with extreme concept drift.
In such cases, approaches with a dynamic learning rate are presumably superior.

\section{Conclusion}

In this work, we investigate the influence and selection of the learning rate and optimization procedure with respect to training neural networks in streaming environments.
We first provide a theoretical background on discrepancies between learning rate optimization in conventional batch learning and online learning.
Based on these differences, we propose a simple mechanism that resets a decayed learning rate on concept drift occurrences to increase the speed of adaption.
Our empirical evaluation shows that our adaptation mechanism is much better suited when using neural networks than resetting the model weights, which is commonly done in conventional online learning.
However, we find no advantage over using an appropriate fixed learning rate.
We then experimentally compare adaptive optimization techniques that are popular for batch learning purposes in the context of online learning using multiple synthetic and real-world data streams.
We find the DoG and especially the COCOB optimizer to yield high average accuracy scores even when compared to techniques requiring manual step size tuning.
Lastly, we contribute a streaming-specific approach to learning rate tuning that grants significant performance increases over conventional tuning via a train-validation split and provides a valuable alternative to the computationally costly COCOB.

\bibliography{collas2024_conference}
\bibliographystyle{collas2024_conference}

\appendix
\section{Hyperparameter Settings for Empirical Evaluation}\label{app:hyperparams}

\begin{table}[h]
   \centering
   \caption{Variable notation as used in this work.}
   \begin{tabular}{lr}
      \toprule
      Parameter                        & Symbol       \\
      \midrule
      Model Parameters                 & $\vtheta $   \\
      Loss Function                    & $L$          \\
      Learning Rate                    & $\eta$       \\
      Learning Rate Decay Factor       & $\gamma$     \\
      Drift Detection Confidence Level & $\delta$     \\
      Steps Between LR Cycles/Updates  & $s$          \\
      Relative LR at Midpoint of Cycle & $\hat{\eta}$ \\
      \bottomrule
   \end{tabular}
\end{table}

\begin{table}[h]
   \centering
   \caption{Search spaces for learning rate of different optimizers.}
   \begin{tabular}{l l}
      \toprule
      Optimizer & Learning Rate Search Space                \\
      \midrule
      SGD       & ${2^1, 2^0, ..., 2^{-8}}$                 \\
      Adam      & ${2^{-3}, 2^{-4}, ..., 2^{-12}}$          \\
      AdaGrad   & ${2^1, 2^0, ..., 2^{-8}}$                 \\
      WNGrad    & ${10^{1.25}, 10^{0.75}, ..., 10^{-7.75}}$ \\
      HD        & ${2^{-3}, 2^{-4}, ..., 2^{-12}}$          \\
      COCOB     & $100$                                     \\
      DoG       & $1$                                       \\
      D-Adapt   & $1$                                       \\
      Mechanic  & $0.01$                                    \\
      \bottomrule
   \end{tabular}
\end{table}

\begin{table}[ht]
   \centering
   \caption{Configuration of other parameters.}
   \begin{tabular}{l l}
      \toprule
      Schedule    & Values                                  \\
      \midrule
      Exponential & $\gamma = 1 - 2^{-13}$                  \\
      LR-Reset    & $\gamma = 1 - 2^{-12}, \delta = 0.0001$ \\
      Step        & $\gamma = 0.75, s = 2000$               \\
      Cyclic      & $\hat{\eta} = 0.25, s = 8000$           \\
      \bottomrule
   \end{tabular}
\end{table}

\section{Full Experimental Results}\label{app:full_results}

In this section, we provide our full experimental results including the RBF\textsubscript{i} and Insects\textsubscript{i} datastreams as well as a detailed

\subsection{Architecture-specific Learning Rate Tuning Results}\label{app:tuning_architectures}

\begin{figure}[h]
   \centering
   \includegraphics[width=\textwidth]{figures/pretune_architectures_exp_schedule.pdf}
   \caption{Average accuracy over all evalutated real-world datasets achieved by Pre-Tuning for different network architectures.}
\end{figure}

\newpage
\subsection{Full Results on Learning Rate Tuning}\label{app:tuning_results}
\vfill

\captionsetup{hypcap=false}

\begin{center}
   \begin{sideways}
      \begin{minipage}{0.9\textheight}
         \centering
         \small
         \captionof{table}{
            Full prequential accuracy results of learning rate tuning approaches averaged over all investigated MLP architectures. For Pre-Tuning\textsubscript{500} and Pre-Tuning\textsubscript{1000} we run our proposed learning rate tuning approach for 2000 steps with either 500 or 1000 samples and select the learning rate yielding the highest average prequential accuracy over the tuning run.
         }
         \begin{tabular}{llllllllll}
            \toprule
            Approach                       & Agrawal                  & LED                      & RBF\textsubscript{a}     & Covertype                  & Electricity                & Insects\textsubscript{a} & Insects\textsubscript{g}   & Insects\textsubscript{i} & RBF\textsubscript{i}     \\ \midrule
            Oracle                         & 83.70                    & 73.95                    & 93.84                    & 83.14                      & 74.15                      & 71.98                    & 75.28                      & 60.75$\pm$10.00          & 81.52$\pm$10.00          \\
            COCOB                          & 83.71$\pm$0.25           & 74.34$\pm$0.09           & 95.29$\pm$0.24           & 82.96$\pm$0.19             & 84.57$\pm$0.08             & 75.39$\pm$0.10           & 77.62$\pm$0.08             & \bfseries 64.02$\pm$.11  & 78.59$\pm$.38            \\ \midrule
            DoG                            & 82.06$\pm$0.52           & \bfseries 73.71$\pm$0.11 & 91.36$\pm$0.46           & \bfseries 82.56$\pm$0.15   & 70.47$\pm$0.40             & 70.59$\pm$0.10           & 73.92$\pm$0.11             & 58.83$\pm$.07            & \bfseries 77.63$\pm$1.24 \\
            Batch Tuning                   & 81.09$\pm$1.34           & 66.67$\pm$3.28           & 91.89$\pm$0.39           & \underline{82.41$\pm$0.61} & 72.86$\pm$0.76             & 69.81$\pm$2.18           & 73.91$\pm$0.64             & 58.27$\pm$2.21           & 72.51$\pm$1.63           \\
            Pre-Tuning\textsubscript{500}  & 82.01$\pm$0.32           & 73.26$\pm$0.03           & \bfseries 92.36$\pm$0.03 & 80.25$\pm$1.86             & \underline{73.34$\pm$0.45} & \bfseries 71.81$\pm$0.04 & \bfseries 75.22$\pm$0.08   & 60.55$\pm$.10            & 73.19$\pm$.31            \\
            Pre-Tuning\textsubscript{1000} & \bfseries 82.56$\pm$0.25 & 73.30$\pm$0.00           & 92.18$\pm$0.38           & 80.80$\pm$1.05             & \bfseries 73.37$\pm$0.34   & \bfseries 71.81$\pm$0.06 & \underline{75.17$\pm$0.05} & 60.33$\pm$.28            & 72.98$\pm$.44            \\
            \bottomrule
         \end{tabular}
      \end{minipage}
   \end{sideways}
\end{center}

\newpage
\subsection{Full Learning Rate Scheduling Results}\label{app:scheduling_results}

\vfill

\begin{center}
   \begin{sideways}
      \begin{minipage}{0.9\textheight}
         \captionof{table}{Full results on average prequential accuracy [\%] for static and drift adaptive learning rate schedules with SGD. LR-Reset\textsubscript{A} uses ADWIN, LR-Reset\textsubscript{K} uses Kolmogorov-Smirnov testing for drift detection. Best values are shown in \textbf{bold}, values within the $1\sigma$ interval of best values \underline{underlined}.}
         \small
         \centering
         \begin{tabular}{llllllllll}
            \toprule
            Schedule                  & Agrawal                    & LED                      & RBF\textsubscript{a}       & Covertype                  & Electricity                & Insects\textsubscript{a}   & Insects\textsubscript{g}   & Insects\textsubscript{i} & RBF\textsubscript{i}    \\ \midrule
            Fixed                     & \bfseries 79.78$\pm$1.09   & \bfseries 73.91$\pm$0.04 & \bfseries 89.57$\pm$2.61   & \underline{83.42$\pm$0.50} & \bfseries 73.77$\pm$0.40   & 71.50$\pm$0.08             & 75.31$\pm$0.21             & 60.48$\pm$.20            & 57.18$\pm$2.69          \\
            Exp.                      & 77.98$\pm$1.36             & 73.38$\pm$0.14           & \underline{87.66$\pm$3.53} & 82.95$\pm$0.26             & \underline{73.51$\pm$0.48} & \underline{72.19$\pm$0.37} & \bfseries 75.91$\pm$0.14   & \bfseries 61.28$\pm$.16  & 50.75$\pm$1.38          \\
            Step                      & 76.56$\pm$1.13             & 72.86$\pm$0.18           & 84.98$\pm$6.46             & 82.89$\pm$0.37             & \underline{73.62$\pm$0.53} & \bfseries 72.23$\pm$0.27   & \underline{75.83$\pm$0.21} & \bfseries 61.18$\pm$.11  & 50.69$\pm$1.93          \\
            Cyclic                    & \underline{78.72$\pm$0.90} & 73.67$\pm$0.17           & \underline{88.40$\pm$4.26} & \bfseries 83.44$\pm$0.08   & 68.38$\pm$0.81             & 71.74$\pm$0.39             & 75.64$\pm$0.06             & 60.48$\pm$.20            & 58.67$\pm$1.63          \\
            Weight-Reset              & 72.03$\pm$0.91             & 64.31$\pm$2.16           & 70.41$\pm$0.92             & 82.92$\pm$0.57             & 71.17$\pm$0.62             & 63.55$\pm$0.42             & 69.66$\pm$0.65             & 49.97$\pm$.67            & \bfseries 61.07$\pm$.76 \\
            LR-Reset\textsubscript{A} & 78.26$\pm$0.73             & 72.95$\pm$0.13           & \underline{87.60$\pm$3.43} & 82.92$\pm$0.32             & 73.07$\pm$0.66             & 71.48$\pm$0.34             & 75.54$\pm$0.11             & 60.39$\pm$.18            & 52.88$\pm$1.72          \\
            LR-Reset\textsubscript{K} & 78.27$\pm$1.88             & 72.95$\pm$0.13           & \underline{87.60$\pm$3.43} & 82.92$\pm$0.32             & 73.07$\pm$0.66             & 71.48$\pm$0.34             & 75.54$\pm$0.11             & 60.39$\pm$.18            & 52.88$\pm$1.72          \\
            \bottomrule
         \end{tabular}

      \end{minipage}
   \end{sideways}
\end{center}

\newpage
\subsection{Full Results on Adaptive Optimization Techniques}\label{app:optimizer_results}
\vfill

\begin{center}
   \begin{sideways}
      \begin{minipage}{0.9\textheight}
         \centering
         \small
         \captionof{table}{Full results on average prequential accuracy [\%] for different optimizers. Best values are shown in \textbf{bold}, values within the $1\sigma$ interval of best values \underline{underlined}.}
         \begin{tabular}{lllllllllll}
            \toprule
                                                               & Optimizer & Agrawal                    & LED                        & RBF\textsubscript{a}       & Covertype                  & Electricity              & Insects\textsubscript{a} & Insects\textsubscript{g} & Insects\textsubscript{i} & RBF\textsubscript{i}     \\ \midrule
            \multirow{5}{*}{\rotatebox[origin=t]{90}{Tuned}}   & SGD       & 79.64$\pm$2.03             & \underline{73.91$\pm$0.04} & \underline{89.57$\pm$2.61} & \bfseries 83.42$\pm$0.50   & 73.77$\pm$0.40           & 71.50$\pm$0.08           & 75.31$\pm$0.21           & 60.48$\pm$.20            & \bfseries 57.18$\pm$2.69 \\
                                                               & Adam      & \underline{78.91$\pm$2.40} & \bfseries 73.98$\pm$0.20   & 86.33$\pm$1.85             & 79.01$\pm$0.27             & 69.79$\pm$0.54           & \bfseries 75.38$\pm$0.24 & 75.78$\pm$0.74           & \bfseries 64.17$\pm$.13  & \bfseries 60.32$\pm$3.75 \\
                                                               & AdaGrad   & \underline{79.13$\pm$1.62} & 72.55$\pm$0.31             & 81.92$\pm$3.89             & 81.68$\pm$0.35             & 76.99$\pm$1.20           & 74.87$\pm$0.40           & 77.15$\pm$0.27           & 62.51$\pm$.59            & 45.00$\pm$1.55           \\
                                                               & WNGrad    & 76.91$\pm$0.47             & 64.66$\pm$0.34             & 80.07$\pm$0.67             & 76.98$\pm$0.15             & 70.80$\pm$0.59           & 66.25$\pm$0.19           & 66.75$\pm$0.40           & 56.14$\pm$.21            & 42.06$\pm$.43            \\
                                                               & HD        & \bfseries 80.19$\pm$1.56   & 73.81$\pm$0.09             & \underline{89.78$\pm$3.37} & \underline{83.36$\pm$0.25} & 73.83$\pm$0.32           & 70.67$\pm$0.06           & 73.37$\pm$0.21           & 59.92$\pm$.18            & 56.37$\pm$5.01           \\ \midrule
            \multirow{4}{*}{\rotatebox[origin=t]{90}{LR-Free}} & COCOB     & 78.21$\pm$1.12             & \underline{73.88$\pm$0.50} & \bfseries 90.75$\pm$1.28   & 82.27$\pm$0.46             & \bfseries 84.48$\pm$0.88 & 74.75$\pm$0.11           & \bfseries 77.67$\pm$0.17 & 63.93$\pm$.17            & 51.70$\pm$2.11           \\
                                                               & DoG       & \underline{78.77$\pm$2.25} & 73.34$\pm$0.13             & 87.04$\pm$3.13             & 83.07$\pm$0.64             & 71.53$\pm$0.70           & 70.59$\pm$0.26           & 74.01$\pm$0.21           & 59.66$\pm$.22            & 55.72$\pm$2.43           \\
                                                               & D-Adapt   & 60.16$\pm$0.96             & 54.69$\pm$8.34             & 41.37$\pm$3.34             & 76.69$\pm$0.79             & 66.03$\pm$1.75           & 50.05$\pm$11.26          & 48.21$\pm$10.62          & 36.00$\pm$11.81          & 42.61$\pm$1.46           \\
                                                               & Mechanic  & 62.09$\pm$9.20             & 69.04$\pm$0.19             & 87.33$\pm$0.50             & 78.67$\pm$0.18             & 50.73$\pm$7.60           & 55.31$\pm$21.47          & 65.80$\pm$0.53           & 47.89$\pm$17.46          & 44.45$\pm$.85            \\
            \bottomrule
         \end{tabular}
      \end{minipage}
   \end{sideways}
\end{center}



\end{document}
