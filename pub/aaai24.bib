@article{al-khameesEnhancingStabilityDeep2023,
  title   = {Enhancing the Stability of the Deep Neural Network Using a Non-Constant Learning Rate for Data Stream},
  author  = {{Al-Khamees}, Hussein and {Al-A'araji}, Nabeel and {Al-Shamery}, Eman},
  year    = {2023},
  month   = apr,
  journal = {International Journal of Electrical and Computer Engineering},
  volume  = {13},
  pages   = {2123--2130},
  doi     = {10.11591/ijece.v13i2.pp2123-2130}
}

@inproceedings{baydinOnlineLearningRate2018,
  title     = {Online {{Learning Rate Adaptation}} with {{Hypergradient Descent}}},
  booktitle = {{{ICLR Proceedings}}},
  author    = {Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
  year      = {2018},
  month     = feb,
  langid    = {english}
}

@misc{bengioPracticalRecommendationsGradientbased2012,
  title         = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  author        = {Bengio, Yoshua},
  year          = {2012},
  month         = sep,
  number        = {arXiv:1206.5533},
  eprint        = {1206.5533},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv},
  langid        = {english}
}@inproceedings{bifetLearningTimeChangingData2007,
  title      = {Learning from {{Time-Changing Data}} with {{Adaptive Windowing}}},
  booktitle  = {Proceedings of the 2007 {{SIAM International Conference}} on {{Data Mining}}},
  author     = {Bifet, Albert and Gavald{\`a}, Ricard},
  year       = {2007},
  month      = apr,
  pages      = {443--448},
  publisher  = {{Society for Industrial and Applied Mathematics}},
  doi        = {10.1137/1.9781611972771.42},
  urldate    = {2023-02-22},
  isbn       = {978-0-89871-630-6 978-1-61197-277-1},
  langid     = {english},
  annotation = {ADWIN}
}

@article{bifetMOAMassiveOnline2010,
  title      = {{{MOA}}: Massive Online Analysis},
  shorttitle = {{{MOA}}},
  author     = {Bifet, Albert and Holmes, Geoffrey and Kirkby, Richard and Pfahringer, Bernhard},
  year       = {2010},
  month      = may,
  journal    = {Journal of Machine Learning Research},
  volume     = {11}
}

@incollection{bottouLargeScaleMachineLearning2011,
  title     = {Large-{{Scale Machine Learning}} with {{Stochastic Gradient Descent L\'eon Bottou}}},
  booktitle = {Statistical {{Learning}} and {{Data Science}}},
  editor    = {Bottou, Leon and Goldfarb, Bernard and Murtagh, Fionn and Pardoux, Catherine and Touati, Myriam},
  year      = {2011},
  month     = dec,
  edition   = {0},
  pages     = {33--42},
  publisher = {{Chapman and Hall/CRC}},
  doi       = {10.1201/b11429-6},
  urldate   = {2023-04-13},
  isbn      = {978-0-429-10768-9},
  langid    = {english}
}

@misc{bottouOptimizationMethodsLargeScale2018,
  title         = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author        = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  year          = {2018},
  month         = feb,
  number        = {arXiv:1606.04838},
  eprint        = {1606.04838},
  primaryclass  = {cs, math, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/1606.04838},
  urldate       = {2023-07-31},
  archiveprefix = {arxiv},
  langid        = {english}
}

@incollection{bottouStochasticGradientDescent2012,
  title     = {Stochastic {{Gradient Descent Tricks}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author    = {Bottou, L{\'e}on},
  editor    = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year      = {2012},
  series    = {Lecture {{Notes}} in {{Computer Science}}},
  pages     = {421--436},
  publisher = {{Springer}},
  address   = {{Berlin, Heidelberg}},
  doi       = {10.1007/978-3-642-35289-8\_25},
  urldate   = {2023-07-27},
  isbn      = {978-3-642-35289-8},
  langid    = {english}
}
@inproceedings{canonacoAdaptiveFederatedLearning2021,
  title     = {Adaptive {{Federated Learning}} in {{Presence}} of {{Concept Drift}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author    = {Canonaco, Giuseppe and Bergamasco, Alex and Mongelluzzo, Alessio and Roveri, Manuel},
  year      = {2021},
  month     = jul,
  pages     = {1--7},
  issn      = {2161-4407},
  doi       = {10.1109/IJCNN52387.2021.9533710}
}
@misc{carmonMakingSGDParameterFree2023,
  title         = {Making {{SGD Parameter-Free}}},
  author        = {Carmon, Yair and Hinder, Oliver},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2205.02160},
  eprint        = {2205.02160},
  primaryclass  = {cs, math, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/2205.02160},
  urldate       = {2023-07-13},
  archiveprefix = {arxiv},
  langid        = {english}
}

@misc{chaudhariEntropySGDBiasingGradient2017,
  title         = {Entropy-{{SGD}}: {{Biasing Gradient Descent Into Wide Valleys}}},
  shorttitle    = {Entropy-{{SGD}}},
  author        = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  year          = {2017},
  month         = apr,
  number        = {arXiv:1611.01838},
  eprint        = {1611.01838},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  urldate       = {2023-07-28},
  abstract      = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
  archiveprefix = {arxiv},
  langid        = {english},
  file          = {C\:\\Users\\Lucas\\Zotero\\storage\\E9VRGB2I\\Chaudhari et al. - 2017 - Entropy-SGD Biasing Gradient Descent Into Wide Va.pdf}
}

@misc{cutkoskyMechanicLearningRate2023,
  title         = {Mechanic: {{A Learning Rate Tuner}}},
  shorttitle    = {Mechanic},
  author        = {Cutkosky, Ashok and Defazio, Aaron and Mehta, Harsh},
  year          = {2023},
  month         = jun,
  number        = {arXiv:2306.00144},
  eprint        = {2306.00144},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/2306.00144},
  urldate       = {2023-06-07},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {0 citations (Semantic Scholar/arXiv) [2023-06-07]}
}

@misc{defazioLearningRateFreeLearningDAdaptation2023a,
  title         = {Learning-{{Rate-Free Learning}} by {{D-Adaptation}}},
  author        = {Defazio, Aaron and Mishchenko, Konstantin},
  year          = {2023},
  month         = may,
  number        = {arXiv:2301.07733},
  eprint        = {2301.07733},
  primaryclass  = {cs, math, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/2301.07733},
  urldate       = {2023-06-07},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {7 citations (Semantic Scholar/arXiv) [2023-06-07]}
}
@article{duchiAdaptiveSubgradientMethods2011,
  title      = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author     = {Duchi, John C. and Hazan, Elad and Singer, Yoram},
  year       = {2011},
  month      = feb,
  journal    = {Journal of Machine Learning Research},
  volume     = {12},
  number     = {61},
  pages      = {2121--2159},
  annotation = {MAG ID: 2146502635}
}
@article{duchiDualAveragingDistributed2012,
  title      = {Dual {{Averaging}} for {{Distributed Optimization}}: {{Convergence Analysis}} and {{Network Scaling}}},
  shorttitle = {Dual {{Averaging}} for {{Distributed Optimization}}},
  author     = {Duchi, J. C. and Agarwal, A. and Wainwright, M. J.},
  year       = {2012},
  month      = mar,
  journal    = {IEEE Transactions on Automatic Control},
  volume     = {57},
  number     = {3},
  pages      = {592--606},
  issn       = {0018-9286, 1558-2523},
  doi        = {10.1109/TAC.2011.2161027},
  urldate    = {2023-08-09},
  langid     = {english}
}


@article{fekriDeepLearningLoad2021,
  title      = {Deep Learning for Load Forecasting with Smart Meter Data: {{Online Adaptive Recurrent Neural Network}}},
  shorttitle = {Deep Learning for Load Forecasting with Smart Meter Data},
  author     = {Fekri, Mohammad Navid and Patel, Harsh and Grolinger, Katarina and Sharma, Vinay},
  year       = {2021},
  month      = jan,
  journal    = {Applied Energy},
  volume     = {282},
  pages      = {116177},
  issn       = {03062619},
  doi        = {10.1016/j.apenergy.2020.116177},
  urldate    = {2023-08-04},
  langid     = {english}
}

@inproceedings{ferreirajoseADADRIFTAdaptiveLearning2020,
  title      = {{{ADADRIFT}}: {{An Adaptive Learning Technique}} for {{Long-history Stream-based Recommender Systems}}},
  shorttitle = {{{ADADRIFT}}},
  booktitle  = {2020 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author     = {Ferreira Jose, Eduardo and Enembreck, Fabricio and Paul Barddal, Jean},
  year       = {2020},
  month      = oct,
  pages      = {2593--2600},
  publisher  = {{IEEE}},
  address    = {{Toronto, ON, Canada}},
  doi        = {10.1109/SMC42975.2020.9282922},
  urldate    = {2023-08-04},
  isbn       = {978-1-72818-526-2},
  langid     = {english},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-08-07]}
}

@book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year      = {2016},
  publisher = {{MIT Press}}
}

@misc{goyalAccurateLargeMinibatch2018,
  title         = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  shorttitle    = {Accurate, {{Large Minibatch SGD}}},
  author        = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  year          = {2018},
  month         = apr,
  number        = {arXiv:1706.02677},
  eprint        = {1706.02677},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/1706.02677},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv},
  langid        = {english}
}@article{hochreiterFlatMinima1997,
  title    = {Flat {{Minima}}},
  author   = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year     = {1997},
  month    = jan,
  journal  = {Neural Computation},
  volume   = {9},
  number   = {1},
  pages    = {1--42},
  issn     = {0899-7667, 1530-888X},
  doi      = {10.1162/neco.1997.9.1.1},
  urldate  = {2023-07-28},
  abstract = {We present a new algorithm for finding low-complexity neural networks with high generalization capability. The algorithm searches for a ``flat'' minimum of the error function. A flat minimum is a large connected region in weight space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to ``simple'' networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require gaussian assumptions and does not depend on a ``good'' weight prior. Instead we have a prior over input output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second-order derivatives, it has backpropagation's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms conventional backprop, weight decay, and ``optimal brain surgeon/optimal brain damage.''},
  langid   = {english}
}
@misc{ivgiDoGSGDBest2023,
  title         = {{{DoG}} Is {{SGD}}'s {{Best Friend}}: {{A Parameter-Free Dynamic Step Size Schedule}}},
  shorttitle    = {{{DoG}} Is {{SGD}}'s {{Best Friend}}},
  author        = {Ivgi, Maor and Hinder, Oliver and Carmon, Yair},
  year          = {2023},
  month         = jul,
  number        = {arXiv:2302.12022},
  eprint        = {2302.12022},
  primaryclass  = {cs, math},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/2302.12022},
  urldate       = {2023-07-21},
  archiveprefix = {arxiv},
  langid        = {english}
}
@misc{kingmaAdamMethodStochastic2017b,
  title         = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle    = {Adam},
  author        = {Kingma, Diederik P. and Ba, Jimmy},
  year          = {2017},
  month         = jan,
  number        = {arXiv:1412.6980},
  eprint        = {1412.6980},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-02-24},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {9990 citations (Semantic Scholar/arXiv) [2023-05-25]}
}
@incollection{kunchevaAdaptiveLearningRate2008,
  title     = {Adaptive {{Learning Rate}} for {{Online Linear Discriminant Classifiers}}},
  booktitle = {Structural, {{Syntactic}}, and {{Statistical Pattern Recognition}}},
  author    = {Kuncheva, Ludmila I. and Plumpton, Catrin O.},
  editor    = {Da Vitoria Lobo, Niels and Kasparis, Takis and Roli, Fabio and Kwok, James T. and Georgiopoulos, Michael and Anagnostopoulos, Georgios C. and Loog, Marco},
  year      = {2008},
  volume    = {5342},
  pages     = {510--519},
  publisher = {{Springer Berlin Heidelberg}},
  address   = {{Berlin, Heidelberg}},
  doi       = {10.1007/978-3-540-89689-0\_55},
  urldate   = {2023-08-04},
  abstract  = {We propose a strategy for updating the learning rate parameter of online linear classifiers for streaming data with concept drift. The change in the learning rate is guided by the change in a running estimate of the classification error. In addition, we propose an online version of the standard linear discriminant classifier (O-LDC) in which the inverse of the common covariance matrix is updated using the Sherman-MorrisonWoodbury formula. The adaptive learning rate was applied to four online linear classifier models on generated and real streaming data with concept drift. O-LDC was found to be better than balanced Winnow, the perceptron and a recently proposed online linear discriminant analysis.},
  isbn      = {978-3-540-89688-3 978-3-540-89689-0},
  langid    = {english},
  file      = {C\:\\Users\\Lucas\\Zotero\\storage\\WVY3R98A\\Kuncheva and Plumpton - 2008 - Adaptive Learning Rate for Online Linear Discrimin.pdf}
}
@misc{loshchilovSGDRStochasticGradient2017,
  title         = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  shorttitle    = {{{SGDR}}},
  author        = {Loshchilov, Ilya and Hutter, Frank},
  year          = {2017},
  month         = may,
  number        = {arXiv:1608.03983},
  eprint        = {1608.03983},
  primaryclass  = {cs, math},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/1608.03983},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv},
  langid        = {english}
}
@inproceedings{martensEstimatingHessianBackpropagating2012,
  title     = {Estimating the Hessian by Back-Propagating Curvature},
  booktitle = {Proceedings of the 29th {{International Coference}} on {{International Conference}} on {{Machine Learning}}},
  author    = {Martens, James and Sutskever, Ilya and Swersky, Kevin},
  year      = {2012},
  month     = jun,
  series    = {{{ICML}}'12},
  pages     = {963--970},
  publisher = {{Omnipress}},
  address   = {{Madison, WI, USA}},
  urldate   = {2023-08-08},
  isbn      = {978-1-4503-1285-1}
}
@article{miyaguchiCograConceptDriftAwareStochastic2019,
  title      = {Cogra: {{Concept-Drift-Aware Stochastic Gradient Descent}} for {{Time-Series Forecasting}}},
  shorttitle = {Cogra},
  author     = {Miyaguchi, Kohei and Kajino, Hiroshi},
  year       = {2019},
  month      = jul,
  journal    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume     = {33},
  number     = {01},
  pages      = {4594--4601},
  issn       = {2374-3468},
  doi        = {10.1609/aaai.v33i01.33014594},
  urldate    = {2023-02-24},
  copyright  = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid     = {english},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-02-24]}
}

@inproceedings{orabonaTrainingDeepNetworks2017,
  title     = {Training {{Deep Networks}} without {{Learning Rates Through Coin Betting}}},
  booktitle = {{{NIPS}}},
  author    = {Orabona, Francesco and Tommasi, Tatiana},
  year      = {2017},
  langid    = {english}
}

@article{rumelhartLearningRepresentationsBackpropagating1986a,
  title     = {Learning Representations by Back-Propagating Errors},
  author    = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year      = {1986},
  month     = oct,
  journal   = {Nature},
  volume    = {323},
  number    = {6088},
  pages     = {533--536},
  publisher = {{Nature Publishing Group}},
  issn      = {1476-4687},
  doi       = {10.1038/323533a0},
  urldate   = {2023-07-27},
  copyright = {1986 Springer Nature Limited},
  langid    = {english}
}
@misc{schaulNoMorePesky2013,
  title         = {No {{More Pesky Learning Rates}}},
  author        = {Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  year          = {2013},
  month         = feb,
  number        = {arXiv:1206.1106},
  eprint        = {1206.1106},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1206.1106},
  urldate       = {2023-05-31},
  archiveprefix = {arxiv}
}

@misc{smithBayesianPerspectiveGeneralization2018,
  title         = {A {{Bayesian Perspective}} on {{Generalization}} and {{Stochastic Gradient Descent}}},
  author        = {Smith, Samuel L. and Le, Quoc V.},
  year          = {2018},
  month         = feb,
  number        = {arXiv:1710.06451},
  eprint        = {1710.06451},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1710.06451},
  urldate       = {2023-07-28},
  archiveprefix = {arxiv}
}
@misc{smithCyclicalLearningRates2017,
  title         = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  author        = {Smith, Leslie N.},
  year          = {2017},
  month         = apr,
  number        = {arXiv:1506.01186},
  eprint        = {1506.01186},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1506.01186},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv}
}
@misc{smithDisciplinedApproachNeural2018,
  title         = {A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay},
  shorttitle    = {A Disciplined Approach to Neural Network Hyper-Parameters},
  author        = {Smith, Leslie N.},
  year          = {2018},
  month         = apr,
  number        = {arXiv:1803.09820},
  eprint        = {1803.09820},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1803.09820},
  urldate       = {2023-06-07},
  archiveprefix = {arxiv},
  annotation    = {761 citations (Semantic Scholar/arXiv) [2023-06-07]}
}
@misc{smithDonDecayLearning2018,
  title         = {Don't {{Decay}} the {{Learning Rate}}, {{Increase}} the {{Batch Size}}},
  author        = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  year          = {2018},
  month         = feb,
  number        = {arXiv:1711.00489},
  eprint        = {1711.00489},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  urldate       = {2022-11-17},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {770 citations (Semantic Scholar/arXiv) [2023-02-17]}
}
@misc{smithSuperConvergenceVeryFast2018a,
  title         = {Super-{{Convergence}}: {{Very Fast Training}} of {{Neural Networks Using Large Learning Rates}}},
  shorttitle    = {Super-{{Convergence}}},
  author        = {Smith, Leslie N. and Topin, Nicholay},
  year          = {2018},
  month         = may,
  number        = {arXiv:1708.07120},
  eprint        = {1708.07120},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/1708.07120},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv},
  langid        = {english}
}

@article{souzaChallengesBenchmarkingStream2020c,
  title         = {Challenges in {{Benchmarking Stream Learning Algorithms}} with {{Real-world Data}}},
  author        = {Souza, Vinicius M. A. and dos Reis, Denis M. and Maletzke, Andre G. and Batista, Gustavo E. A. P. A.},
  year          = {2020},
  month         = nov,
  journal       = {Data Mining and Knowledge Discovery},
  volume        = {34},
  number        = {6},
  eprint        = {2005.00113},
  primaryclass  = {cs, stat},
  pages         = {1805--1858},
  issn          = {1384-5810, 1573-756X},
  doi           = {10.1007/s10618-020-00698-5},
  urldate       = {2023-05-31},
  archiveprefix = {arxiv}
}

@inproceedings{sutskeverImportanceInitializationMomentum2013,
  title     = {On the Importance of Initialization and Momentum in Deep Learning},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author    = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  year      = {2013},
  month     = may,
  pages     = {1139--1147},
  publisher = {{PMLR}},
  issn      = {1938-7228},
  urldate   = {2023-05-31},
  langid    = {english}
}
@incollection{tielemanLecture5rmspropDivide2012,
  title     = {Lecture 6.5-Rmsprop: {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  booktitle = {{{COURSERA}}: {{Neural Networks}} for {{Machine Learning}}},
  publisher = {Coursera},
  author    = {Tieleman, Tijmen and Hinton, Geoffrey},
  year      = {2012},
  month     = apr
}

@misc{vanervenMetaGradMultipleLearning2016a,
  title         = {{{MetaGrad}}: {{Multiple Learning Rates}} in {{Online Learning}}},
  shorttitle    = {{{MetaGrad}}},
  author        = {{van Erven}, Tim and Koolen, Wouter M.},
  year          = {2016},
  month         = nov,
  number        = {arXiv:1604.08740},
  eprint        = {1604.08740},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1604.08740},
  urldate       = {2023-05-30},
  archiveprefix = {arxiv},
  annotation    = {74 citations (Semantic Scholar/arXiv) [2023-05-30]}
}
@article{velosoHyperparameterSelftuningData2021,
  title   = {Hyperparameter Self-Tuning for Data Streams},
  author  = {Veloso, Bruno and Gama, Jo{\~a}o and Malheiro, Benedita and Vinagre, Jo{\~a}o},
  year    = {2021},
  month   = dec,
  journal = {Information Fusion},
  volume  = {76},
  pages   = {75--86},
  issn    = {1566-2535},
  doi     = {10.1016/j.inffus.2021.04.011},
  urldate = {2023-08-04},
  langid  = {english}
}
@misc{wuDemystifyingLearningRate2019b,
  title         = {Demystifying {{Learning Rate Policies}} for {{High Accuracy Training}} of {{Deep Neural Networks}}},
  author        = {Wu, Yanzhao and Liu, Ling and Bae, Juhyun and Chow, Ka-Ho and Iyengar, Arun and Pu, Calton and Wei, Wenqi and Yu, Lei and Zhang, Qi},
  year          = {2019},
  month         = oct,
  number        = {arXiv:1908.06477},
  eprint        = {1908.06477},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/1908.06477},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv},
  langid        = {english}
}
@misc{wuWNGradLearnLearning2020,
  title         = {{{WNGrad}}: {{Learn}} the {{Learning Rate}} in {{Gradient Descent}}},
  shorttitle    = {{{WNGrad}}},
  author        = {Wu, Xiaoxia and Ward, Rachel and Bottou, L{\'e}on},
  year          = {2020},
  month         = nov,
  number        = {arXiv:1803.02865},
  eprint        = {1803.02865},
  primaryclass  = {cs, math, stat},
  publisher     = {{arXiv}},
  urldate       = {2023-05-25},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {63 citations (Semantic Scholar/arXiv) [2023-05-30]}
}

@misc{zeilerADADELTAAdaptiveLearning2012a,
  title         = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  shorttitle    = {{{ADADELTA}}},
  author        = {Zeiler, Matthew D.},
  year          = {2012},
  month         = dec,
  number        = {arXiv:1212.5701},
  eprint        = {1212.5701},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-05-25},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {6033 citations (Semantic Scholar/arXiv) [2023-05-25]}
}


@misc{zhangPOLAOnlineTime2021a,
  title         = {{{POLA}}: {{Online Time Series Prediction}} by {{Adaptive Learning Rates}}},
  shorttitle    = {{{POLA}}},
  author        = {Zhang, Wenyu},
  year          = {2021},
  month         = feb,
  number        = {arXiv:2102.08907},
  eprint        = {2102.08907},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/2102.08907},
  urldate       = {2023-08-04},
  archiveprefix = {arxiv},
  langid        = {english}
}