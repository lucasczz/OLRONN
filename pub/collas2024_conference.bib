@article{agrawalDatabaseMiningPerformance1993,
  title      = {Database Mining: A Performance Perspective},
  shorttitle = {Database Mining},
  author     = {Agrawal, R. and Imielinski, T. and Swami, A.},
  year       = {1993},
  month      = dec,
  journal    = {IEEE Transactions on Knowledge and Data Engineering},
  volume     = {5},
  number     = {6},
  pages      = {914--925},
  issn       = {1041-4347, 1558-2191, 2326-3865},
  doi        = {10.1109/69.250074},
  urldate    = {2024-02-13},
  annotation = {1666 citations (Semantic Scholar/DOI) [2024-02-13]}
}

@article{al-khameesEnhancingStabilityDeep2023,
  title   = {Enhancing the Stability of the Deep Neural Network Using a Non-Constant Learning Rate for Data Stream},
  author  = {{Al-Khamees}, Hussein and {Al-A'araji}, Nabeel and {Al-Shamery}, Eman},
  year    = {2023},
  month   = apr,
  journal = {International Journal of Electrical and Computer Engineering},
  volume  = {13},
  pages   = {2123--2130},
  doi     = {10.11591/ijece.v13i2.pp2123-2130}
}

@inproceedings{baydinOnlineLearningRate2018,
  title     = {Online {{Learning Rate Adaptation}} with {{Hypergradient Descent}}},
  booktitle = {{{ICLR Proceedings}}},
  author    = {Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
  year      = {2018},
  month     = feb,
  langid    = {english}
}@misc{bengioPracticalRecommendationsGradientbased2012,
  title         = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  author        = {Bengio, Yoshua},
  year          = {2012},
  month         = sep,
  number        = {arXiv:1206.5533},
  eprint        = {1206.5533},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv},
  langid        = {english}
}

@inproceedings{bifetAdaptiveLearningEvolving2009,
  title      = {Adaptive {{Learning}} from {{Evolving Data Streams}}},
  booktitle  = {Advances in {{Intelligent Data Analysis VIII}}},
  author     = {Bifet, Albert and Gavald{\`a}, Ricard},
  editor     = {Adams, Niall M. and Robardet, C{\'e}line and Siebes, Arno and Boulicaut, Jean-Fran{\c c}ois},
  year       = {2009},
  series     = {Lecture {{Notes}} in {{Computer Science}}},
  pages      = {249--260},
  publisher  = {{Springer}},
  address    = {{Berlin, Heidelberg}},
  doi        = {10.1007/978-3-642-03915-7_22},
  isbn       = {978-3-642-03915-7},
  langid     = {english},
  annotation = {370 citations (Semantic Scholar/DOI) [2023-08-11]}
}

@inproceedings{bifetLearningTimeChangingData2007,
  title      = {Learning from {{Time-Changing Data}} with {{Adaptive Windowing}}},
  booktitle  = {Proceedings of the 2007 {{SIAM International Conference}} on {{Data Mining}}},
  author     = {Bifet, Albert and Gavald{\`a}, Ricard},
  year       = {2007},
  month      = apr,
  pages      = {443--448},
  publisher  = {{Society for Industrial and Applied Mathematics}},
  doi        = {10.1137/1.9781611972771.42},
  urldate    = {2023-02-22},
  isbn       = {978-0-89871-630-6 978-1-61197-277-1},
  langid     = {english},
  annotation = {ADWIN}
}

@article{bifetMOAMassiveOnline2010,
  title      = {{{MOA}}: Massive Online Analysis},
  shorttitle = {{{MOA}}},
  author     = {Bifet, Albert and Holmes, Geoffrey and Kirkby, Richard and Pfahringer, Bernhard},
  year       = {2010},
  month      = may,
  journal    = {Journal of Machine Learning Research},
  volume     = {11}
}

@incollection{bottouLargeScaleMachineLearning2011,
  title     = {Large-{{Scale Machine Learning}} with {{Stochastic Gradient Descent L\'eon Bottou}}},
  booktitle = {Statistical {{Learning}} and {{Data Science}}},
  editor    = {Bottou, Leon and Goldfarb, Bernard and Murtagh, Fionn and Pardoux, Catherine and Touati, Myriam},
  year      = {2011},
  month     = dec,
  edition   = {0},
  pages     = {33--42},
  publisher = {{Chapman and Hall/CRC}},
  doi       = {10.1201/b11429-6},
  urldate   = {2023-04-13},
  isbn      = {978-0-429-10768-9},
  langid    = {english}
}
@misc{bottouOptimizationMethodsLargeScale2018,
  title         = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author        = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  year          = {2018},
  month         = feb,
  number        = {arXiv:1606.04838},
  eprint        = {1606.04838},
  primaryclass  = {cs, math, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/1606.04838},
  urldate       = {2023-07-31},
  archiveprefix = {arxiv},
  langid        = {english}
}
@incollection{bottouStochasticGradientDescent2012,
  title     = {Stochastic {{Gradient Descent Tricks}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author    = {Bottou, L{\'e}on},
  editor    = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year      = {2012},
  series    = {Lecture {{Notes}} in {{Computer Science}}},
  pages     = {421--436},
  publisher = {{Springer}},
  address   = {{Berlin, Heidelberg}},
  doi       = {10.1007/978-3-642-35289-8\_25},
  urldate   = {2023-07-27},
  isbn      = {978-3-642-35289-8},
  langid    = {english}
}

@inproceedings{canonacoAdaptiveFederatedLearning2021,
  title     = {Adaptive {{Federated Learning}} in {{Presence}} of {{Concept Drift}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author    = {Canonaco, Giuseppe and Bergamasco, Alex and Mongelluzzo, Alessio and Roveri, Manuel},
  year      = {2021},
  month     = jul,
  pages     = {1--7},
  issn      = {2161-4407},
  doi       = {10.1109/IJCNN52387.2021.9533710}
}

@misc{carmonMakingSGDParameterFree2023,
  title         = {Making {{SGD Parameter-Free}}},
  author        = {Carmon, Yair and Hinder, Oliver},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2205.02160},
  eprint        = {2205.02160},
  primaryclass  = {cs, math, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/2205.02160},
  urldate       = {2023-07-13},
  archiveprefix = {arxiv},
  langid        = {english}
}

@misc{chaudhariEntropySGDBiasingGradient2017,
  title         = {Entropy-{{SGD}}: {{Biasing Gradient Descent Into Wide Valleys}}},
  shorttitle    = {Entropy-{{SGD}}},
  author        = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  year          = {2017},
  month         = apr,
  number        = {arXiv:1611.01838},
  eprint        = {1611.01838},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  urldate       = {2023-07-28},
  abstract      = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
  archiveprefix = {arxiv},
  langid        = {english},
  file          = {C\:\\Users\\Lucas\\Zotero\\storage\\E9VRGB2I\\Chaudhari et al. - 2017 - Entropy-SGD Biasing Gradient Descent Into Wide Va.pdf}
}
@misc{cutkoskyMechanicLearningRate2023,
  title         = {Mechanic: {{A Learning Rate Tuner}}},
  shorttitle    = {Mechanic},
  author        = {Cutkosky, Ashok and Defazio, Aaron and Mehta, Harsh},
  year          = {2023},
  month         = jun,
  number        = {arXiv:2306.00144},
  eprint        = {2306.00144},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/2306.00144},
  urldate       = {2023-06-07},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {0 citations (Semantic Scholar/arXiv) [2023-06-07]}
}
@misc{defazioLearningRateFreeLearningDAdaptation2023a,
  title         = {Learning-{{Rate-Free Learning}} by {{D-Adaptation}}},
  author        = {Defazio, Aaron and Mishchenko, Konstantin},
  year          = {2023},
  month         = may,
  number        = {arXiv:2301.07733},
  eprint        = {2301.07733},
  primaryclass  = {cs, math, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/2301.07733},
  urldate       = {2023-06-07},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {7 citations (Semantic Scholar/arXiv) [2023-06-07]}
}


@misc{dohareLossPlasticityDeep2023,
  title         = {Loss of {{Plasticity}} in {{Deep Continual Learning}}},
  author        = {Dohare, Shibhansh and {Hernandez-Garcia}, J. Fernando and Rahman, Parash and Sutton, Richard S. and Mahmood, A. Rupam},
  year          = {2023},
  month         = aug,
  number        = {arXiv:2306.13812},
  eprint        = {2306.13812},
  primaryclass  = {cs},
  publisher     = {arXiv},
  url           = {http://arxiv.org/abs/2306.13812},
  urldate       = {2024-03-26},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {8 citations (Semantic Scholar/arXiv) [2024-03-26]}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title      = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author     = {Duchi, John C. and Hazan, Elad and Singer, Yoram},
  year       = {2011},
  month      = feb,
  journal    = {Journal of Machine Learning Research},
  volume     = {12},
  number     = {61},
  pages      = {2121--2159},
  annotation = {MAG ID: 2146502635}
}

@article{duchiDualAveragingDistributed2012,
  title      = {Dual {{Averaging}} for {{Distributed Optimization}}: {{Convergence Analysis}} and {{Network Scaling}}},
  shorttitle = {Dual {{Averaging}} for {{Distributed Optimization}}},
  author     = {Duchi, J. C. and Agarwal, A. and Wainwright, M. J.},
  year       = {2012},
  month      = mar,
  journal    = {IEEE Transactions on Automatic Control},
  volume     = {57},
  number     = {3},
  pages      = {592--606},
  issn       = {0018-9286, 1558-2523},
  doi        = {10.1109/TAC.2011.2161027},
  urldate    = {2023-08-09},
  langid     = {english}
}
@misc{elsayedUtilitybasedPerturbedGradient2023,
  title         = {Utility-Based {{Perturbed Gradient Descent}}: {{An Optimizer}} for {{Continual Learning}}},
  shorttitle    = {Utility-Based {{Perturbed Gradient Descent}}},
  author        = {Elsayed, Mohamed and Mahmood, A. Rupam},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2302.03281},
  eprint        = {2302.03281},
  primaryclass  = {cs},
  publisher     = {arXiv},
  url           = {http://arxiv.org/abs/2302.03281},
  urldate       = {2024-03-27},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {2 citations (Semantic Scholar/arXiv) [2024-03-27]}
}
@article{fekriDeepLearningLoad2021,
  title      = {Deep Learning for Load Forecasting with Smart Meter Data: {{Online Adaptive Recurrent Neural Network}}},
  shorttitle = {Deep Learning for Load Forecasting with Smart Meter Data},
  author     = {Fekri, Mohammad Navid and Patel, Harsh and Grolinger, Katarina and Sharma, Vinay},
  year       = {2021},
  month      = jan,
  journal    = {Applied Energy},
  volume     = {282},
  pages      = {116177},
  issn       = {03062619},
  doi        = {10.1016/j.apenergy.2020.116177},
  urldate    = {2023-08-04},
  langid     = {english}
}

@inproceedings{ferreirajoseADADRIFTAdaptiveLearning2020,
  title      = {{{ADADRIFT}}: {{An Adaptive Learning Technique}} for {{Long-history Stream-based Recommender Systems}}},
  shorttitle = {{{ADADRIFT}}},
  booktitle  = {2020 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author     = {Ferreira Jose, Eduardo and Enembreck, Fabricio and Paul Barddal, Jean},
  year       = {2020},
  month      = oct,
  pages      = {2593--2600},
  publisher  = {{IEEE}},
  address    = {{Toronto, ON, Canada}},
  doi        = {10.1109/SMC42975.2020.9282922},
  urldate    = {2023-08-04},
  isbn       = {978-1-72818-526-2},
  langid     = {english},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-08-07]}
}
@article{gamaSurveyConceptDrift2014,
  title   = {A Survey on Concept Drift Adaptation},
  author  = {Gama, Jo{\~a}o and {\v Z}liobait{\.e}, Indr{\.e} and Bifet, Albert and Pechenizkiy, Mykola and Bouchachia, Abdelhamid},
  year    = {2014},
  month   = apr,
  journal = {ACM Computing Surveys},
  volume  = {46},
  number  = {4},
  pages   = {1--37},
  issn    = {0360-0300, 1557-7341},
  doi     = {10.1145/2523813},
  urldate = {2023-08-04},
  langid  = {english}
}
@book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year      = {2016},
  publisher = {{MIT Press}}
}@article{gordonClassificationRegressionTrees1984,
  title      = {Classification and {{Regression Trees}}.},
  author     = {Gordon, A. D. and Breiman, L. and Friedman, J. H. and Olshen, R. A. and Stone, C. J.},
  year       = {1984},
  month      = sep,
  journal    = {Biometrics},
  volume     = {40},
  number     = {3},
  eprint     = {2530946},
  eprinttype = {jstor},
  pages      = {874},
  issn       = {0006341X},
  doi        = {10.2307/2530946},
  urldate    = {2024-02-13}
}@misc{goyalAccurateLargeMinibatch2018,
  title         = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  shorttitle    = {Accurate, {{Large Minibatch SGD}}},
  author        = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  year          = {2018},
  month         = apr,
  number        = {arXiv:1706.02677},
  eprint        = {1706.02677},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/1706.02677},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv},
  langid        = {english}
}

@article{harriesSPLICE2ComparativeEvaluation2003,
  title      = {{{SPLICE-2 Comparative Evaluation}}: {{Electricity Pricing}}},
  shorttitle = {{{SPLICE-2 Comparative Evaluation}}},
  author     = {Harries, Michael and {Nsw-cse-tr}, U and Wales, New},
  year       = {2003},
  month      = may
}

@article{hazanIntroductionOnlineConvex2016,
  title     = {Introduction to {{Online Convex Optimization}}},
  author    = {Hazan, Elad},
  year      = {2016},
  month     = aug,
  journal   = {Foundations and Trends\textregistered{} in Optimization},
  volume    = {2},
  number    = {3-4},
  pages     = {157--325},
  publisher = {{Now Publishers, Inc.}},
  issn      = {2167-3888, 2167-3918},
  doi       = {10.1561/2400000013},
  urldate   = {2023-08-11},
  langid    = {english}
}@article{hochreiterFlatMinima1997,
  title    = {Flat {{Minima}}},
  author   = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year     = {1997},
  month    = jan,
  journal  = {Neural Computation},
  volume   = {9},
  number   = {1},
  pages    = {1--42},
  issn     = {0899-7667, 1530-888X},
  doi      = {10.1162/neco.1997.9.1.1},
  urldate  = {2023-07-28},
  abstract = {We present a new algorithm for finding low-complexity neural networks with high generalization capability. The algorithm searches for a ``flat'' minimum of the error function. A flat minimum is a large connected region in weight space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to ``simple'' networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require gaussian assumptions and does not depend on a ``good'' weight prior. Instead we have a prior over input output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second-order derivatives, it has backpropagation's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms conventional backprop, weight decay, and ``optimal brain surgeon/optimal brain damage.''},
  langid   = {english}
}

@misc{ivgiDoGSGDBest2023,
  title         = {{{DoG}} Is {{SGD}}'s {{Best Friend}}: {{A Parameter-Free Dynamic Step Size Schedule}}},
  shorttitle    = {{{DoG}} Is {{SGD}}'s {{Best Friend}}},
  author        = {Ivgi, Maor and Hinder, Oliver and Carmon, Yair},
  year          = {2023},
  month         = jul,
  number        = {arXiv:2302.12022},
  eprint        = {2302.12022},
  primaryclass  = {cs, math},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/2302.12022},
  urldate       = {2023-07-21},
  archiveprefix = {arxiv},
  langid        = {english}
}

@misc{kingmaAdamMethodStochastic2017b,
  title         = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle    = {Adam},
  author        = {Kingma, Diederik P. and Ba, Jimmy},
  year          = {2017},
  month         = jan,
  number        = {arXiv:1412.6980},
  eprint        = {1412.6980},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-02-24},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {9990 citations (Semantic Scholar/arXiv) [2023-05-25]}
}

@incollection{kunchevaAdaptiveLearningRate2008,
  title     = {Adaptive {{Learning Rate}} for {{Online Linear Discriminant Classifiers}}},
  booktitle = {Structural, {{Syntactic}}, and {{Statistical Pattern Recognition}}},
  author    = {Kuncheva, Ludmila I. and Plumpton, Catrin O.},
  editor    = {Da Vitoria Lobo, Niels and Kasparis, Takis and Roli, Fabio and Kwok, James T. and Georgiopoulos, Michael and Anagnostopoulos, Georgios C. and Loog, Marco},
  year      = {2008},
  volume    = {5342},
  pages     = {510--519},
  publisher = {{Springer Berlin Heidelberg}},
  address   = {{Berlin, Heidelberg}},
  doi       = {10.1007/978-3-540-89689-0\_55},
  urldate   = {2023-08-04},
  abstract  = {We propose a strategy for updating the learning rate parameter of online linear classifiers for streaming data with concept drift. The change in the learning rate is guided by the change in a running estimate of the classification error. In addition, we propose an online version of the standard linear discriminant classifier (O-LDC) in which the inverse of the common covariance matrix is updated using the Sherman-MorrisonWoodbury formula. The adaptive learning rate was applied to four online linear classifier models on generated and real streaming data with concept drift. O-LDC was found to be better than balanced Winnow, the perceptron and a recently proposed online linear discriminant analysis.},
  isbn      = {978-3-540-89688-3 978-3-540-89689-0},
  langid    = {english},
  file      = {C\:\\Users\\Lucas\\Zotero\\storage\\WVY3R98A\\Kuncheva and Plumpton - 2008 - Adaptive Learning Rate for Online Linear Discrimin.pdf}
}

@misc{loshchilovSGDRStochasticGradient2017,
  title         = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  shorttitle    = {{{SGDR}}},
  author        = {Loshchilov, Ilya and Hutter, Frank},
  year          = {2017},
  month         = may,
  number        = {arXiv:1608.03983},
  eprint        = {1608.03983},
  primaryclass  = {cs, math},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/1608.03983},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv},
  langid        = {english}
}

@inproceedings{martensEstimatingHessianBackpropagating2012,
  title     = {Estimating the Hessian by Back-Propagating Curvature},
  booktitle = {Proceedings of the 29th {{International Coference}} on {{International Conference}} on {{Machine Learning}}},
  author    = {Martens, James and Sutskever, Ilya and Swersky, Kevin},
  year      = {2012},
  month     = jun,
  series    = {{{ICML}}'12},
  pages     = {963--970},
  publisher = {{Omnipress}},
  address   = {{Madison, WI, USA}},
  urldate   = {2023-08-08},
  isbn      = {978-1-4503-1285-1}
}

@article{masseyKolmogorovSmirnovTestGoodness1951,
  title      = {The {{Kolmogorov-Smirnov Test}} for {{Goodness}} of {{Fit}}},
  author     = {Massey, Frank J.},
  year       = {1951},
  journal    = {Journal of the American Statistical Association},
  volume     = {46},
  number     = {253},
  eprint     = {2280095},
  eprinttype = {jstor},
  pages      = {68--78},
  publisher  = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn       = {0162-1459},
  doi        = {10.2307/2280095},
  urldate    = {2024-02-13}
}


@misc{misc_covertype_31,
  title        = {Covertype},
  author       = {Blackard, Jock},
  year         = {1998},
  howpublished = {UCI Machine Learning Repository}
}

@article{miyaguchiCograConceptDriftAwareStochastic2019,
  title      = {Cogra: {{Concept-Drift-Aware Stochastic Gradient Descent}} for {{Time-Series Forecasting}}},
  shorttitle = {Cogra},
  author     = {Miyaguchi, Kohei and Kajino, Hiroshi},
  year       = {2019},
  month      = jul,
  journal    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume     = {33},
  number     = {01},
  pages      = {4594--4601},
  issn       = {2374-3468},
  doi        = {10.1609/aaai.v33i01.33014594},
  urldate    = {2023-02-24},
  copyright  = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid     = {english},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-02-24]}
}@misc{montielRiverMachineLearning2020a,
  title         = {River: Machine Learning for Streaming Data in {{Python}}},
  shorttitle    = {River},
  author        = {Montiel, Jacob and Halford, Max and Mastelini, Saulo Martiello and Bolmier, Geoffrey and Sourty, Raphael and Vaysse, Robin and Zouitine, Adil and Gomes, Heitor Murilo and Read, Jesse and Abdessalem, Talel and Bifet, Albert},
  year          = {2020},
  month         = dec,
  number        = {arXiv:2012.04740},
  eprint        = {2012.04740},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2012.04740},
  urldate       = {2024-02-07},
  archiveprefix = {arxiv},
  annotation    = {120 citations (Semantic Scholar/arXiv) [2024-02-07]}
}
@inproceedings{orabonaTrainingDeepNetworks2017,
  title     = {Training {{Deep Networks}} without {{Learning Rates Through Coin Betting}}},
  booktitle = {{{NIPS}}},
  author    = {Orabona, Francesco and Tommasi, Tatiana},
  year      = {2017},
  langid    = {english}
}

@misc{paikOvercomingCatastrophicForgetting2019a,
  title         = {Overcoming {{Catastrophic Forgetting}} by {{Neuron-level Plasticity Control}}},
  author        = {Paik, Inyoung and Oh, Sangjun and Kwak, Tae-Yeong and Kim, Injung},
  year          = {2019},
  month         = jul,
  number        = {arXiv:1907.13322},
  eprint        = {1907.13322},
  primaryclass  = {cs},
  publisher     = {arXiv},
  url           = {http://arxiv.org/abs/1907.13322},
  urldate       = {2024-03-27},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {29 citations (Semantic Scholar/arXiv) [2024-03-27]}
}
@inproceedings{paszkePyTorchImperativeStyle2019,
  title     = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  booktitle = {{{NeurIPS Proceedings}}},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor    = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'{\aftergroup\ignorespaces} {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year      = {2019},
  pages     = {8024--8035},
  publisher = {{Curran Associates, Inc.}},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{raabReactiveSoftPrototype2020a,
  title   = {Reactive {{Soft Prototype Computing}} for {{Concept Drift Streams}}},
  author  = {Raab, Christoph and Heusinger, Moritz and Schleif, Frank-Michael},
  year    = {2020},
  month   = nov,
  journal = {Neurocomputing},
  volume  = {416},
  pages   = {340--351},
  issn    = {0925-2312},
  doi     = {10.1016/j.neucom.2019.11.111},
  urldate = {2023-08-11},
  langid  = {english}
}@article{rumelhartLearningRepresentationsBackpropagating1986a,
  title     = {Learning Representations by Back-Propagating Errors},
  author    = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year      = {1986},
  month     = oct,
  journal   = {Nature},
  volume    = {323},
  number    = {6088},
  pages     = {533--536},
  publisher = {{Nature Publishing Group}},
  issn      = {1476-4687},
  doi       = {10.1038/323533a0},
  urldate   = {2023-07-27},
  copyright = {1986 Springer Nature Limited},
  langid    = {english}
}


@misc{schaulNoMorePesky2013,
  title         = {No {{More Pesky Learning Rates}}},
  author        = {Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  year          = {2013},
  month         = feb,
  number        = {arXiv:1206.1106},
  eprint        = {1206.1106},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1206.1106},
  urldate       = {2023-05-31},
  archiveprefix = {arxiv}
}
@inproceedings{schraudolphLocalGainAdaptation1999,
  title      = {Local Gain Adaptation in Stochastic Gradient Descent},
  booktitle  = {9th {{International Conference}} on {{Artificial Neural Networks}}: {{ICANN}} '99},
  author     = {Schraudolph, N.N.},
  year       = {1999},
  volume     = {1999},
  pages      = {569--574},
  publisher  = {IEE},
  address    = {Edinburgh, UK},
  doi        = {10.1049/cp:19991170},
  urldate    = {2024-03-27},
  isbn       = {978-0-85296-721-8},
  langid     = {english},
  annotation = {196 citations (Semantic Scholar/DOI) [2024-03-27]}
}
@article{shalev-shwartzOnlineLearningOnline2011,
  title   = {Online {{Learning}} and {{Online Convex Optimization}}},
  author  = {{Shalev-Shwartz}, Shai},
  year    = {2011},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume  = {4},
  number  = {2},
  pages   = {107--194},
  issn    = {1935-8237, 1935-8245},
  doi     = {10.1561/2200000018},
  urldate = {2023-08-10},
  langid  = {english}
}
@misc{shamirStochasticGradientDescent2012,
  title         = {Stochastic {{Gradient Descent}} for {{Non-smooth Optimization}}: {{Convergence Results}} and {{Optimal Averaging Schemes}}},
  shorttitle    = {Stochastic {{Gradient Descent}} for {{Non-smooth Optimization}}},
  author        = {Shamir, Ohad and Zhang, Tong},
  year          = {2012},
  month         = dec,
  number        = {arXiv:1212.1824},
  eprint        = {1212.1824},
  primaryclass  = {cs, math, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/1212.1824},
  urldate       = {2023-08-16},
  archiveprefix = {arxiv},
  langid        = {english}
}
@misc{smithBayesianPerspectiveGeneralization2018,
  title         = {A {{Bayesian Perspective}} on {{Generalization}} and {{Stochastic Gradient Descent}}},
  author        = {Smith, Samuel L. and Le, Quoc V.},
  year          = {2018},
  month         = feb,
  number        = {arXiv:1710.06451},
  eprint        = {1710.06451},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1710.06451},
  urldate       = {2023-07-28},
  archiveprefix = {arxiv}
}@misc{smithCyclicalLearningRates2017,
  title         = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  author        = {Smith, Leslie N.},
  year          = {2017},
  month         = apr,
  number        = {arXiv:1506.01186},
  eprint        = {1506.01186},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1506.01186},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv}
}
@misc{smithDisciplinedApproachNeural2018,
  title         = {A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay},
  shorttitle    = {A Disciplined Approach to Neural Network Hyper-Parameters},
  author        = {Smith, Leslie N.},
  year          = {2018},
  month         = apr,
  number        = {arXiv:1803.09820},
  eprint        = {1803.09820},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1803.09820},
  urldate       = {2023-06-07},
  archiveprefix = {arxiv},
  annotation    = {761 citations (Semantic Scholar/arXiv) [2023-06-07]}
}

@misc{smithDonDecayLearning2018,
  title         = {Don't {{Decay}} the {{Learning Rate}}, {{Increase}} the {{Batch Size}}},
  author        = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  year          = {2018},
  month         = feb,
  number        = {arXiv:1711.00489},
  eprint        = {1711.00489},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  urldate       = {2022-11-17},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {770 citations (Semantic Scholar/arXiv) [2023-02-17]}
}


@misc{smithSuperConvergenceVeryFast2018a,
  title         = {Super-{{Convergence}}: {{Very Fast Training}} of {{Neural Networks Using Large Learning Rates}}},
  shorttitle    = {Super-{{Convergence}}},
  author        = {Smith, Leslie N. and Topin, Nicholay},
  year          = {2018},
  month         = may,
  number        = {arXiv:1708.07120},
  eprint        = {1708.07120},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/1708.07120},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv},
  langid        = {english}
}

@article{souzaChallengesBenchmarkingStream2020,
  title         = {Challenges in {{Benchmarking Stream Learning Algorithms}} with {{Real-world Data}}},
  author        = {Souza, Vinicius M. A. and dos Reis, Denis M. and Maletzke, Andre G. and Batista, Gustavo E. A. P. A.},
  year          = {2020},
  month         = nov,
  journal       = {Data Mining and Knowledge Discovery},
  volume        = {34},
  number        = {6},
  eprint        = {2005.00113},
  primaryclass  = {cs, stat},
  pages         = {1805--1858},
  issn          = {1384-5810, 1573-756X},
  doi           = {10.1007/s10618-020-00698-5},
  urldate       = {2023-05-31},
  archiveprefix = {arxiv}
}
@inproceedings{sutskeverImportanceInitializationMomentum2013,
  title     = {On the Importance of Initialization and Momentum in Deep Learning},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author    = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  year      = {2013},
  month     = may,
  pages     = {1139--1147},
  publisher = {{PMLR}},
  issn      = {1938-7228},
  urldate   = {2023-05-31},
  langid    = {english}
}

@inproceedings{suttonAdaptingBiasGradient1992,
  title      = {Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta},
  shorttitle = {Adapting Bias by Gradient Descent},
  booktitle  = {Proceedings of the Tenth National Conference on {{Artificial}} Intelligence},
  author     = {Sutton, Richard S.},
  year       = {1992},
  month      = jul,
  series     = {{{AAAI}}'92},
  pages      = {171--176},
  publisher  = {AAAI Press},
  address    = {San Jose, California},
  urldate    = {2024-04-04},
  isbn       = {978-0-262-51063-9}
}
@incollection{tielemanLecture5rmspropDivide2012,
  title     = {Lecture 6.5-Rmsprop: {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  booktitle = {{{COURSERA}}: {{Neural Networks}} for {{Machine Learning}}},
  publisher = {Coursera},
  author    = {Tieleman, Tijmen and Hinton, Geoffrey},
  year      = {2012},
  month     = apr
}
@misc{vanervenMetaGradMultipleLearning2016a,
  title         = {{{MetaGrad}}: {{Multiple Learning Rates}} in {{Online Learning}}},
  shorttitle    = {{{MetaGrad}}},
  author        = {{van Erven}, Tim and Koolen, Wouter M.},
  year          = {2016},
  month         = nov,
  number        = {arXiv:1604.08740},
  eprint        = {1604.08740},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1604.08740},
  urldate       = {2023-05-30},
  archiveprefix = {arxiv},
  annotation    = {74 citations (Semantic Scholar/arXiv) [2023-05-30]}
}

@article{vanschorenOpenMLNetworkedScience2014,
  title         = {{{OpenML}}: Networked Science in Machine Learning},
  shorttitle    = {{{OpenML}}},
  author        = {Vanschoren, Joaquin and {van Rijn}, Jan N. and Bischl, Bernd and Torgo, Luis},
  year          = {2014},
  month         = jun,
  journal       = {ACM SIGKDD Explorations Newsletter},
  volume        = {15},
  number        = {2},
  eprint        = {1407.7722},
  primaryclass  = {cs},
  pages         = {49--60},
  issn          = {1931-0145, 1931-0153},
  doi           = {10.1145/2641190.2641198},
  urldate       = {2023-08-14},
  archiveprefix = {arxiv}
}@article{velosoHyperparameterSelftuningData2021,
  title   = {Hyperparameter Self-Tuning for Data Streams},
  author  = {Veloso, Bruno and Gama, Jo{\~a}o and Malheiro, Benedita and Vinagre, Jo{\~a}o},
  year    = {2021},
  month   = dec,
  journal = {Information Fusion},
  volume  = {76},
  pages   = {75--86},
  issn    = {1566-2535},
  doi     = {10.1016/j.inffus.2021.04.011},
  urldate = {2023-08-04},
  langid  = {english}
}
@article{widmerLearningPresenceConcept1996,
  title      = {Learning in the Presence of Concept Drift and Hidden Contexts},
  author     = {Widmer, Gerhard and Kubat, Miroslav},
  year       = {1996},
  month      = apr,
  journal    = {Machine Learning},
  volume     = {23},
  number     = {1},
  pages      = {69--101},
  issn       = {1573-0565},
  doi        = {10.1007/BF00116900},
  urldate    = {2022-03-18},
  langid     = {english},
  annotation = {451 citations (Semantic Scholar/DOI) [2023-06-07]}
}

@misc{wuDemystifyingLearningRate2019b,
  title         = {Demystifying {{Learning Rate Policies}} for {{High Accuracy Training}} of {{Deep Neural Networks}}},
  author        = {Wu, Yanzhao and Liu, Ling and Bae, Juhyun and Chow, Ka-Ho and Iyengar, Arun and Pu, Calton and Wei, Wenqi and Yu, Lei and Zhang, Qi},
  year          = {2019},
  month         = oct,
  number        = {arXiv:1908.06477},
  eprint        = {1908.06477},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/1908.06477},
  urldate       = {2023-04-13},
  archiveprefix = {arxiv},
  langid        = {english}
}

@misc{wuWNGradLearnLearning2020,
  title         = {{{WNGrad}}: {{Learn}} the {{Learning Rate}} in {{Gradient Descent}}},
  shorttitle    = {{{WNGrad}}},
  author        = {Wu, Xiaoxia and Ward, Rachel and Bottou, L{\'e}on},
  year          = {2020},
  month         = nov,
  number        = {arXiv:1803.02865},
  eprint        = {1803.02865},
  primaryclass  = {cs, math, stat},
  publisher     = {{arXiv}},
  urldate       = {2023-05-25},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {63 citations (Semantic Scholar/arXiv) [2023-05-30]}
}

@misc{zeilerADADELTAAdaptiveLearning2012a,
  title         = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  shorttitle    = {{{ADADELTA}}},
  author        = {Zeiler, Matthew D.},
  year          = {2012},
  month         = dec,
  number        = {arXiv:1212.5701},
  eprint        = {1212.5701},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-05-25},
  archiveprefix = {arxiv},
  langid        = {english},
  annotation    = {6033 citations (Semantic Scholar/arXiv) [2023-05-25]}
}


@misc{zhangPOLAOnlineTime2021a,
  title         = {{{POLA}}: {{Online Time Series Prediction}} by {{Adaptive Learning Rates}}},
  shorttitle    = {{{POLA}}},
  author        = {Zhang, Wenyu},
  year          = {2021},
  month         = feb,
  number        = {arXiv:2102.08907},
  eprint        = {2102.08907},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  url           = {http://arxiv.org/abs/2102.08907},
  urldate       = {2023-08-04},
  archiveprefix = {arxiv},
  langid        = {english}
}